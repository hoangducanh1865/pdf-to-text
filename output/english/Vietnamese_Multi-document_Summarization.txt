1.1 Problem Statement Text Summarization is the task of condensing lengthy texts into concise and coherent summaries that effectively conveys the main ideas and important information.
This task is exceptionally difficult, even for human beings. With the massive amount of textual content available, especially due to the ever-expanding Internet, comprising web pages, news articles, status updates, blogs, and more, it becomes increasingly challenging to navigate through this vast amount of unstructured data. As a result, we often rely on search engines and skim through the search results. However, there is a need to reduce the length of this textual data by generating focused summaries that capture the important details. This would enable us to navigate the content more efficiently and determine whether larger documents contain the information we are looking for. Additionally, it is impractical to manually create summaries for all documents.
The task of Multi-document Text Summarization is even more challenging as it involves clusters of related documents that can be lengthy, containing overlapping content and spanning thousands of words. In my research on developing a deep neural network for Vietnamese Multi-document Summarization, I encountered a major obstacle due to the scarcity of labeled data. Until recently, only three small publicly available datasets, each comprising approximately 300 samples, existed.
To overcome this limitation, recent studies such as Phobert, BartPho , and ViT5  have utilized a large amount of unlabeled data for pre-training language models, followed by fine-tuning them on a smaller set of labeled data specifically for multi-document summarization.
However, these pre-trained language models have two common limitations. Firstly, they are designed as general-purpose language models, meaning that they are trained with the objective of modeling the probability distribution over sequences of words.
For instance, BARTpho  is based on the BART model , which corrupts input text using a noising function and then learns to reconstruct the original text. Secondly, these models have architectures that are not well-suited for processing long input sequences, which poses a challenge for multi-document summarization tasks. Additionally, studies on multi-document text summarization in Vietnamese texts are still in the early phases and there are not many Vietnamese language models designed especially for this task because the resources are quite scarce.
Motivated by these limitations and my willing to contribute to the development 1of research on abstractive multi-document summarization for Vietnamese text, this Bachelor thesis focuses on building a Vietnamese task-specific language model using an objective function specifically designed for Multi-document Summarization, which is modified from a English model to be suitable for Vietnamese contexts.
This model will be then applied to a variety of Vietnamese summarization datasets and texts.
1.2 Background and Problems of Research Recent advancements in deep learning techniques have significantly improvedthe performance of text summarization models over the past few decades. State-of the-art approaches can be categorized as extractive (MatchSum , DiscoBERT , BertSumExt , and PNBert ), abstractive (BRIO  and SimCLS ), or hybrid (EASE ). The task is challenging due to the difficulty in data creation and performance evaluation. For Vietnamese, there has been little attention on Text Summarization, especially in the context of Multi-document Summarization.
Early studies in Vietnamese Text Summarization primarily focused on Single document Summarization, predominantly employing traditional statistical methods.
For instance,  combined various techniques such as word co-occurrences, TF IDF, position-based, title-based, and proper noun-based methods to select salientsentences and generate the summary. Another study by  introduced a graph based system that utilized a self-organizing map to cluster documents and extract the main idea.
In terms of Vietnamese Multi-document Summarization, there have been limited studies. One of the earliest research works by  presented an extractive system comprising three phases: pre-processing, score computation, and summarization generation. The system employed a set of manually selected features at both word and sentence levels, specifically tailored for Vietnamese news text, to compute sentence scores. These features included word frequency, word location, sentence position, time, and PageRank-based sentence features.
Recent research efforts have predominantly focused on optimizing the capabilities of transformer-based models for Vietnamese Text Summarization. For instance,  introduced extractive models that utilize variations of BERT to generate sentence embeddings. These models concatenate multiple documents into a single paragraph, employ BERT for sentence encoding, and utilize K-means clustering to rank and select salient sentences for summarization. More recently,  presented BARTpho, a large-scale Vietnamese sequence-to-sequence model based on the BART architecture . BARTpho includes two versions: BARTpho wordand BARTpho syllable . These 2