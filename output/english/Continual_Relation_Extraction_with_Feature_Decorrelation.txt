1.1 Problem Statement The Relation Extraction (RE) problem is a very important task in Information Extraction. Because of its effectiveness in extracting information from unstructured text, it is the key component of many NLP tasks, such as Information Retrieval , Question Answering  and Knowledge Graph Construction . In particular, a relation extraction system is expected to classify the semantic relation between two entity mentions in the given context. To deal with this problem, many methods have been proposed and achieved remarkable results –. Nevertheless, a majority of previous RE studies only considered the traditional setting where the set of relations is pre-defined and fixed during the training and testing phases. This setting is not practical as new relations of interest might emerge during the deployment time of RE systems in practice, requiring the models to adapt their operation to accommodate new types. Therefore, Continual Relation Extraction was proposed, aiming to learn new relations from new coming data. Recently, CRE has attracted considerable attention in the literature – because of not only its appealing practical applications but also the challenging problems which come from both fields, Continual Learning, and Relation Extraction.
1.2 Background and Problems of ResearchCompared to conventional RE, CRE has to face the stability-plasticity trade off. Generally speaking, this is a fundamental problem in the continual learning paradigm, where stability relates to maintaining accuracy on the previous tasks, while plasticity relates to learning emerging tasks effectively. Modern deep learning models adapt to new knowledge quickly while lacking stability, this phenomenon is called Catastrophic Forgetting (CF).
Based on how models mitigate Catastrophic Forgetting, existing approaches can be categorized into one of the following three groups: (1) Regularization-based methods – mitigate catastrophic forgetting by constraining the updates of some network parameters depending on their importance. (2) Dynamic architecture methods ,  adjust network capacity dynamically to learn emerging tasks effectively. (3) Replay-based methods – deploy a memory buffer to save a small number of samples from old tasks for later replay. Among three major approaches, the replay-based method has shown the best results for NLP tasks, including CRE.
The current state-of-the-art of the CRE task is Contrastive Replay (CRL)  1which is a replay-based method. Different from previous work, CRL maintains learned knowledge by introducing a contrastive replay mechanism that removes the uses of Softmax classifier and Cross-Entropy loss. It instead proposes to use Supervised Contrastive loss and Nearest Class Mean classifier to make the representation space well-separated. Although CRL outperforms older approaches that use traditional Cross-Entropy loss, it did not learn this pattern in-depth. As a result, it begs the question of why models with Cross-Entropy loss perform so poorly.
1.3 Research Objectives and Conceptual Framework The goal of my research is to investigate the poor performance in the CRE task caused by Cross-Entropy loss. From the investigation, I will propose solutions to improve the performance of the CRE task.
Existing works in Continual Learning mainly focus on the Catastrophic Forgetting problem, while overlooking other factors. In this thesis, instead of traditional approaches where many methods were proposed to preserve old knowledge, I give a concrete analysis of the transferability of the representations in the CRE setting. Especially, I focus on the bad impact of traditional Cross-Entropy loss on the transferability and how to improve it.
In an earlier work,  stated an issue in representation learning for image classification, representation bias. First, if the encoder is fixed after learning old tasks, it can preserve the learned representation space but the learned features are only helpful for the old tasks and not for the new tasks to classify. In contrast, if the encoder is updated with new knowledge, the updated representations would be not suitable for previous tasks. For better understanding, I give an example of CRE. First, a classifier is trained to classify between two relations city_of_birth andparents , but I assume that all the cities from city_of_birth samples are located in China. Then the learned feature extractor, by chance, can only classify texts if it mentions China or not. In the next task, it has to classify between capital _ofand siblings , I see that the learned feature cannot help if capital _ofsamples mention cities around the world. This learned feature is considered not transferable. While if the feature extractor can learn a feature that distinguishes between people and places, it can still perform well on classifying between capital _ofandsiblings .
This feature is transferable because, without updating the feature extractor, therepresentation space is separated to some extent. If the feature extractor learns non transferable features in the previous task, these features can be changed considerably to adapt to the current task. Consequently, old learned features for previous tasks can be forgotten, leading to the performance reduction on those tasks . They 2