Hệ thống đèn giao thông thông minh ứngGiảng viên hướng dẫn: PGS. TS. Thân Quang Lời đầu tiên, em xin được dành lời cảm ơn đến toàn thể các giảng viên đang côngtác tại trường Đại Học Bách Khoa Hà Nội, và đặc biệt là các thầy cô đã tận tìnhhướng dẫn và chỉ bảo em trong suốt 5 năm học tập tại trường. Em tin chắc rằngnhững kiến thức mình đã được chỉ dạy sẽ trở thành hành trang vững chắc cho eĐặc biệt trong đồ án này, em muốn gửi lời cảm ơn chân thành tới PGS. TS.Thân Quang Khoát, người đã hướng dẫn và theo sát em suốt 4 tháng làm việc vàthực hiện đồ án. Em đã học hỏi được rất nhiều kiến thức cũng như kinh nghiệm,cùng với đó là cách làm việc và phân chia công việc một cách hợp lý. Những kinhnghiệm và kiến thức quý báu này sẽ là bước đệm vững chắc trong những dự án tiếEm cũng muốn cảm ơn những người bạn cùng lớp CNTT-10, khoá K62 đã đồnghành cùng em trong suốt những năm học tập tại trường Đại Học Bách Khoa HàNội. Các bạn chắc chắn đã góp phần làm quãng thời gian sinh viên trở nên vô cùnCuối cùng, em xin được cảm ơn gia đình, những người đã luôn luôn cố gắng hếtsức để hỗ trợ em, tạo điều kiện tốt nhất để em có thể có quãng thời gian học tậphiệu quả, luôn ở bên ủng hộ em những thời điểm tuyệt vời cũng như khi khó khăn.Em xin chân thành cảm ơn tất cả mọi ngườiGiao thông là thứ mà tất cả chúng ta tham gia và tương tác mỗi ngày. Điều đódẫn đến sự quan trọng của việc tối ưu hoá hệ thống giao thông. Tuy nhiên, trongthực tế tại Việt Nam nói riêng và trên toàn thế giới nói riêng, ta có thể thấy hệ thốnggiao thông vẫn tồn tại một số khuyết điểm có thể được cải thiện. Trong đó, trongphạm vi đồ án này, em chọn một vấn đề liên quan đến hệ thống đèn giao thông.Mỗi chúng ta tham gia giao thông đôi khi đều có thể rơi vào tình huống đứngchờ đèn đỏ một thời gian vô cùng lâu, trong đó đôi khi có những trường hợp mà tacó thể thấy các làn xe khác đang rất thoáng và đèn giao thông nên thay đổi để phùhợp hơn với tình trạng hiện tại. Sự không tối ưu này dẫn đến việc lãng phí thời giancủa người tham gia giao thông, đồng thời cũng có những tác động tiêu cực đến cảmxúc cũng như sức khoẻ của họ trong những điều kiện thời tiết cực đoan.Hiện tại, hệ thống đèn giao thông đang hoạt động theo cơ chế thay đổi tuần tựtheo các mức thời gian định trước. Ví dụ ở một cụm đèn sẽ có 30 giây đèn xanh,30 giây đèn đỏ. Do đó ta có thể thấy rằng nó hoàn toàn không phản ảnh được lưulượng giao thông thực tế tại chính thời điểm đó, từ đó dẫn tới sự không tối ưu.Giải pháp mà em để xuất trong phạm vi đồ án tốt nghiệp này là sử dụng một môhình học tăng cường, cùng với việc giả định có thông tin các cảm biến trên lòngđường để tính toán lưu lượng giao thông, từ đó đưa ra một quyết định thay đổi đènhợp lý tuỳ thuộc vào tình hình giao thông trong thời gian thực. Hướng tiếp cận nàykhắc phục được điểm yếu chính của giải pháp hiện tại, đó là mang tính chất thờigian thực (real-time) và các lựa chọn đều được tối ưu cho một hàm mục tiêu cụ thể(ở đây là giảm thiểu tổng thời gian dừng lại của toàn bộ phương tiện tham gia tronSau quá trình thực hiện, em đã xây dựng một môi trường mô phỏng tương đốihoàn thiện, thể hiện tốt một số tính chất quan trọng trong môi trường thực tế, vàhuấn luyện được một mô hình học tăng cường có khả năng tối ưu hoá hệ thống đèngiao thông. Mô hình này cũng đạt hiệu năng gấp 2 lần so với hệ thống không tốVới kết quả này, em hi vọng đồ án của mình sẽ phần nào góp phần xây dựngnhững giải pháp tối ưu hoá trong thực tế. Từ đó mọi người có thể tham gia giaothông một cách nhanh chóng, an toàn, thoải mái hơnTraffic is something that we encounter everyday; therefore, optimizing the trafficsystem is crucial. However, we can certainly notice that there are some part of thetraffic system that are not fully optimized. In the scope of this thesis, I propose Sometimes when we are in the traffic, we can notice that the traffic light signaldecision is not reasonable. For example, when the other lanes are free but we stillhave to wait for the red light in our lane to get turned off to start moving. This is dueto the system not taking into account the real-time traffic flow, the usual behavioris just changing the light signal after a pre-defined amount of time. This can leadto huge waste of time, and also affect our health and mood.My proposed solution in the scope of this thesis is using reinforcement learning,with the assumption that there are sensors on the street to measure the traffic flow.From there, the reinforcement learning agent can use the information to makedecision about the traffic light signal. This solution resolve the current problemof not being real-time and optimize for a certain objective function which is thetotal stopping time of all vehicles in the environment.Throughout the course of the thesis, I managed to build a fairly complete environmentwith current, which represent important properties of real-world environment. Also,I develop and trained a reinforcement learning model that are capable of makingoptimized decision. The trained model out-perform the current unoptimized behavioI hope that my thesis and the result I managed to achieve will play a part infuture optimizing solution that help to solve the problem in real-world. From that,everyone can join traffic with fast, safe, and please manner4.6.2 Huấn luyện mô hình với môi trường có hệ thống cHiện nay, trong thời đại cách mạng công nghiệp 4.0, sự phát triển và ứng dụngcông nghệ trở thành một trong những yếu tố tiên quyết dẫn đến thành công củanhiều ngành nghề và lĩnh vực. Trong số đó, trí tuệ nhân tạo (AI [1]) được xem làmột trong những công nghệ được đánh giá cao, có nhiều ứng dụng và đã mang lạinhững cải thiện rất đáng kể cho nhiều lĩnh vực. Tuy nhiên, có một số lĩnh vực màkhông có nhiều ứng dụng nổi bật của AI, trong số đó có thể kể đến giao thông.Một hệ thống giao thông tối ưu là nhu cầu quan trọng do đây là thứ mà mỗi cánhân chúng ta đều tương tác, tham gia và sử dụng mỗi ngày. Tuy nhiên hệ thốnggiao thông mà hiện chúng ta vẫn đang tham gia và tương tác mỗi ngày vẫn cònnhiều điểm có thể cải tiến. Một phần chưa tối ưu trong hệ thống này có thể kể đếncác cụm đèn giao thông. Hiện nay, ta có thể dễ dàng thấy rằng có nhiều trườnghợp mà tín hiệu đèn giao thông không phản ánh đúng tình trạng giao thông thựctế. Điều này dẫn đến các quyết định tín hiệu đèn và thời gian đèn được đưa ra mộtcách không hợp lý, gây ra lãng phí thời gian, ảnh hưởng đến sức khoẻ và tâm lý củChính vì vậy, trong đồ án này em đã chọn ứng dụng AI, mà cụ thể là phươngpháp học tăng cường, để tìm ra giải pháp tối ưu hoá những vấn đề của hệ thốngđèn giao thông hiện tại. Với đồ án này, em hướng tới việc thực hiện nó như mộtbằng chứng khái niệm (Proof of concept), để thể hiện sự khả thi của việc ứng dụnghọc tăng cường trong giải quyết bài toán về lĩnh vực giao thông, hướng tới một hệthống giao thông thông minh, đem lại giá trị lớn cho xã hội.1.2 Các giải pháp hiện tại và hạn chếHiện nay, giải pháp đang được sử dụng ở ngoài thực tế là một hệ thống đèn giaothông thay thay đổi tuần tự. Trong đó, các mốc thời gian tương ứng cho từng màuđèn, cũng như thứ tự thay đổi màu của các đèn đều được xác định từ trước. Điềunày giải quyết vấn đề tương đối tốt với những trường hợp giao thông xảy ra vớiđúng như những mức thời gian và những quy luật đã được đề ra từ trước.Tuy nhiên, những ước lượng và tính toán để ra được các quy luật trên đều đượctính một cách trung bình với các mốc thời gian lớn, điều này dẫn tới sai số đối vớithực tế. Bởi vì bản chất của phương pháp trên không thể hiện được các thông tinvề tình trạng giao thông thực tế ở ngoài môi trường, do đó sẽ không thể đưa ra cáctín hiệu chuẩn xác và phù hợp nếu các ước lượng trên không được thoả mãn. Bêcạnh đó thì việc tính toán trên cũng phải diễn ra thường xuyên để có thể theo kịpvới những thay đổi ngoài thực tế, điều này cũng yêu cầu có một nguồn nhân lực Với các hạn chế được nêu trong phần 1.2, em đã đi đến một hướng giải quyếtmới cho bài toán. Với việc giả định là các tuyến đường có thông tin từ cảm biếnđược chuyển về cho ta biết tại từng vị trí có phương tiện đang chiếm hay không. Từthông tin đó, mô hình sẽ đưa ra cho ta các quyết định về tín hiệu đèn giao thônGiải pháp này hướng tới giải quyết các vấn đề của phương pháp truyền thốngđược nhắc đến trong phần 1.2. Với các thông tin môi trường được cập nhật liên tục,mô hình sẽ đưa ra các quyết định phù hợp nhất với đúng thông tin tương ứng đó, từđó có thể giải quyết được vấn đề về tín hiệu không tương ứng với thực tế. Bên cạnhđó, ta cũng có thể liên tục tiếp tục việc huấn luyện mô hình với các quyết định thựctế trên, và sử dụng một số hướng triển khai mô hình mới một cách an toàn, từ đócó thể đảm bảo việc luôn luôn có một mô hình hoạt động hiệu quả và phù hợp vướimôi trường đặt ra, không bị giảm hiệu năng với các thay đổi về môi trường thĐồ án này có 3 đóng góp chính như sau:1. Đề xuất một phương pháp tối ưu hoá hệ thống đèn giao thông, sử dụng cácthông tin của môi trường và đưa ra các tín hiệu đèn giao thông phù hợp vớitrình trang giao thông thực tế trong thời gian thực.2. Xây dựng một môi trường mô phỏng với bài toán đặt ra là tối ưu hoá hệ thốngđèn giao thông ở một ngã tư đơn giản.3. Thử nghiệm một số phương pháp học tăng cường để giải quyết bài toán trênPhần còn lại của báo cáo đồ án tốt nghiệp này được tổ chức như sau.Ở chương 2, em sẽ giới thiệu về những kiến thức nền tảng cần liên quan đến bàitoán đặt ra cũng như những lý thuyết được áp dụng trong giải pháp của bài toán.Thông qua chương này, người đọc có thể hiểu rõ hơn về nền tảng của giải pháp cầntrình bày, có khả năng hiểu các khái niệm, kiến thức, thuật toán được sử dụng trongđồ án này. Đồng thời, em cũng trình bày một số phương pháp đã và đang được pháttriển để giải quyết bài toán, cùng với đó là những điểm mạnh và hạn chế của cChương 3 sẽ bao gồm thông tin chi tiết về giải pháp mà em đưa ra và quá trìnhphát triển nó. Người đọc có thể hiểu được tư tưởng cốt lõi của giải pháp, đồng thờicũng có khả năng kết hợp với lý thuyết để có thể tái tạo lại được các kết quả mà eChương tiếp theo sẽ bao gồm những kết quả thử nghiệm và đánh giá của em.Trong chương này, em trình bày về một số tham số đánh giá được sử dụng và kếtquả đạt được đối với việc thử nghiệm nhiều tham số khác nhau của môi trường vTrong phần tổng kết em sẽ tổng kết lại thông tin về đồ án, kết quả đạt được vàtrình bày một số hướng phát triển trong tương lai.Cuối báo cáo sẽ là danh sách các tài liệu tham khảo mà em sử dụng trong quátrình phát triển đồ án cũng như thực hiện viết báo cáo cho đồ áĐối với bài toán đã được giới thiệu trong Chương 1, để giải quyết được bài toán,trước hết ta cần tìm hiểu một số nền tảng lý thuyết. Từ những lý thuyết này, ta cóthể hiểu hơn về bài toán đặt ra cũng như có cái nhìn tổng quan các hướng đi choviệc giải quyết bài toán. Chương này sẽ giới thiệu các kiến thức từ đơn giản tớinâng cao để người đọc có khả năng hiểu được rõ về giải pháp được đề xuất troNhư đã giới thiệu trong phần trước, bài toán mà ta đang hướng tới là giải quyếtmột môi trường ngã tư đơn giản bao gồm 4 hướng. Ở đây, đối tượng mà ta cần tốiưu là hệ thống đèn giao thông, trong đó tại mỗi thời điểm đưa ra quyết định, ta cóthể lựa chọn tiếp tục màu đèn cũ hoặc thay đổi màu đèn. Khi màu đèn được thayđổi, hệ thống các phương tiện giao thông sẽ tuân thủ theo tín hiệu đèn mà ta đưa ra.Trong phạm vi của bài toán, để đơn giản hoá quá trình này, ta ngầm hiểu hệ thốngphương tiện tham gia giao thông sẽ tuân thủ hoàn toàn theo đèn tín hiệu. Bên cạnhđó, để có thể đề xuất một số giải pháp tối ưu, ta cũng ngầm định việc có đủ nguồnlực để thay đổi một số yếu tố môi trường như việc thêm hệ thống cảm biến dưHiện tại, hệ thống đèn giao thông đang hoạt động theo phương pháp thay đổituần tự dựa trên một khung thời gian đã được cố định từ trước. Khung thời gian nàyđược tính toán thông qua việc lấy các thông tin về mật độ xe trong khu vực trongmột thời gian dài và tính toán ra một giá trị trung bình cho các thời gian đó. Từ đó,sử dụng thông tin trên để tìm ra được một bộ số thời gian phù hợp. Phương phápnày có điểm hạn chế lớn là không sử dụng các thông tin trong thời gian thực, dođó không thể tối ưu được cho từng thời điểm cụ thể của môi trường. Bên cạnh đó,các thông tin về giao thông cũng thay đổi theo thời gian, điều này đặt ra yêu cầu vềviệc tính toán lại các giá trị bộ số trên cho phù hợp với môi trườngHọc tăng cường đã được ứng dụng trong kiểm soát tín hiệu đèn giao thông trongmột số nghiên cứu khoa học trước đây tuy nhiên còn tương đối đơn giản với môitrường mô phỏng thiếu thực tế. Học tăng cường đã bắt đầu được ứng dụng trongkiểm soát tín hiệu đèn giao thông từ những năm của thập niên 90. Nghiên cứu [tổng hợp các phương pháp từ 1997 tới năm 2010 sử dụng học tăng cường trong lĩnhvực này. Trong giai đoạn này, các phương pháp phần lớn bị giới hạn ở Q-learning[3] dạng bảng với giá trị Q được tính toán bằng một hàm tuyến tính. Bên cạnh đó,do những giới hạn về giải thuật thời điểm đó, không gian trạng thái của môi trườngthường được thu hẹp lại thành một vài giá trị như số lượng xe đang chờ đèn đỏ, sốlượng xe ở mỗi làn đường,... Chính vì thế, thông tin của môi trường không được thểhiện một cách đầy đủ, dẫn tới hiệu năng không được nâng cao.Với sự phát triển của học tăng cường trong các năm gần đây, các phương phápmới và hiệu quả liên tục được giới thiệu. Do đó, một số nghiên cứu cũng thử nghiệmứng dụng học tăng cường vào kiểm soát tín hiệu đèn giao thông. Nghiên cứu [4]và [5] đã sử dụng mạng nơ-ron để xấp sỉ hàm giá trị Q để vượt qua vấn đề về kíchthước không gian trạng thái của phương pháp Q-learning truyền thống, đạt đượckết quả khả quan hơn so với các phương pháp trước đó. Nghiên cứu [6] đã ứngdụng những phương pháp tiên tiến hơn của học tăng cường trong thời điểm đó nhưDouble DQN [7] hay Prioritised Experience Replay (PER) [8].Các phương pháp trên đều đạt được kết quả tốt, tuy nhiên vẫn tồn tại một sốvấn đề. Các kết quả đạt được đều rất khó hoặc không thể tái tạo lại do sự thiếu hụtthông tin về chi tiết triển khai thuật toán, phương pháp tính toán các độ đo đánhgiá và chi tiết phương pháp tạo ra hiệu năng cơ sở. Môi trường được sử dụng trongviệc phát triển thuật toán và đánh giá kết quả cũng thiếu sự tùy biến, khó có thểthiết kế và thu thập được thêm các thông tin mới nếu muốn phát triển bài toán. Bêncạnh đó, cùng với sự phát triển liên tục của lĩnh vực học tăng cường, các phươngpháp mới được phát triển liên tục và đạt được kết quả cải thiện nhiều so với cácphương pháp cũ. Do đó, việc ứng dụng các phương pháp này vào bài kiểm soát tínhiệu đèn giao thông, dưới một môi trường giả lập phù hợp với thực tế là một việcquan trọng, là bước đầu cho quá trình ứng dụng học tăng cường vào vấn đề nàyPhương pháp học tăng cường (reinforcement learning [9]) là một nhánh con củahọc máy (machine learning [10]) mà bản chất hoạt động theo cơ chế thử nghiệmnhiều lần và rút ra kinh nghiệm sau mỗi lần thử. Gần đây, học tăng cường đang trởnên khá nổi tiếng nhờ một số những kết quả đạt được như việc AlphaGo [11] thắngáp đảo các tuyển thủ thế giới trong cờ vây, OpenAI [12] chiến thắng các đội mạnhnhất trong trò chơi DOTA2. Trong phần này, chúng ta sẽ tìm hiểu về các kiến thứcnền tảng của học tăng cường từ đó đi đến một thuật toán nổi tiếng trong nhóm Trong phần lớn các phương pháp, giải thuật trong trí tuệ nhân tạo và đặc biệtlà trong học máy, ta thường hay mô hình hoá bài toán bằng các mô hình toán học.Do đó, đối với học tăng cường, việc mô hình hoá bài toán bằng toán học cũng làmột hướng tiếp cận khả dĩ. Cụ thể ở đây, ta sử dụng Quá trình quyết định Markov(Markov Decision Process [14]) để mô hình hoá bài toánMarkov Decision Process (MDP) cung cấp một nền tảng toán học cho việc môhình quá việc đưa ra quyết định trong các tình huống mà kết quả là một phần ngẫunhiên và một phần dưới sự điều khiển của người ra quyết định. Tại mỗi bước thờigian, quá trình này ở trong một trạng thái s, agent đưa ra quyết định có thể chọn bấtkỳ hành động a. MDP đáp ứng trong bước thời gian tiếp theo bằng việc đưa ta tớimột trạng thái mới s′và cho agent một điểm thưởng tương ứng Ra(s, s′).Xác suất mà agent đi tới trạng thái s′sau khi đưa ra hành động ađược tính bởihàm chuyển tiếp trạng thái Pa(s, s′). Do đó, trạng thái kế tiếp s′phụ thuộc vào trạnMột quá trình quyết định Markov là tập các vector 5 chiều (S, A, P (·,·), R(·,·), •Pa(s, s′)là xác suất mà hành động atrong trạng thái stại thời điểm tsẽ dẫ•Ra(s, s′)là điểm thưởng mà agent nhận được sau khi chuyển tiếp sang trạn•γ∈[0,1]là hệ số chiết khấu, đại diện cho sự khác biệt về giá trị của điểmthưởng nhận ngay lập tức và điểm thưởng nhận trong tương lai.Bài toán cốt lõi của MDP đó là tìm ra một nguyên tắc (policy) πmà xác địnhhành động a=π(s). Mục đích của bài toán là tìm được policy πtối ưu sao chTrong học tăng cường, điều tiên quyết mà ta hướng đến là tìm ra một policy tốiưu. Từ đó, agent có thể biết được nên đưa ra các hành động như thế nào trong từỞ đây, ta thấy πcó thể được tham số hoá bằng θ, điều này tương tự với việc cácmô hình trong các phương pháp học sâu được tham số hoá.Như đã đề cập, sau khi mô hình hoá bài toán bằng MDP, mục tiêu của ta là tìmra policy mà đem về cho ta lượng điểm thưởng tối Đối với trường hợp ngẫu nhiên, policy sẽ cho chúng ta một phân phối xác suấTrong phương pháp học tăng cường, ta muốn tìm được một chuỗi các hành độngmà dẫn đến cho ta một lượng điểm thưởng kỳ vọng lớn nhất hoặc là giảm tối đa phí(cost). Có nhiều cách để giải được vấn đề này:•Đánh giá mức độ tốt khi đạt được từng trạng thái nhất định hoặc là thực hiệncác hành động nhất định tại từng trạng thái, từ đó chọn hành động tốt nhấ•Sử dụng mô hình của môi trường để tìm ra hành động trả về điểm thưởng lớnnhất (ví dụ như việc tính trước các nước đi trong cờ vua khi ta đã biết rõ luật•Tìm ra một policy trực tiếp tối đa hoá lượng điểm thưởng có thể nhận đượSau đây ta sẽ tìm hiểu sơ qua về 3 phương pháp trênTrong bài toán này, ta biết trước được mô hình gồm các tính chất của môi trường,từ đó, ta có thể dự đoán được trạng thái tiếp theo khi ta đưa ra một hành động. Mộtví dụ điển hình cho bài toán này là cờ vua. Trong cờ vua, với mỗi trạng thái slà thếcờ hiện tại, khi ta đưa ra hành động a, một nước đi nào đó, ta có thể biết được trạĐiểm cốt lõi của học tăng cường dựa và mô hình là sử dụng các tính chất đã biếtcủa môi trường, cùng với đó là xây dựng các hàm giá trị để tìm ra một chuỗi cáctrạng thái và thành động (trajectory τ) tối ưuTrong một số trường hợp, dù biết đầy đủ các tính chất của môi trường, ta vẫnkhông thể tự mình tìm được các hành động tối ưu. Đặc biệt là các trường hợp màcác khả năng có thể xảy ra ở mỗi trạng thái rất lớn. Thay vì thế, ta có thể chuyểnqua đánh giá giá trị của việc đạt được các trạng thái cụ thể, từ đó mà tìm ra cáchành động tương ứng nhờ vào mô hình của bài toán. Sự đánh giá này có thể thựchiện thông qua hàm giá trị (value function) Vπ(s)Hàm giá trị Vπ(st)tính toán lượng điểm thưởng kỳ vọng đã chiết khấu cho trạngtháisttại thời điểm tdưới policy hiện tại. Ta cũng có thể hiểu là đơn giản là đây làmột hàm có tác dụng đánh giá và dự đoán lượng điểm thưởng mà ta có thể nhật khiđạt một trạng thái và tiếp tục sử dụng policy hiện tại cho các hành động và tVí dụ như trong cờ vua, sẽ có những nước đi mà ở đó ta có lợi thế lớn so với khita chuẩn bị thua, thông thường khi theo một policy tối ưu thì hàm giá trị cho nướcđi đó sẽ trả về giá trị lớn hơn so với khi ta sắp thua.Khi ta có được một hàm giá trị hợp lý, có một số cách để ta tìm được một policytối ưu tương ứng. Tại mỗi bước thời gian, ta sử dụng V(s)để tìm ra trạng thái tốiưu trong bước thời gian tiếp theo mà ta muốn đạt tới, sau đó ta sử dụng các thôngtin tính chất của môi trường để từ đó đưa ra được lựa chọn tốt nhất.Đối với cách bài toán đã biết rõ mô hình thì đây là một điều dễ dàng, tuy nhiêntrong trường hợp ta không biết rõ các tính chất của môi trường, việc tìm ra lựa chọntương ứng để đưa ta tới trạng thái tối ưu tiếp theo mà ta tìm được là rất khóTrong một số môi trường, số lượng trạng thái là rất lớn. Điều đó dẫn đến việclưu trữ giá trị cho từng trạng thái trở thành một việc bất khả thi. Để giải quyết vấnđề này, ta có thể ứng dụng các kết quả đã đạt được trong học giám sát (supervisedlearning) và huấn luyện một mô hình có khả năng ước lượng •ylà giá trị mục tiêu và ta có thể dùng các phương pháp như Monte Carlo [1Vấn đề của hàm giá trị thông thường cho từng trạng thái là với việc tìm ra trạngthái chính xác, đôi khi ta vẫn không tìm được các hành động tương ứng dẫn ta tớitrạng thái đó. Đặc biệt là ở trong các môi trường mà ta không biết rõ các tính chấtĐể giải quyết vấn đề này, ta có thể tính toán giá trị cho từng hành động Q(s, a)thay vì cho mỗi trạng thái của môi trường. Khi này, chúng ta sẽ cần phải theo dõilượng dữ liệu lớn hơn (có nhiều action tại mỗi state), tuy nhiên, khi ta tìm đượcmột hàm giá trị hành động đủ tốt, ta có thể từ đó mà cho ra ngay lập tức hành độngta cần chọn. Điều này giúp cho ta không còn phụ thuộc vào các tính chất của môKhi ta đã có được một hàm Qđủ tốt, ta tìm hành động tối ưNhư vậy, ta có thể nói phương pháp học hàm giá trị hành động là không pTrong deep learning, gradient descent [17] thường hoạt động tốt nếu các giá trịcủa chúng ta được cân bằng hoá xung quanh 0. Ý tưởng này cũng được triển khaitrong học tăng cường. Hàm lợi thế là một hàm có tác dụng đánh giá lợi thế của mộthành động atrong trạng thái sso với mức điểm thưởng kỳ vọng thông thường ta•Vπ(st)là lượng điểm thưởng kỳ vọng mà ta có thể đạt được từ trạng thái st•Qπ(st, at)là lượng điểm thưởng kỳ vọng mà ta có thể đạt được nếu thực hiệnhành động attrong trạng thái stvà làm theo policy từ đó trở đi.•Aπ(st, at)là lượng lợi thế khi ta thực hiện hành động attại trạng thái stvà lEpsilon-greedy [3] là một phương pháp giúp ta cân bằng được việc khám phámôi trường và sử dụng policy. Việc khám phá môi trường có mục đích để có thểbao quát được toàn bộ không gian trạng thái và hành động, tránh cho việc agent bịoverfit vào một ngữ cảnh cụ thể. Trong khi đó, việc sử dụng policy là mục đích saucùng của ta khi đã có một policy tối ưu, từ đó các hành động của agent không cònlà các hành động ngẫu nhiên mà trên thực tế là có giá trị giúp cho ta đưa ra đượCách hoạt động của phương pháp này khá đơn giản, tại mỗi bước thời gian t,agent sẽ có thể chọn giữa việc ra quyết định ngẫu nhiên (explore), hoặc sử dụngpolicy để ra quyết định (exploit). Điều này được quyết định thông qua hệ số ϵ. Khigiá trị epsilon sẽ quyết định việc agent chọn explore hay exploit môi trường. Cùngvới đó giá trị này cũng sẽ thay đổi theo thời gian, từ đó càng về sau thì tỉ lệ exploitcàng cao. Sau cùng agent sẽ đưa ra quyết định hoàn toàn theo policy mà không tiVới những môi trường mà không gian trạng thái và hành động nhỏ, ta có thể lưutất cả các giá trị Qcho từng cặp trạng thái và hành động và từ đó tạo ra một bảng Qtable lưu các giá trị này. Khi đó, ta sử dụng thuật toán Q-learning [3] đơn giản vớiQ-table để tính toán các giá trị Q, thuật toán được trình bày trong mục thuật toán Tham số: kích thước bước α∈(0,1], giá trị ϵ Với xác suất ϵ, chọn một hành động ngẫu nhiên atThực hiện hành động at, theo dõi điểm thưởng rt, trạng thái mới st+Cập nhật Q(st, at)←Q(st, at) +α[rt+γmax aQ(st+1, a)−Q(st,Bằng việc liên tục tương tác với môi trường và lấy được nhiều mẫu, thuật toángiúp ta cập nhập liên tục được các giá trị trên Q-table. Khi Q-table có các giá trịchính xác, ta có thể sử dụng nó để giúp ta đưa ra các hành động (ví dụ ở trạng tháttrong bảng Q-table mà cho ta giá trị lớn nhấtCó những trường hợp môi trường có không gian trạng thái và hành động lớn, takhông thể có đủ không gian để lưu được hết từng giá trị này vào trong một bảng nàođó. Deep Q Network [18] cung cấp giải pháp là sử dụng một mạng neuron networkđể thực hiện tính toán một cách gần đúng nhất các giá trị Q. Khi này, thuật toánQ-learning cần phải được thay đổi để phù hợp, được thể hiện ở trong mục thuậAlgorithm 2 Deep Q-learning sử dụng kho kinh nghiệmKhởi tạo kho kinh nghiệm Dvới kích thước tối đa NKhởi tạo mạng giá trị hành động Qvới tham số ngẫu nhiVới xác suất ϵ, chọn một hành động ngẫu nhiên atThực hiện hành động at, theo dõi điểm thưởng rt, và trạng thái mới sLấy mẫu một lượng nhỏ các tập ở trong kho kinh nghiệThực hiện tối ưu hoá bằng gradient descent với (yj−Q(st, aj;θ1. Lấy các mẫu hành động từ một trạng thái, có thể sử dụng các phương phápkhác nhau để tăng tính ngẫu nhiên cho các mẫu, từ đó giúp cân bằng được s2. Theo dõi lượng điểm thưởng nhận được từ môi trường và trạng thái môi trườn3. Sử dụng hàm Qđể tìm hành động a′đem lại cho ta giá trị QlớBên cạnh học tăng cường dựa vào mô hình và giá trị, còn một nhánh các nhómphương pháp học tăng cường nữa là nhóm học tăng cường dựa vào policy. Cácphương pháp này tập trung vào việc trực tiếp tối ưu hoá policy πthay vì tìm policymột cách gián tiếp thông qua việc tối ưu hàm giá trị hay dựa vào các bộ luật cụ thHàm mục tiêu của các phương pháp học tăng cường dựa vào policy gr•J(θ)là hàm mục tiêu, là giá trị điểm thưởng kỳ vọng mà ta có thể đạt được vt=0R(st, ut)là tổng các điểm thưởng của từng cặp trạng thái và hành độτP(τ;θ)R(τ)là diễn giải cho vế trái, ta lấy tổng của các tích điểm thưởngcủa một chuỗi hành động và trạng thái τnhân với khả năng mà chuỗi đó xảMục tiêu của ta là tìm được bộ tham số θcho policy πθ, sao cho từ policy này tatìm được chuỗi τmà cho ta lượng điểm thưởng lớn Việc tối ưu hoá bài toán trên cần ta phải sử dụng một số biếTa tính gradient để sử dụng cho việc tốTuy ta không thể tìm được toàn bộ các giá trị τđể tính được gradient của policy,sau khi biến đổi như trên, ∇θJ(θ)trở thành một kỳ vọng, do đó ta có thể dùng việclấy mẫu (sampling) để tính toán xấp xỉ giá logp(s1)vlogp(st+1|st,at)không phụ thuộc vào θnên ta có thể bỏTa dùng policy gradient đã tính toán được này để tối ưu hoá policyThuật toán REINFORCE [19] sử dụng Monte Carlo để tính toán các giá trị điểmthưởng một cách chính xác. Quá trình này được thực hiện qua việc chạy các chuỗitương tác hoàn chỉnh đến khi môi trường đạt trạng thái kết thúc, sau đó mới tiếnhành tính các giá trị điểm thưởng trả về thông qua Monte Carlo. Các giá trị nàyđược sử dụng để tính toán policy gradient. Thuật toán được trình bày tronChọn giá trị chiết khấu điểm thưởng 0< γ <Chọn số lượng tập cho một lần cập nhật mô hình Tạo một chuỗi hoàn chỉnh các bước chuyển τtrong các tập tuân thủTính giá trị mất mát policy cho toàn bộ các tập trong bộ Ta có thể sử dụng hàm lợị thế Agiống ý tưởng khi sử dụng nó trong Q-leaBằng cách này, ta cân bằng được các giá trị điểm thưởng và giúp cho việc họb, Vấn đề của các thuật toán Policy GradientCác phương pháp học tăng cường dựa vào policy gradient được gọi là cácphương pháp học on-policy. Lí do là để tính policy gradient cho πthì ta cần thucác dữ liệu được thực hiện bằng chính policy π. Điều này dẫn đến việc sử dụng dữliệu không hiệu quả do toàn bộ lượng lớn dữ liệu sẽ chỉ được dùng cho một bưĐể cải thiện vấn đề sử dụng dữ liệu không hiệu quả, importance sampling [20]cho phép ta tính toán lượng điểm thưởng kỳ vọng bằng một policy vĐiều này hoàn toàn đúng với kết quả mà ta có được vềNếu ta có thể ràng buộc sự thay đổi của policy, ta có thể bỏ Với việc thêm một ràng buộc rằng policy mới và policy cũ không được cáchnhau quá xa, ta có thể sử dụng các thông tin thu thập được một cách hiệu quả hơnPhương pháp Proximal Policy Optimization [13] (PPO) được phát triển bởiOpenAI và đạt được những kết quả rất ấn tượng, thể hiện được giá trị ứng dụng củahọc tăng cường trong thực tế. Ý tưởng của phương pháp này là sử dụng importancesampling, nâng cao sự hiệu quả của việc sử dụng dữ liệu.Trong đó, ta lưu giữ 2 policy πθ(at|st)là policy mà ta muốn tối ưu hoá, πθk(at|st)là policy cuối cùng gần nhất mà ta dùng để thu được các chuỗi dữ liệu mà MDP trảvề. Sử dụng importance sampling, ta tối ưu hoá policy mới bằng dữ liệu thuTuy nhiên, khi ta cập nhập policy mới trong mỗi bước tối ưu, khoảng cách giữa2 policy ngày càng xa, sự ước lượng của importance sampling ngày càng lớn. Dođó, sau một số bước tối ưu hoá policy mới, ta lại phải cập nhật lại policy tương Để giải quyết vấn đề này, ta thêm vào bước giới hạn sự thay đổi policy. Ta giớihạn điểm lợi thế tính được nếu khoảng cách giữa 2 policy là quá xa. Khi Chi tiết thuật toán được trình bày trong mục thuật toán 4.Algorithm 4 Proximal Policy Optimization với hàm mục tiêu bị chặnĐầu vào: các tham số khởi tạo của policy θ0, giới hạn chặn ϵ,số bước cập nhật policy với một bộ các kinh nghiệm thu đượThu thập các chuỗi chuyển trạng thái Dktrên policy πk=π(θktsử dụng các phương pháp tính toán lợibằng Kbước cập nhật các bộ kinh nghiệm thNếurt(θ)nằm ngoài khoảng (1−ϵ)và(1 + ϵ), giá trị này sẽ được chặn lại tại2 giá trị trên. Ta cũng cần hiểu rằng ở đây, giá trị rt(θ)càng lớn càng có nghĩa làhành động hiện đang được chọn có xác suất được chọn cao hơn trên policy hiện tạiso với policy cũ. Khi đó có một vài trường hợp mà ta cần chú ý.1.A > 0, rt(θ)≥(1 +ϵ): Khi này hành động được chọn được đánh giá là tốt hơnso với kỳ vọng. rt(θ)lớn đồng nghĩa với việc hành động này đang sẵn có khảnăng được chọn cao hơn nhiều so với policy trước. Đây là điều tốt, tuy nhiên,vì ngay thời điểm này nó đã đang làm khá tốt rồi, ta không muốn model tiếptục học theo hướng này quá nhiều vì điều đó có khả năng dẫn đến việc bướquá xa và dễ dẫn đến một bước cập nhật policy quá mức, dẫn đến hỏng quátrình huấn luyện. Do đó ta giới hạn giá trị này lại ở mức (1 +ϵ).2.A > 0, rt(θ)≤(1 + epsilon ): Cùng cách đánh giá như trên, khi này ta thấypolicy mới của ta tiếp tục cần phải học theo hướng này (tăng khả năng chọnhành động atrong trạng thái shiện tại). Do đó hàm mục tiêu không bị chặnkhi mà với rt(θ)<(1−ϵ)thì phần đầu của hàm minsẽ được chọn, giúp chobước cập nhập policy được thực hiện trọn vẹn.3.A < 0, rt(θ)<(1−ϵ): Tương tự với ý tưởng của trường hợp 1 nhưng ngượchướng, việc này giúp cho agent có thể tiếp tục cập nhập policy theo hướn4.A < 0, rt(θ)>1: Khi này, hành động này mang lại giá trị lợi thế âm, do đópolicy hiện tại nên bước cập nhật policy nên khiến cho xác suất hành độngnày được chọn tại trạng thái hiện tại giảm đi. Tuy nhiên ở đây, ta thấy nó lạiđang tăng lên so với bước cập nhật policy trước đó. Điều này chứng tỏ bướccập nhật trước đã bị sai. Điểm hay ở đây là vì khi này giá trị lợi thế đang âm,do đó vế phải của hàm minsẽ được chọn, cho phép agent thực hiện một bướccập nhật policy theo hướng ngược lại với lần cập nhật trước với một giá trịkhông bị chặn, điều này phần nào hoạt động như một cơ chế sửa sai sau khVậy là chỉ bằng một hàm mục tiêu duy nhất, đơn giản, không có nhiều ràngbuộc, phương pháp Proximal Policy Optimization giúp ta giải quyết được rất nhiềuđiểm yếu của các phương pháp phụ thuộc vào policy gradient truyền thống. PPOhoạt động một cách hiệu quả về mặt dữ liệu, có khả năng hội tụ tốt và tránh đượccác vấn đề liên quan đến cập nhật policy sai và phá hỏng quá trình huấn luyệUnity [21] cung cấp một engine trò chơi và một IDE bao gồm hệ thống tươngtác với môi trường (editor), hệ thống vật dụng (asset), hệ thống xây dựng cảnh, hệthống mạng và một số yếu tố khác. Và với những yếu tố trên, tất nhiên Unity 3D cóthể được sử dụng để làm nhiều việc, không chỉ bị giới hạn ở việc thiết kế trò chơi.Unity3D gồm 5 thánh phần chính là project view, scene view, game view, heirarcĐây là phần chính mà ta sử dụng trong quá trình phát triển ứng dụng. Sceneview cung cấp cho ta một cái nhìn tổng quát về ngữ cảnh của môi trường, trong đta có thể thấy các vật thể và vị trí của chúng. Scene view được thể hiện trong hìnScene view cho phép người dùng sử dụng các thao tác kéo thả để di chuyển vàđịnh hình các vật thể trong môi trường được thực hiện thông qua một số công cụnhư bộ công cụ dịch chuyển (translate tool) dùng để dịch chuyển các vật thể, bộcông cụ xoay (rotation tool) dùng để thay đổi độ quay của vật thể theo các trục, bộcông cụ tăng giảm kích thước (dùng để thay đổi kích thước của vật thể theo cáctrục), và một số công cụ khác. Bên cạnh đó thì scene view cũng cho phép ngườidùng nhìn được toàn thể các yếu tố của môi trường mà một số trong đó người chơikhông thể nhìn được, ví dụ như các thông tin về bộ khung vật lý của vật thể, haycác thông tin phục vụ cho việc sửa các lỗi trong quá trình phát triển trò chơiGame view là phần mà người chơi sẽ nhìn thấy khi tương tác với trò chơi đượctạo ra. View này cho phép người dùng có thể xem trước được góc nhìn của ngườichơi, từ đó có thể điều chỉnh các hành động của trò chơi cũng như các vật thể trontrò chơi một cách hiệu quả. Góc nhìn này được lấy thông qua camera chính của tròchơi được thiết lập vị trí ở trong scene view, qua đó ta có thể giới hạn không gianmà người dùng có thể nhìn thấy trong sản phẩm hoàn thiện, thay vì cho họ nhìntoàn bộ khung cảnh môi trường. Một ví dụ cho game view được thể hiện trong hìHierarchy view cho phép ta theo dõi toàn bộ các vật thể đang hiện hữu trongmôi trường theo dạng danh sách. Ở đây các vật thể được tạo ra sẽ có dạng cha vàcon. Các vật thể con sẽ được nằm trong cha và có thể được mở đóng thông qua cha.Trong heirachy view được thể hiện trong hình 2.3, ta có thể thấy tổng quát các vậtthể trong môi trường, trong đó các vật thể cha được gắn hình mũi tên ở bên trái, thểhiện nó có thể được mở để ta thấy các vật thể con trong đProject view 2.4 chứa toàn bộ các thông tin liên quan đến dự án hiện tại của ta,bao gồm các cảnh, đoạn mã nguồn, hay các mô hình đồ hoạ, và toàn bộ các dữliệu khác mà ta sử dụng trong dự án. Project view hoạt động gần tương tự như cácphần mềm duyệt tập dữ liệu trong máy tính của chúng ta, tuy nhiên một số thôngtin không cần thiết đã được lược bỏ. Do đó, view này cũng cho phép người dùngtạo, xoá tập dữ liệu và quản lý toàn bộ các tập dữ liệu này trong dự án một cách Inspector view 2.5 cho phép ta theo dõi toàn bộ thông tin về một vật thể. Nhữngthông tin này rất đa dạng tuỳ thuộc vào từng kiểu vật thể mà ta muốn theo dõi,trong đó có những thông tin mà vật thể nào cũng có như vị trí, độ quay, tỉ lệ. Mộtsố thông tin khác có thể có liên quan đến các yếu tố vật lý, cấu tạo màu sắc của vậtthể. Bên cạnh đó thì inspector view cũng cho ta thông tin và khả năng thay đổi cácthông tin được lấy từ trong các mã nguồn kịch bản gắn với vật thể.f, Hệ thống mã nguồn xây dựng hành độngĐể có thể sử dụng Unity như một công cụ xây dựng môi trường, ta cần sử dụnghệ thống mã nguồn để lập trình từng hành động của các yếu tố trong môi trường.Các thao tác này được thực hiện nhờ vào hệ thống xây dựng các hành động của vậTrong Unity, mỗi vật thể đều có thể được gắn một thành phần cấu tạo là mộthoặc nhiều tập mã nguồn C #. Mã nguồn này sẽ quản lý các hành vi của vật tromôi trường theo yêu cầu của người lập trình. Trong đó, vật thể của ta sẽ kế thừalớpMonoBehavior , đây là một lớp được thiết kế để thực hiện các công việc nhưhiển thị vật, thực hiện các sự thay đổi của vật do ta lập trình. Để cho lập trình viêncó khả năng này, lớp này cũng có một số hàm mà ta có thể ghi đè để tuỳ biến hàn•Update : Hàm được gọi liên tục sau đã hoàn thành các thao tác của lần gọitrước. Số lần mà hàm được gọi trong một khoảng thời gian phụ thuộc vào hiệunăng của máy và hiệu năng của phần chương trình được viết trong hàm.•FixedUpdate : Hàm được gọi liên tục với một tần xuất cố định. Giá trị mặcđịnh cho tần xuất này là 50 lần một giây và có thể được thay đổi bởi ngườidùng. Do đó hàm này có số lần gọi cố định trong một giây, điều này giúp tacó khả năng tính toán một cách chính xác hơn.•Start : Hàm được gọi khi object được khởi tạo, cho ta khả năng thay đổi cácthông tin trước khi hàm Update vàFixedUpdate được gọi.Bên cạnh đó thì Unity cung cấp một số API hữu dụng khác để thao tác với cácobject phục vụ các mục đích khác nhau như tạo vật thể và xoá vật thể khỏi mô2.6.2 Một số cơ chế thường dùng trong UnitNhận diện va chạm là nhận biết khi có hai vật thể lấn chiếm lên nhau. Điều nàyxảy ra khi hai vật có vị trí gần nhau hơn là mức kích thước của chúng cho phép.Trong Unity, cơ chế này thường được lập trình theo hướng sử dụng hệ thống khuKhung va chạm (hình 2.6) có thể hiểu là một bộ khung do người dùng tự địnhnghĩa cho mỗi vật thể, đây là yếu có tố thể thêm vào hoặc không tuỳ thuộc vào mụcđích của người lập trình viên đối với từng vật thể. Để sử dụng cơ chế này, Unitycung cấp API OnCollisionEnter trong mã nguồn của mỗi vật thể. Hàm nàycho phép ta thao tác với khung va chạm của vật thể mà đang va chạm với khungcủa bản thân. Từ đó, ta có thể lấy được hầu hết các thông tin của vật như vị trí, độquay, tỉ lệ và các thuộc tích công khai khác của vật thể va chạmMáy trạng thái hữu hạn (Finite State Machine [22]) (FSM) là một cách thiết kếmô hình thể hiện cách mà một vật thể trong môi trường sẽ hoạt động. Trong đó, vậtthể này sẽ hoạt động dựa vào một số hữu hạn các trạng thái khác nhau, trong đó, tạimỗi khung thời gian, vật thể này sẽ đối chiếu giá trị trạng thái hiện tại và với mỗitrạng thái sẽ đưa ra hành động cụ thể. Bên cạnh đó, việc xử lý thông tin để thay đổitrạng thái hợp lý hoàn toàn được tách biệt với quá trình hành động.FSM có thể được áp dụng vào việc thiết kế hệ thống lớn như quản lý môi trường,hay cũng có thể sử dụng trong từng object nhỏ trong môi trường như là mỗi xe. Mộtví dụ về FSM được thể hiện trong hình 2.7 với các trạng thái là xanh, vàng và đỏ.Trong đó, mỗi trạng thái sẽ có cơ chế chuyển trạng thái khác nhau, ví dụ từ đènxanh khi hết thời gian sẽ chuyển sang vàng, khi hết thời gian vàng thì chuyển quađỏ. Bên cạnh đó, một số trạng thái nhất định sẽ chỉ chuyển được tới một vài trạngthái nhất định khác, ví dụ như đỏ không thể chuyển thành vàng. Một điểm quantrọng khác là cơ chế tác động của đèn lên môi trường không phụ thuộc vào phần cơchế thay đổi trạng thái. Tại mỗi khung thời gian của môi trường, hệ thống chỉ kiểmtra trạng thái hiện tại và hoạt động theo logic của trạng thái đóHệ thống đếm giờ là phương pháp được sử dụng khá nhiều trong Unity. Phươngpháp này được sử dụng khi ta cần thực hiện một việc nào đó sau một khoảng thờgian cho trước. Các ví dụ có thể kể đến như các đồng hồ đếm giờ, đồng hồ đếmngược, hay hệ thống điểm thưởng theo giờ,... Chính vì việc có nhiều trường hợp sửdụng như vậy, Unity cung cấp cho ta một số giải pháp liên quan đến tính toán thờigian, một giải pháp được sử dụng nhiều nhất đó là API Time.deltaTime . Đâylà một phương thức tĩnh của lớp Time , cho phép ta trả ra khoảng cách giữa khungthời gian hiện tại và ngay trước đó, từ đó ta có thể sử dụng giá trị này phù hợp vớVới ví dụ về việc sử dụng hệ thống đếm giờ trong việc sinh xe trong một khoảngthời gian. Ta sử dụng một biến để tích trữ thời gian, khởi đầu là 0, sau đó tại mỗikhung thời gian ta tiến hành cộng nó với giá trị Time.deltaTime , và kiểm traxem nó đã vượt qua khoảng thời gian mà ta mong muốn chưa, nếu đã vượt qua thìta tiến hành thực hiện hành động cần thiết và chuyển lại giá trị tích trữ về Prefab là các vật thể được tạo trước, chính vì vậy hệ thống prefab ở đây là một hệthống cho phép ta tạo và quản lý các vật thể một cách bài bản. Có thể hiểu prefabnhư một bản mẫu cho một object được cấu tạo một cách hoàn chỉnh gồm bất kỳ yếutố nào mà lập trình viên mong muốn. Các prefab này sau đó được chuyển thành cácvật thể trong môi trường thông qua việc người dùng thêm chúng vào trong sceneview hoặc thông qua việc tạo chúng ở trong mã nguồn.Đối với việc tạo vật thể trong scene view, hệ thống này cho ta quyền quản lýcác vật thể này thông qua prefab của chúng. Ta có thể thay đổi một số thuộc tínhtrong mỗi vật thể trong môi trường một cách khác nhau, khiến chúng khác đi sovới prefab. Tuy nhiên khi cần, ta có thay đổi một số thông tin trong prefab vào ápdụng chúng vào toàn bộ các vật thể đang có trong môi trường. Việc này giống vớicơ chế kế thừa trong lập trình hướng đối tượng.Để tạo các vật thể trong môi trường từ những prefab này thông qua lập trình mãnguồn, Unity cung cấp API Instantiate . API này nhận vào một prefab và mộtsố thông tin như vị trí tạo, độ quay, và vật thể cha, sau đó tạo vật thể này trong môitrường và trả về chính nó. Từ đó, ta có thể thay đổi một số thuộc tính công khai củavật thể phù hợp với mục đích của mình. Cơ chế tạo vật thể cho phép ta tự động hoáquá trình tạo vật thể trong môi trường, rất hữu dụng trong một số trường hợp Bộ công cụ Unity ML-Agents [23] cung cấp một SDK được phát triển bởi Unityvà được sử dụng trong các môi trường Unity để tạo ra một môi trường học máy. Bộcông cụ cho phép ta tạo ra một môi trường mô phỏng trong Unity có các yếu tố đhỗ trợ cho việc học máy, bên cạnh đó cũng cung cấp cho ta một hệ thống giao tiếpvới môi trường này thông qua các Python API. Các thành phần chính trong UnityML-Agents là cảm biến, các chủ thể hành động (agent), và một hệ thống học việUnity ML-Agents biến một môi trường thông thường trong Unity trở thành mộtmôi trường học máy. Trong đó, các agent hoạt động thông qua các bước thời gian.Trong mỗi bước thời gian, từng agent sẽ thu thập các thông tin về môi trường vàsử dụng chúng và policy tương ứng để tìm ra hành động tối ưu cho trạng thái hiệntại, đồng thời nhận về điểm thưởng cho từng hành động. Việc biến đổi các vật thểthông thường thành các agent được thực hiện thông qua việc kế thừa lớp Agentđược cung cấp bởi bộ công cụ. Lớp Agent cung cấp cho ta các chức năng mà lớpMonoBehavior có, đồng thời cung cấp thêm các logic liên quan đến môi trườnghọc máy và một số phương thức giúp ta tuỳ chỉnh phương thức tương tác của agentvới môi trường. Một số phương thức quan trọng mà ta cần thực hiện ghi đè để hoà•OnEpisodeBegin : Cho phép lập trình viên tuỳ biến các giá trị hay các hoạtđộng mỗi khi tập của môi trường được bắt đầu.•ResetEnv : Cho phép ta kết thúc tập hiện tại, có thể sử dụng kèm một số điềukiện nhất định để đạt được các cơ chế hoạt động mong muốn.•CollectObservations : Cho phép ta thiết lập các thông tin mà agentnhận được trong mỗi lần yêu cầu hành động, ở đây ta có thể hiểu là tương ứn•OnActionReceived : Trả về cho ta hành động của agent và cho phép tathực hiện thay đổi môi trường tương ứng với hành động đóBên cạnh đó, ta cũng được quyền thay đổi một số thông tin liên quan đến hànhvi của agent thông qua một số thuộc tính công khai của agent trong Inspector View2.5. Trong đó có một số thông tin quan trọng như:•Kích thước số chiều của các quan sát về môi trường•Số bước tối đa - số bước thời gian tối đa trong một tập môi trường, khi bướcthời gian đạt giới hạn này, môi trường sẽ tự động thực hiện ResetEnv màkhông cần chờ điều kiện mà lập trình viên thiết kế.Hệ thống học viện (academy) sẽ là thành phần thực hiện quản lý các agent. Cácchức năng của học viện bao gồm theo dõi các bước thời gian cho từng agent vàquản lý các thông tin liên quan đến agent. Trong quá trình huấn luyện, học việnsẽ đóng vai trò giao tiếp với các mã nguồn huấn luyện trong Python và chạy mộtloạt các tập của môi trường khác nhau. Trong mỗi tập này sẽ bao gồm nhiều bướcthời gian, tại đó học viện tổng hợp các quan sát từ agent và gửi qua mã nguồn huấnluyện, nhận lại các hành động tương ứng và gửi chúng lại cho từng agent.Các mã nguồn huấn luyện trong Python không phải là một phần của môi trường.Các mã nguồn này chứa các thuật toán học máy khác nhau để có thể huấn luyệnagent trông môi trường học. Python API là một giao thức giữa ứng dụng huấn luyệnvà hệ thống giao tiếp. Hệ thống giao tiếp nằm ở trong môi trường và có tác dụnCác thành phần trong bộ công cụ ML-Agents được thể hiện trong hình 2.Trong chương này, em đã trình bày về các lý thuyết và kỹ thuật cần có để cóthể thực hiện giải pháp được đề xuất. Các phương pháp tối ưu dựa vào mô hìnhcó điểm mạnh là sự đơn giản nhưng lại phụ thuộc hoàn toàn vào môi trường bàitoán. Phương pháp tiên tiến hơn là Deep Q Network đã khắc phục điểm yếu trênnhưng vẫn tồn tại một số vấn đề về tính tối ưu do không trực tiếp tối ưu hoá policy.Phương pháp Proximal Policy Optimization thể hiện khả năng tối ưu tốt và tốc độhội tụ nhanh, phù hợp với phần lớn các bài toán hiện nay. Bên cạnh đó, ta cũng tìmhiểu về các kĩ thuật cơ bản trong Unity và gói công cụ Unity ML-Agent, từ đó giúpta có khả năng phát triển một môi trường hoàn thiện và kết nối tới các thuật toánhuấn luyện. Những kiến thức này sẽ được sử dụng trong chương tiếp theo để pVới những kiến thức nền tảng được trình bày từ Chương 2, em tiến hành áp dụngchúng vào việc phát triển môi trường và sử dụng phương pháp Proximal PolicyOptimization để giải bài toán trên môi trường đã được xây dựng. Trong chươngnày, em sẽ trình bày về các gói hỗ trợ, thư viện được sử dụng, quá trình phát triểnmôi trường và ứng dụng PPO để tìm được một policy tối ưu cho môi trường đóTrong đồ án này, em đưa ra giải pháp sử dụng Unity và bộ công cụ ML-Agentsđể xây dựng một môi trường học cho bài toán hệ thống đèn giao thông thông minh.Em sử dụng phương pháp học tăng cường và cụ thể là giải thuật Proximal PolicyGradient, tương tác với môi trường đã tạo thông qua các Python API được cungcấp, từ đó xây dựng được một policy hợp lý và tối ưu, giúp cho agent đưa ra đượccác quyết định về tín hiệu đèn giao thông một cách hợp lý.3.3 Xây dựng ngữ cảnh môi trường trong UniĐể có thể phát triển được một môi trường chân thực, giống với thực tế, em sửdụng gói đồ hoạ giao thông đơn giản được mua ở trên hệ thống cửa hàng của Unity.Gói này cung cấp cho ta các mô hình đồ hoạ chân thực liên quan đến giao thôngnhư đường, vạch phân cách, xe cộ,... Với các mô hình đồ hoạ có sẵn này, ta có thểbỏ qua bước thiết kế đồ hoạ để giảm thiểu được thời gian phát triển môi trường,phù hợp với phạm vi của một đồ án. Một số các mô hình đồ hoạ được gói này Để có thể định hướng di chuyển của các phương tiện trong môi trường mộtcách chính xác và giống với thực tế, em sử dụng gói hỗ trợ tạo đường đi là PathCreator . Tác dụng của gói hỗ trợ này là tạo cho ta các đường cong và cung cấpcho ta một số API để lấy được điểm nằm trên đường cong trên gần nhất với mộtđiểm mà ta cần tìm. Từ đó ta có thể sử dụng thông tin này vào việc di chuyển củacác phương tiện, giúp các phương tiện của ta có thể di chuyển trong môi trườngmột cách hợp lý, góp phần giúp cho môi trường trở nên thêm chân thực và thể hiệnđược đúng các tính chất của việc di chuyển ngoài đời thVới bài toán đặt ra là hệ thống đèn giao thông tại một ngã tư đơn giản, em đãxây dựng một ngã tư có với 4 hướng đường hai chiều, trong đó mỗi chiều có 2 lànđường xe chạy. Cụ thể phần nền của môi trường được trình bày như hình 3.2. Nhưcó thể thấy trong hình, môi trường sẽ bao gồm 4 hướng đi giao nhau tại một ngã tư.Ở đây, mỗi hướng đi sẽ có một đèn giao thông tương ứng quyết định việc có cho xedi chuyển hay không. Hệ thống bao gồm 4 đèn giao thông này sẽ đóng vai trò quảnlý việc lưu thông của các phương tiện giao thông trong ngã tư này, đồng thời cũngchính là đối tượng mà ta đang hướng tới để tối ưu hoá các quyết định.Trước hết, em sử dụng các mô hình đường để tạo ra các làn và sau đó là thêmvào các hình giải phân cách để phân chia làn đường. Tiếp đó là thêm vào phần vỉahè để hoàn thiện mô hình một làn đường đơn giản. Từ đó ta có thể lặp lại các bướctrên và xây dựng được mô hình nền đường hoàn chỉnh.Để các xe có thể di chuyển trên môi trường, ta tạo các đoạn đường lộ trình màcác xe sẽ chọn để đi theo, các đoạn đường này sẽ được lưu lại và trong thao tác tạora xe và thêm vào môi trường, các xe đồng thời cũng sẽ được gắn cho một lộ trìnhcụ thể để đi theo, từ đó đảm bảo được tính đúng đắn của môi trường. Các đoạnđường được thêm vào môi trường được thể hiện bởi các đường màu xanh troĐể tạo được một hệ thống phương tiện hợp lý phù hợp với nhu cầu của bài toán,ta cần phải hiểu là đây không thể là các object cố định luôn hiện hữu trong môitrường được, ta cần liên tục sinh ra vật thể và xoá chúng đi sau khi đã đi ra khỏimôi trường. Bên cạnh đó thì mỗi cá thể phương tiện sẽ là yếu tố chịu trách nhiệmcho việc đảm bảo các yêu cầu của môi trường giao thông, các cơ chế tránh va cĐể đảm bảo môi trường có tính bao quát, thể hiện được toàn bộ các tình huốngcủa có thể xảy ra của môi trường thực tế, em sử dụng một số thông số để kiểm soá•Giãn cách tạo xe: Khoảng cách giữa 2 lần tạo xe và thêm vào môi trường,việc này giúp ta có thể kiểm soát được mật độ xe nói chung trên toàn bộ mô•Thứ tự ưu tiên của các làn: Độ ưu tiên dành cho từng làn đường khác nhau, lànnào có độ ưu tiên cao thì khả năng xe được sinh ra trên làn đó càng cao. Điềunày giúp ta có thể tạo được những tình huống khác nhau của giao thông hiệ•Độ chênh lệch của các mức ưu tiên: Bên cạnh thứ tự ưu tiên, ta cũng dùng cáctham số để kiểm soát độ chênh lệch của các mức ưu tiên với nhau, kết hợp vớisự thay đổi thứ tự ưu tiên giúp ta tạo được một môi trường đa dạng, mô phỏngđược các trường hợp có thể xảy ra ngoài thực tế.Bằng việc tạo ra và kiểm soát, thay đổi các thông số trên liên tục, ta có thể tạora một bộ mô phỏng hoàn thiện, ứng với môi trường đề ra. Tương ứng với mỗi tậpcủa môi trường học, hiện tại đang là 5 phút, các thông số trên sẽ được thay đổi saocho vừa đủ để có thể hoàn thành được một bộ trạng thái. Cụ thể, với mỗi tập củ•Giãn cách tạo xe sẽ thay đổi từ 0.5s tới 1.5s. Điều này khiến cho lượng xe trêntoàn bộ môi trường được thay đổi. Ở tập tiếp theo của môi trường, sự thay đổinày sẽ ngược lại, tức là từ 1.5s tới 0.5s. Điểu này là để đảm bảo hoạt động tốtvới các thông số thứ tự ưu tiên lẫn mật độ của các mức ưu tiên.•Thứ tự ưu tiên giữ nguyên để tạo môi trường với ưu tiên đó, và thay đổi khi t•Độ chênh lệch của các mức ưu tiên sẽ liên tục chuyển từ (0.25, 0.25, 0.25,0.25) thành (0.55, 0.2, 0.15, 0.10) trong quá trình 5 phút. Tức là trong 5 phútnày ta liên tục có các độ chênh lệch khác nhau của môi trường thoả mãn vớVới phương pháp trên, sau 24 tập của môi trường, ta có được đầy đủ các thứ tựưu tiên khác nhau của môi trường, và với mỗi thứ tự ưu tiên khác nhau, ta lại có cácmật độ khác nhau cho các mức ưu tiên. Kết hợp với giãn cách tạo xe, sau 48 tậpcủa môi trường, ta có thể có được toàn bộ các trạng thái về mặt lưu lượng ở từSau khi đảm bảo được tính bao quát trên, tiếp đến là bước thực hiện sinh xe. Vớiviệc các mô hình xe khác nhau được thiết kế hoàn thiện, sau mỗi khoảng giãn cáchđược định nghĩa ở trên, ta tiến hành tạo ra xe, đồng thời gắn cho mỗi xe một lộtrình. Việc lựa chọn lộ trình là một quá trình ngẫu nhiên có kiểm soát, được địnhđoạt thông qua các thông số về thứ tự ưu tiên và mật độ của các mức ưu tiên. Việclựa chọn lộ trình sẽ giúp cho ta có thể sử dụng được các thông số trên để kiểm soáĐể đảm bảo không xảy ra hiện tượng các xe khi sinh ra bị chèn vào nhau, saukhi ta tìm được một lộ trình, ta sẽ tạo ra một điểm ngẫu nhiên gần với đầu của lộtrình, điểm này sẽ được coi là điểm xuất phát. Tuy nhiên, ta cần kiểm tra xem khuvực xung quanh điểm xuất phát này có phương tiện nào đang lấn chiến không, nếucó ta sẽ cần chọn lại một điểm xuất phát mới. Sau khi đã tìm được một điểm xuấtphát đảm bảo có thể sinh xe an toàn, công việc sinh xe mới được diễn ra thông quBên cạnh việc liên tục tạo ra các xe, để đảm bảo môi trường được diễn ra trơntru, ta cũng cần tiến hành xử lý các vấn đề liên quan đến tương tác của các xe vớinhau. Ở đây, để đảm bảo được hiệu năng của môi trường, tránh việc mô phỏng quánặng, sau khi xe đi hết khỏi phần đường trước đèn đỏ, các cơ chế tránh va chạmcủa xe sẽ được giảm thiểu. Điều này có thể được thực hiện là do đối với bài toánhiện tại, ta chỉ quan tâm về phần di chuyển trước đèn đỏ của xe. Thêm vào đó, tasẽ tiến hành xoá xe khỏi môi trường khi xe đi tới điểm kết thúc của lộ trình.b, Hệ thống hoạt động của các phương tiệnCác xe trong môi trường hoạt động theo cơ chế đi theo một lộ trình đã có. Cáclộ trình này đã được gán cho mỗi xe trong quá trình tạo ra xe trong môi trường, làyếu tố chính quyết định sự di chuyển của xe. Bên cạnh đó, ta có thêm thông số vềvận tốc, giúp ta kiểm soát được tốc độ xe di chuyển theo lộ trình định sẵn.Để cho việc xe di chuyển trong môi trường một cách hợp lý và đồng thời có đượccác cơ chế tránh va chạm, các xe sẽ hoạt động theo cơ chế máy trạng thái hữu hạ•DRIVE: di chuyển thông thường với vận tốc tối đa đã định nghĩa.•RED_LIGHT_STOP: vào trạng thái dừng do gặp đèn đỏ.•VEHICLE_COLLISION_STOP: vào trạng thái dừng do gặp phương tiện.•ACCELERATE: vào trạng thái tăng tốc để chuyển từ trạng thái dừng sang dQuá trình thay đổi trạng thái này được thực hiện thông qua một cảm biến đượgắn ở đầu xe, cảm biến này cho ta biết trong đoạn thẳng trước mặt có vật thể nàokhông và đồng thời biết được phân loại của object đó. Việc này được thực hiệnthông qua hệ thống RaycastHit của Unity, cho phép ta kiểm tra thông tin trênmột đoạn thẳng trước mặt. Trong trường hợp có vật cản được nhận diện trongkhoảng đoạn thẳng trước mặt, ta có thể biết được khoảng cách của vật cản, đồngthời thay đổi trạng thái của xe nếu các điều kiện tránh va chạm thoả mãn:•Nếu khoảng cách đủ nhỏ và phân loại của vật cảm phía trước là đèn giao thôn•Nếu khoảng cách đủ nhỏ và phân loại của vật cản phía trước là phương tiệ•Nếu phía trước không có vật cản và trạng thái hiện tại đang là một trạng tháidừng, lúc này có nghĩa là xe đang dừng và cần bắt đầu di chuyển, ta chuyể•Nếu phía trước không có vật cản và vận tốc hiện tại đang đạt vận tốc tối đa, tachuyển trạng thái hoặc giữ nguyên trạng thái là DRIVE.Với các cơ chế trên, ta có thể đảm bảo xe lưu thông theo đúng những luật lệ đềra, phù hợp với thực tế. Đồng thời ta cũng có thể đảm bảo được quá trình dừng đènđỏ của xe khi kết hợp với việc bật tắt đèn để tạo ra các vật cản ảo.Khi xe dừng lại do đèn đỏ hoặc do một xe khác đang chịu tác động của đèn đỏ,ta sẽ lưu lại các giá trị về thời gian dừng, giá trị này sẽ có tác dụng đánh giá độ hiệuquả của tín hiệu đèn giao thông, từ đó đưa ra các mức điểm thưởng phù hợpHệ thống đèn giao thông cho môi trường một ngã tư đơn giản bao gồm 4 đèngiao thông tương ứng với 4 hướng đi của ngã tư, được kiểm soát bởi một vật thểcha đóng vai trò quản lý hoạt động của 4 đèn này.Cách hoạt động của hệ thống này là tại các vị trí đèn đỏ, phần khung va chạmcủa vật thể đèn giao thông đó sẽ được chuyển trạng thái thành bật. Việc này sẽ gópphần tạo thành một vật cản vô hình mà các xe có thể thấy được thông qua cảm biếnđặt phía trước xe. Để đạt được điều này, ta thêm mã nguồn C #vào vật thể quảnlý hệ thống đèn, cho nó quyền thao tác với 4 đèn giao thông tương ứng. Trong đó,mỗi khi chuyển trạng thái đèn, ta cần thay đổi các thông số của cả 4 đèn trong hệthống trên. Đồng thời, ta cần có thay đổi để có thể dễ dàng quan sát tín hiệu đènhơn, do khi đặt hệ thống máy quay lên trên cao thì mô hình đèn trở nên khá nhỏ vàkhó quan sát. Để khắc phục vấn đề này, ở mỗi đèn ta thêm một khối cầu đơn giảncó tác dụng thể hiện màu đèn. Sự thay đổi màu của khối cầu cùng đồng thời đượthực hiện cùng với việc thay đổi trạng thái đèn, đảm bảo cung cấp được các thônCác logic trên được chuyển hoá thành một API, từ đó bất kỳ vật thể nào có quyềntruy cập API này đều có khả năng thay đổi trạng thái đèn. Điều này giúp cho agentcủa ta có khả năng thực thi hành động, tạo ra các tác động trên môi trường, từ đócó thể tạo thành một môi trường huấn luyện hoàn chỉnh.3.4 Cấu hình Markov Decision Process cho môi trườngCấu hình Markov Decision Process cho môi trường hiện tại bao gồm một sốbước cơ bản như định nghĩa một tập (episode) của môi trường, định nghĩa trạngthái (state) của môi trường, định nghĩa hành động (action) của agent, tạo quy luậtđiểm thưởng. Với nhu cầu của từng bài toán khác nhau, các định nghĩa và quy địnhtrên có thể được thay đổi để phù hợp với bài toán đó. Do đó, việc định nghĩa mộtcách chi tiết các yếu tố của MDP trong bài toán hiện tại là việc quan trọng hàĐối với bài toán hiện tại, môi trường được đặt ra là một mô phỏng ngã tư đơngiản với 4 hướng đi, mỗi hướng đi bao gồm 2 làn đường. Giãn cách đưa ra quyếtđịnh của agent là 5 giây, giúp cho việc quy định ý nghĩa của hành động được rõràng hơn. Ví dụ, ta có thể thấy, nếu ta đưa ra quyết định mỗi khung thời gian vớikhoảng 50 khung thời gian một giây thì từng quyết định một sẽ không đáng kể vàphải lặp rất nhiều lần để tạo thành một chuỗi thời gian đèn xanh hoặc đỏ, điều nàyảnh hưởng rất nhiều tới việc học của agent. Trước mỗi khi đưa ra quyết định, agentsẽ nhận được thông tin trạng thái của môi trường. Ở trong bài toán hiện tại, trạngthái của môi trường được thể hiện thông qua thông tin về các cảm biến đặt trênđường, chúng sẽ trả về 1 nếu có xe đang đứng ở trên và 0 trong trường hợp ngượclại. Cùng với đó, thời gian dừng tích luỹ của các phương tiện trong khoảng thờigian 5 giây trước đó được tính vào làm điểm thưởng cho agent cho hành động trướĐể thực hiện các cấu hình này, ta cần thêm vào vật thể quản lý hệ thống đèn giaothông một tệp mã nguồn C #làTrafficLightAgent . Mã nguồn này sẽ kế thừalớpAgent được cung cấp bởi gói ML-Agents, giúp ta thực hiện việc quản lý môitrường và giao tiếp với agent từ mã nguồn trong pythonMỗi tập của môi trường sẽ bao gồm 64 bước. Điều này giúp cho mỗi tập sẽbao quát được một mức thứ tự ưu tiên của các phần đường, giúp cho việc quản lmôi trường dễ dàng hơn. Mỗi tập sẽ bắt đầu bằng một môi trường trống không cóphương tiện, và từ đây là các phương tiện sẽ được sinh ra theo quy luật đã đượctrình bày ở trên. Ở đây, do đặc thù của bài toán, ta không có các bước thời gian kếtthúc giúp ta kết thúc tập, mà chỉ thực hiện làm mới môi trường mỗi khi đã đạt 6bước thời gian. Khi được làm mới, môi trường sẽ tiến hành xoá toàn bộ các phươngtiện đang hoạt động trong đó, đồng thời đưa các giá trị tích luỹ trở về giá trị mặTa tiến hành thay đổi một số thông số của TrafficLightAgent để phù hợpvới các thông tin trên. Vì bản thân gói ML-Agents không hỗ trợ giãn cách đưa quyếtđịnh của môi trường mức 5 giây, ta cần vào phần mã nguồn gốc của gói để thayđổi. Sau đó ta chuyển giá trị DecisionPeriod (tương đương với số khung thờigian của môi trường giữa 2 lần yêu cầu quyết định từ agent) thành 250. Ở đây, môitrường hoạt động với 50FPS, do đó 250 tương ứng với 5 giây thực tế.Trong mã nguồn của agent, ta tính số lần đưa ra quyết định của agent, sau khisố này đạt 64, ta thực hiện việc làm mới môi trường. Ở đây, ta lấy toàn bộ các vậtthể nằm trong vật chứa và tiến hành xoá từng cá thể. Sau đó, ta chuyển các giá trịkhác về giá trị khởi tạo của chúng. Sau quá trình này, ta sẽ có một môi trường ởtrạng thái bắt đầu, với các thông số về thứ tự ưu tiên, mật độ ưu tiên, mật độ x3.4.3 Thiết kế thông tin trạng thái của môi trườngĐể lấy được thông tin về mật độ xe của từng làn đường của môi trường, ta sửdụng các cảm biến được đặt dưới đường. Từng cảm biến này sẽ cho ta thông tin vềsự tồn tại của các phương tiện tại từng điểm trên môi trường. Ở môi trường cơ bản,em sử dụng 252 cảm biến đặt ở khắp các làn đường đi tới ngã tư (ở đây ta khôngcần quan tâm tới thông tin về các làn đường sau khi ra khỏi ngã tư). Các thông tinnày được lấy thông qua việc kiểm tra sự hiện diện của vật thể ở trong một vùng,được cung cấp thông qua phương thức OverlapBox được Unity cung cấp. Cácthông tin này sẽ được chuyển thành một vector rồi gửi tới cho agent để xử lýĐối với môi tường đơn giản, hành động của agent chỉ là các quyết định giánđoạn (discrete) với lựa chọn một hành động tại một thời điểm, ở đây agent sẽ đưara giá trị 0 hoặc 1 tương ứng với việc cặp đèn 0 hoặc 1 chuyển thành màu đỏ. Khiđó, sử dụng API ta có từ script gốc của hệ thống đèn giao thông, ta có thể thực hi3.4.5 Thiết kế hệ thống điểm thưởng cho từng hành độngĐiểm thưởng cho agent trong từng hành động sẽ được tích luỹ trong vòng 5 giâysau khi thực hiện hành động đó. Ở đây, điểm thưởng được sử dụng chính là âm củatổng thời gian mà từng phương tiện phải dừng lại từ hệ quả của đèn đỏ. Bên cạnhđó, để đảm bảo tránh trường hợp một phương tiện bất kỳ phải chờ quá lâu, điểmthưởng này cũng được nhân lên cùng với số thời gian mà xe đã chờ. Điều này giúpcho ta cân bằng được việc tối ưu tổng thời gian chờ cho toàn bộ xe cũng như khônĐể tích luỹ được điểm thưởng cho toàn bộ xe trong môi trường, việc cập nhậtđiểm thưởng phải diễn ra trên từng xe chứ không phải trong bản thân agent. Do đó,trong mỗi khung thời gian, từng xe sẽ có cơ chế tích luỹ thời gian chờ và thêm nóvào trong điểm thưởng của môi trường. Như vậy, con số này trong agent sẽ đượctích luỹ liên tục trong quá trình 5 giây, tăng được sự chính xác cho việc đánh giá.3.5 Tạo cơ chế hành động cơ sở của bài toánĐể đánh giá hiệu năng của các phương pháp được sử dụng, trước hết ta cần cómột bản mô phỏng hệ thống đèn giao thông chưa tối ưu và đánh giá nó để làm mứchiệu năng gốc, từ đó ta mới đánh giá hiệu năng của các phương pháp khác so vớimức gốc đó. Như đã đề cập, nguyên lý hoạt động của hệ thống đèn giao thông chưatối ưu hiện tại là sử dụng các dữ liệu về lưu lượng giao thông trung bình trong thờigian dài để tìm ra được một bộ số chia thời gian phù hợp với các tình huống trungbình của môi trường. Việc tính toán này có thể chia thành nhiều quãng thời giantrong ngày, ví dụ như buổi sáng hoặc buổi chiều có thể có các mốc thời gian khácnhau. Từ cơ sở này, em xây dựng hệ thống mô phỏng lại nguyên lý hoạt động trên.Do mỗi trường có sự thay đổi liên tục về thứ tự ưu tiên của các làn cũng nhưmật độ xe trên từng mức độ ưu tiên, để việc thiết lập một hiệu năng cơ sở được hợplý nhất, em đã xây dựng các luật chia thời gian cụ thể cho từng giai đoạn của môitrường. Cụ thể, để tìm được giá trị tỉ lệ xe của 2 cặp hướng đi vuông góc nhau ởngã tư, ta lấy giá trị trung bình mật độ của 2 cặp hướng đi chia cho nhau. Ví dụvới thứ tự ưu tiên là (0, 1, 2, 3) tương ứng với 4 hướng đi mà ở đây (0, 2) và (1, 3)vuông góc với nhau, ta lấy giá trị mật độ ưu tiên trung bình của 2 cặp này để tìmra tỉ lệ hợp lý là0.55+0 .25+0 .15+0 .20.20+0 .25+0 .10+0 .25= 1.5. Như vậy, trong trường hợp này, tỉ lệ mậtđộ xe trung bình của hai phần đường tương ứng tín hiện giao thông ngược nhau là1.5. Ta có thể sử dụng tỉ lệ này như một sự tham khảo cho việc tạo ra một bộ sốthời gian đèn một cách hợp lý. Tất nhiên, với chỉ tỉ lệ trên thì ta không đủ để xâydựng một bộ hệ số tối ưu, do đó, em đã thử nhiều bộ số thời gian khác nhau để tìmra được bộ số tối ưu nhất cho các đoạn đường. Với bài toán này, em chọn quy lu•Đối với 2 phía đường không được ưu tiên về mặt tạo xe: 2.5 +ratio×5, tươngứng với 7.5 giây đối với ví dụ trên do ratio ở đây là 1.•Đối với 2 phía đường được ưu tiên về mặt tạo xe: 2.5 +ratio×5, tương ứng với10s đối với ví dụ trên do ratio ở đây là 1.Sử dụng phương pháp Proximal Policy Gradient, em tiến hành huấn luyện agentđể tối ưu hoá policy của nó để giải được bài toán đề ra.Trong đó, policy của bài toán được thể hiện thông qua một mạng neuron cơ bảnvới đầu vào là 252 đơn vị tương ứng với 252 giá trị thông tin nhận được từ các cảmbiến (ở đây chính là trạng thái scủa môi trường). Từ 252 node này, mạng sẽ baogồm 5 lớp ẩn với 128 đơn vị ẩn ở mỗi lớp. Đầu ra của mạng là 2 đơn vị tương ứngvới giá trị chọn từng hành động (ở đây chính là chọn tín hiệu cho từng cặp đèn đTrong quá trình huấn luyện, agent và môi trường sẽ được chạy liên tục nhiều lầnđể tích luỹ được lượng dữ liệu dưới policy hiện tại, lượng dữ liệu này sẽ được lưu lạitrong bộ nhớ. Sau đó khi lượng dữ liệu này đủ lớn, ta tiến hành sử dụng các thôngtin trên để cập nhật policy hiện tại. Ở đây, như đã trình bày trong phần lý thuyết,thuật toán PPO cho phép ta sử dụng dữ liệu này cho việc cập nhật policy này nhiềulần, miễn là số lần thực hiện đảm bảo policy không thay đổi quá xa so với policygốc. Với quá trình như vậy, sau một khoảng thời gian đủ lớn và với những cấu hìnhphù hợp của môi trường cũng như những tham số của việc huấn luyện, agent sẽ họcđược một policy hợp lý và giải quyết được môi trường một cách tối ưuTa tạo một bộ nhớ để agent có thể lưu trữ được các thông tin trên. Trong đó, tạimỗi bước thời gian mà agent trải qua, ta lưu lại các giá tr•Rt- điểm thưởng nhận được do hệ quả của hành động a•π(at|st)- xác suất chọn hành động attrong trạng thái stvới policy hiện tại.Thông tin này sẽ được sử dụng trong việc tính ra tỉ lệ giữa hai policy trong q•Giá trị thể hiện bước thời gian hiện tại có phải bước kết thúc hay không.Các thông tin này sẽ được lưu lại dưới dạng một bộ dữ liệu, khi số lượng đủ lớn,ta tiến hành lấy mẫu các thông tin từ đây theo từng bước thời gian để sử dụng ch3.6.3 Huấn luyện mô hình dựa trên dữ liệu đã thu đượcVới các dữ liệu đã thu được trong quá trình tương tác với môi trường, agent sẽsử dụng các dữ liệu này để tiến hành cập nhật policy của mình. Các thông tin ta cólà chuỗi các bước thời gian mà ở mỗi bước ta có trạng thái môi trường, hành độngđược lựa chọn, điểm thưởng cho hành động, trạng thái tiếp theo, xác suất chọn hànTrước hết, sau khi có một chuỗi các bước thời gian, ta có thể sử dụng phươngpháp Monte Carlo chuẩn hoá giá trị điểm thưởng tại mỗi bước thời gian. Như đãtrình bày trong phần lý thuyết, việc này sẽ giúp giá trị tại mỗi bước thời gian cótính liên kết hơn và tính toán một cách chính xác hơn cho MDP. Thông tin này sẽđược sử dụng thay cho các thông tin điểm thưởng thông thường của môi trường.Sau đó, ta tiến hành cập nhật policy dựa vào các thông tin đã có. Ở đây, vì tasử dụng phương pháp PPO, ta có thể thực hiện nhiều lần cập nhật với bộ dữ liệu.Cụ thể, tại mỗi lần thực hiện cập nhật (tương ứng với mỗi epoch), ta tiến hành tín1. Tính toán các giá trị xác suất chọn hành động attại trạng thái stvới policymới nhất. Từ đó tính được tỉ lệ giữa policy cũ và mới.2. Sử dụng hàm giá trị để tính toán giá trị của các trạng thái trong tập dữ liệu.3. Tính toán giá trị lợi thế tại từng bước thời gian bằng cách lấy giá trị điểmthưởng đã được chuẩn hoá (thông qua Monte Carlo) trừ đi giá trị của trạn4. Tính toán hàm mất mát của PPO và tiến hành tối ưu hoá policy.Ở đây, cụ thể trong hàm mất mát của PPO sẽ bao gồm 2 phần, mất mát cho hàmmục tiêu của PPO và mất mát cho hàm giá trị.•Hàm mục tiêu của PPO là giá trị mincủa hai phần là tỉ lệ giữa 2 policy nhânvới giá trị lợi thế và phiên bản bị chặn của tích trên.•Mất mát của hàm giá trị được tính toán dựa trên mục tiêu là giá trị điểm thưởngđã được chuẩn hoá (nhận được từ Monte Carlo).Sau khi có 2 hàm mất mát trên, ta sử dụng phương pháp tối ưu Adam để tiếnhành tối ưu hoá policy cũng như hàm giá trTrong chương này, em đã trình bày về quá trình phát triển môi trường và huấnluyện mô hình giải quyết bài toán. Trong đó, em đã trình bày về các bước cơ sở nhưphát triển nền môi trường, thêm các logic về hệ thống đèn giao thông và hệ thốngquản lý phương tiện. Sau đó, môi trường được phát triển thành một quá trình quyếtđịnh Markov đóng vai trò là môi trường học cho agent. Từ đó, ta tiến hành huấnluyện agent và tìm được policy tối ưu cho bài toáVới môi trường và thuật toán đã được phát triển trong chương 3, em tiến hànhthử nghiệm huấn luyện mô hình với một số cấu hình khác nhau của môi trường vàgiải thuật. Các thử nghiệm này được thực hiện theo hướng phát triển mô hình vàtối ưu hoá số lượng cảm biến và kích thước của mô hình, từ đó giảm thiểu chi phícho giải pháp. Trong chương này, em sẽ trình bày về các cấu hình được sử dụngcho việc thử nghiệm và các kết quả đạt được. Bên cạnh đó, em cũng đưa ra một sốđánh giá về các kết quả và giá trị của chúng trong việc giải quyết bài toánĐối với bài toán đặt ra và giải pháp đã trình bày, mục tiêu đánh giá của em tron•Thử nghiệm các cấu hình khác nhau của môi trường.•Thử nghiệm các độ lớn khác nhau của mô hình.•Đánh giá vai trò của phương pháp học tăng cường trong việc giải bài toán đ•Đánh giá khả năng ứng dụng của giải pháp trong thực tếĐể đánh giá hiệu năng của các phương pháp trên với nhau cũng như với hiệunăng cơ sở, em sử dụng một số độ đo đánh giá như sau:•Thời gian chờ trung bình: Giá trị trung bình mỗi xe chờ đèn đỏ.•Chiều dài hàng đợi: Chiều dài của một dãy xe chờ tín hiệu đèn chuyển quThời gian chờ trung bình cho phép ta nhìn vào hiệu năng của các phương phápđối với một mục tiêu quan trọng của bài toán là giảm thiểu thời gian mà mọi ngườitham gia giao thông phải chờ đợi tín hiệu đèn. độ đo đánh giá giúp ta có thể ướclượng được các giá trị mà giải pháp mang lại.Chiều dài hàng chờ cũng là một độ đo đánh giá quan trọng. Khi chiều dài dãycác xe đang chờ đèn đỏ càng lớn có nghĩa là số lượng các xe cùng dừng tại mộtđiểm đèn đỏ càng lớn. Điều này gây ra sự trì hoãn khi chuỗi xe nhận được tín hiệuđèn xanh để đi, do các xe đều có khoảng thời gian chờ xe phía trước cách xa trướckhi nhấn ga. Bên cạnh đó, khi chuỗi xe dài không được giải phóng cùng lúc sẽ gâra tình trạng tăng tốc rồi giảm tốc liên tục, kết hợp với sự trì hoãn đã giải thích ởtrên, khiến cho đoạn đường càng thêm tắc nghẽn, cùng với đó là thải ra nhiều loạikhí thải có hại cho sức khoẻ và môi trườngĐể đánh giá hiệu năng của từng phương pháp, em cấu hình môi trường phù hợpvới việc đánh giá. Một số thông số sau được cố định:•Tổng số bước thời gian cho mỗi lần sinh giá trị đánh giá là 3072 bước. Điềunày đảm bảo bao quát được hết toàn bộ các trường hợp của môi trường. Ở đâygiá trị 3072 được tính thông qua 24 (Số lượng các thứ tự ưu tiên khác nhau) ×64 (Số lượng bước thời gian trong một giai đoạn của thứ tự ưu tiên) ×2 (Ha•Với mỗi phương pháp, thực hiện tính toán các giá trị độ đo đánh giá đối vớibội số đủ lớn của 3072 bước thời gian để tính toán giá trị trung bình một các•Môi trường sử dụng hạt giống cố định cho các hàm số ngẫu nhiên. Điều nàygiúp ta đảm bảo được môi trường giống nhau giữa các lần đánh giá, tăng đượBên cạnh đó, môi trường được xây dựng thành một ứng dụng, nâng cao hVới môi trường được thiết kế cho việc đưa ra các quyết định giống với cơ chếhiện tại ở ngoài thực tế, sau quá trình đánh giá, hệ thống đạt được hiệu năng khôngtốt và không ổn định được thể hiện trong hình 4.1. Như ta có thể thấy, tổng điểmthưởng trong mỗi tập của môi trường có sự khác biệt khá nhiều. Cho thấy điểm hạnchế không thể thích ứng với các trạng thái khác nhau của môi trường, trong đó tasẽ có những giai đoạn mà hệ thống này giải quyết tốt, nhưng cũng có những giaiđoạn mà hiệu năng của hệ thống là rất tệ. Giá trị điểm thưởng trung bình đạt đư4.6.1 Thiết lập siêu tham số cho thuật toán PPOĐể huấn luyện mô hình, trước tiên em thử một hệ thống siêu tham số cơ bản choagent và phát triển từ đó đối với các kết quả đạt được khác nhau. Các hệ số cần ch•buffer_size = 512: Số lượng bước thời gian cần thu thập trước khi thực hiệ•batch_size = 64: Số lượng bước thời gian được sử dụng cho một lần tính toá•learning_rate= 3e-5: Tốc độ học, kiểm soát tốc độ thay đổi của mô hình tron•num_layers = 5: Số lớp ẩn trong mạng neuron sử dụng.•hidden_units = 128: Số đơn vị ẩn trong một lớp ẩn trong mạng neuron s•epsilon = 0.2: Giá trị ϵtrong hàm mục tiêu của phương pháp PPO. Giá trị 0.được tác giả khuyến nghị cho hầu hết bài toán.•gamma = 0.95: Hệ số chiết khấu, thể hiện sự thụt giảm giá trị điểm thưởngnhận được trong tương lại, tính cho thời điểm hiện tại.Với hệ thống siêu tham số như trên, ta có thể tiến hành huấn luyện mô hình vàsử dụng kết quả huấn luyện để tiếp tục quá trình tối ưu hệ thống tham số.4.6.2 Huấn luyện mô hình với môi trường có hệ thống cảm biến dày đặcĐể khởi đầu quá trình phát triển, trước hết em sử dụng một hệ thống môi trườngcó hệ thống cảm biến rất dày, được đặt ở hầu hết các vị trí trong môi trường. Trongđó, mỗi hướng đường bao gồm 63 cảm biến trong đoạn đường dài 100m và rộng15m được thể hiện trong hình 4.2. Điều này đảm bảo agent có đủ thông tin nhậthức về môi trường do hệ thống cảm biến rất dày và không bị mất mát trong cáVới cấu hình môi trường này, em sử dụng một kiến trúc mạng neuron đơn giảnvới 5 lớp ẩn, trong đó mỗi lớp có 128 đơn vị ẩn và với các giá trị siêu tham số cơbản đã được chọn, quá trình huấn luyện diễn ra khá ổn định và hiệu quả, được thểhiện qua hình 4.3. Trong đó, ta có thể thấy mô hình học một cách tương đối ổn địnhvà hội tụ sau khoảng 25000 bước thời gian.Với kết quả khá tốt, đạt giá trị điểm thưởng trung bình -2162. Giảm 57% so vớihiệu năng cơ sở. Bên cạnh đó, sau khi hội tụ, lượng điểm thưởng nhận được củamô hình này cũng rất ổn định, thay đổi trong khoảng 216. Sự vượt trội này đượcthể hiện rõ trong hình 4.3 so sánh kết quả của hai hệ thống. Những kết quả này thhiện hiệu năng áp đảo của hệ thống thông minh so với hệ thống truyền thống vềcả số điểm thưởng trung bình lẫn sự ổn định. Với hệ thống này, ta có thể đảm bảođược việc tối ưu các quyết định với mục đích hướng tới là sự thoải mái của ngườVới nhận định rằng các thông tin quan sát của agent về môi trường là tương đốiđầy đủ do có số lượng cảm biến khá dày. Ta thấy hệ thống có thể được cải tiến vềmặt mô hình và thử nghiệm giảm số lượng cảm biến để tối ưu hoá hệ thống, giĐể giảm thiểu một lượng lớn chi phí cho việc mua và lắp đặt các cảm biến, emthử nghiệm giảm lượng lớn các cảm biến trong môi trường. Cụ thể, thay lắp đặt dải cảm biến song song như trong hệ thống ban đầu, ta có thể sử dụng hai dải cảmbiến do trên môi trường hiện tại chỉ có 2 làn đường xe chạy. Đồng thời, ta cũnggiảm bớt số lượng cảm biến thông qua việc đặt chúng cách xa nhau hơn, làm tăngđộ thưa của chúng. Hệ thống cảm biến mới được thể hiện ở hình 4.4, số lượng cảVới số lượng cảm biến giảm đáng kể, chi phí cho việc mua cũng như lắp đặt hệthống giảm đáng kể. Trong khi đó, theo kết quả huấn luyện mô hình được thể hiệntrong hình, ta vẫn đạt được mức điểm thưởng tương đương như trong hệ thống cảmbiến dày đặc, được thể hiện trong hình 4.5 Chỉ với lượng cảm biến tương đối thưavề môi trường, agent vẫn có thể đưa ra được các quyết định hợp lý và đạt kết quKhi giảm kích thước mô hình một cách tương đối nhiều với số lượng lớp ẩn giảmtừ 5 xuống 2. Hiệu năng trên đã có giảm nhẹ và có sự thay đổi về quá trình hội tcấu hình khác nhau và với kết quả của hệ thống cơ sở. Khi giảm kích thước môhình, hiệu năng của hệ thống khi hội tụ có sự giảm nhẹ so với hai hệ thống trước.Điều này là do kích thước mô hình quá nhỏ dẫn tới không thể học được đủ các đặctính của môi trường. Từ đó, trong một số trường hợp không hiểu rõ, hệ thống có thểđưa ra một số hành động không tối ưu nhất.Tuy nhiên, với những kết quả trên, ta vẫn có thể đánh giá rằng 104 cảm biến làmột số lượng đủ để agent có thể nắm bắt được các thông tin cần thiết về lưu lượnggiao thông trong thời gian thực trong môi trường. Do đó, ta có thể thử nghiệm tĐể cắt giảm số lượng cảm biến trong môi trường. Ta tiến hình cắt giảm 2 dảicảm biến xuống một, từ đó ta chọn điểm trung tâm 2 làn đường để đặt cảm biến,từ đó có thể nhận được thông tin về cả 2 làn đường. Hệ thống được thể hiện tronVới số lượng cảm biến giảm đáng kể, kết quả ta đạt được thể hiện sự sụt giảmtương đối so với hệ thống 104 cảm biến. Điểm thưởng trung bình đạt mức -243với độ chênh lệch giữa các tập môi trường khá thấp. Nhìn vào hình 4.7, ta có thểthấy được một sự so sánh về kết quả của các hệ thống với cấu hình khác nhau. Ởđây, hệ thống với lượng cảm biến tối giản vẫn đem lại một kết quả khả quan khi chỉthấp hơn hai hệ thống tốn kém nhất và ngang bằng với hiệu năng của hệ thống Bên cạnh đó, sự sụt giảm hiệu năng này có thể hiểu được do, do ta chỉ sử dụng9 cảm biến ở mỗi hướng đường, trong đó cảm biến được đặt ở giữa 2 làn đường vàkhông có sự phân biệt giữa việc có 1 xe hay 2 xe đang trong vùng cảm biến. Khinày, những quan sát về môi trường sẽ quá thưa và có một chút sai lệch so với thựctế. Tuy nhiên, ta có thể thấy kết quả của mô hình vẫn khá ổn định qua các tập môitrường với các tình trạng môi trường khác nhau, thể hiện được sự thích nghi của môhình với mọi tính huống trong môi trường. Những kết quả đạt được vẫn là tươngđối tốt so với hệ thống tốt nhất, giảm 12%. Và so với mức hiệu năng cơ sở thì 4.6.5 Đánh giá kết quả các cấu hình hệ thốngSau quá trình huấn luyện mô hình, em tiến hành đánh giá kết quả của các cấuquan về kết quả của các cấu hình khác nhau của hệ thống cảm biến cũng như kíchthước mô hình sử dụng. Ta có thể thấy những kết quả của hệ thống có sự khác biệHệ thống 252 cảm biến cung cấp mức điểm thưởng trung bình cao nhất trong3 cấu hình cảm biến. Trong đó, mức tối đa đạt được tại mô hình với 152283 thamsố. Thực nghiệm cho thấy không có thay đổi đáng kể khi tăng kích thước mô hình.Tuy nhiên, hệ thống này có những điểm hạn chế như yêu cầu quá nhiều cảm biến.Đồng thời, do đầu vào của hệ thống có nhiều cảm biến hơn nên số tham số của mHệ thống 104 cảm biến có sự giảm về điểm thưởng so với hệ thống cảm biếndày đặc trên. Điều này là do sự giảm thiểu lớn về độ dày đặc của những thông tinmôi trường nhận được từ các cảm biến cùng với đó là sự giảm thiểu kích thước môhình. Tuy nhiên, kết quả nhận được vẫn rất khả quan và cách biệt không quá nhiềuso với mức hiệu năng tối đa của hệ thống cảm biến dày đặc. Bên cạnh đó, do sốlượng cảm biến giảm đáng kể, với cùng một số lượng đơn vị ẩn trong mỗi lớp vàsố lớp ẩn trong mô hình, lượng tham số của mô hình cũng giảm đi đáng kể. Ở mộtmức lượng tham số khá tương đồng, hệ thống này đạt hiệu năng tương đương vớhệ thống cảm biến dày đặc với cùng kích thước mô hình.Hệ thống 36 cảm biến có lượng điểm thưởng thấp hơn so với hai hệ thống kểtrên. Mặc dù có số lượng cảm biến khá ít và thông tin đầu vào từ môi trường rấtthưa, bằng việc tăng kích thước mô hình, ta vẫn có thể xây dựng được hệ thống nàyvới kết quả tương đối tốt. Nếu so sánh với hệ thống cơ sở thì ta vẫn có một sự khácbiệt lớn. Hơn nữa, việc giảm lượng cảm biến từ 252 xuống 36 là sự giảm thiểu rấtlớn về chi phí cần thiết cho giải pháp, từ đó giúp cho tính ứng dụng của giải phápđược tăng cường, giúp giải quyết được bài toán thực tế.Qua những đánh giá này, ta có thể thấy được một cái nhìn tổng quan về quá trìnhhuấn luyện mô hình và những kết quả đạt được với những cấu hình khác nhau củahệ thống. Từ đó, tuỳ thuộc vào nhu cầu thực tế mà ta có thể sử dụng các cấu hìn4.7 Đánh giá các hệ thống qua các độ đo đánh giThời gian chờ trung bình là trọng tâm tối ưu hoá trong bài toán đặt ra. Đánh giánày thể hiện giá trị trung bình mà mỗi xe xuất hiện trong môi trường phải chờ. Mụctiêu của bài toán là giảm giá trị này tới mức tối thiểu, đồng nghĩa với việc ngườitham gia giao thông có thể lưu thông một cách nhanh và thoải mái hơn, đồng thờitiết kiệm được lượng thời gian tham gia giao thông hằng ngày. Sau quá trình đánhgiá hệ số này trên nhiều cấu hình hệ thống và kích thước của mô hình, một số kếquả tiêu biểu được thể hiện trong hình 4.9.Ta có thể thấy, hàm mục tiêu của mô hình ta đặt ra có phần khác so với hệ sốđánh giá này, trong đó hàm mục tiêu của ta có đưa thêm một trọng số liên quan đếnthời gian chờ của xe (xe chờ càng lâu thì trọng số điểm thưởng âm càng lớn). Mặcdù vậy, hệ thống được xây dựng từ mô hình trên vẫn cho ta một kết quả vượt trội sovới hệ thống gốc. Hệ thống tối giản với 36 cảm biến đạt mức thời gian chờ trungbình giảm 45% so với hệ thống gốc. Và ở các hệ thống với nhiều cảm biến hơn cKết quả đánh giá các hệ thống với độ đo đánh giá chiều dài hàng chờ trung bìnhđược thể hiện trong hình 4.10. Ở đây ta cũng thấy được sự vượt trội của hệ thốngtối ưu so với hệ thống gốc. Các cấu hình hệ thống tối ưu khác nhau không có quánhiều sự chênh lệch về hiệu năng, đều ở mức giảm 21% chiều dài hàng chờ tru•Với các cấu hình khác nhau của môi trường, em đã thực hiện đánh giá và rútra kết quả các mức hiệu năng khác nhau với từng cấu hình. Đồng thời, mỗi cấuhình cũng sẽ phù hợp với từng điều kiện thực tế của bài toán như mức hiệ•Với các độ lớn khác nhau của mô hình, kết quả thử nghiệm cho thấy hiệu năntăng lên khi ta tăng kích thước của mô hình. Trong đó, mức kích thước với lớp ẩn gồm 128 đơn vị ẩn là một mốc giới hạn mà từ đó khi tăng kích thướcmô hình hiệu năng không có nhiều thay đổi.•Trong việc giải bài toán đề ra, các phương pháp học tăng cường giúp tối ưuhoá các quyết định tín hiệu đèn giao thông một cách hiệu quả dựa trên chuỗicác kinh nghiệm tích luỹ trong quá khứ. Do đó, phương pháp này giúp cho cácquyết định này không bị phụ thuộc vào các thông tin nhãn từ con người - điềukhó thực hiện trong bài toán mà ta đặt ra. Chính vì vậy, giải pháp giúp ta vượtqua được vật cản về việc tạo thông tin nhãn cho hành động, và đồng thời cungcấp một mô hình tối ưu dựa trên điều kiện thực tế, giúp ta nâng cao được gi•Các kết quả đạt được cho thấy, với các mức chi phí yêu cầu khác nhau, kết quảđạt được có sự thay đổi nhưng đều vượt trội so với hệ thống không tối ưu hiệntại. Do đó, với lượng dữ liệu thật ngoài thực tế, sau quá trình huấn luyện vàthử nghiệm kết quả, hệ thống có thể được đem vào thí điểm tại một số cụmđèn giao thông, và từ đó đem vào sử dụng thực tế nếu đạt kết quả tốt. Từ việccó thể tiếp tục nhận các thông tin và đưa ra các hành động khi được đem vàothực tế sử dụng, hệ thống có thể liên tục học và cập nhật policy của mình. Kếthợp với một phương pháp triển khai hệ thống một cách an toàn, việc sử dụngvà liên tục tối ưu hoá hệ thống là khả quanNhư vậy, trong suốt quá trình thực hiện đồ án, em đã xây dựng được một môitrường học tăng cường với bài toán tối ưu hoá các tín hiệu đèn giao thông tại mộtngã tư đơn giản, xây dựng các hệ thống cảm biến khác nhau và thử nghiệm phươngpháp học tăng cường vào tối ưu hoá hệ thống đèn giao thông. Em đã đưa ra kếtquả khả thi và đạt kỳ vọng đặt ra khi hệ thống tối ưu có hiệu năng vượt trội so vớihệ thống gốc. Kết quả này khẳng định tính khả thi của giải pháp và khả năng pháttriển và ứng dụng vào thực tiễn trong tương lTrong nội dung đồ án, em đã đạt được một số kết quả chính như sau:•Xây dựng môi trường hệ thống giao thông ở một ngã tư đơn giản, hoạt độngmột cách hiệu quả và có khả năng tuỳ biến.•Thử nghiệm phương pháp học tăng cường với các cấu hình khác nhau của môitrường và mô hình. Đạt kết quả tốt với mức thời gian chờ trung bình của cácphương tiện giảm 45% và độ dài hàng chờ giảm 21%Giải pháp đưa ra có hiệu năng tăng nhiều so với hiệu năng cơ sở, cùng với đólà khả năng ứng dụng cao trong thực tế. Do đó, hệ thống có thể được phát triển đểsử dụng ngoài đời thực. Bên cạnh đó, môi trường được xây dựng trong đồ án cóthể được phát triển để tăng độ phức tạp của bài toán. Mô hình bài toán cũng là mộtđiểm mạnh khi có thể áp dụng vào một số bài toán khácĐiểm hạn chế của đồ án là môi trường đang xét tới còn tương đối đơn giản, dođó việc học của agent chưa thực sự quá khó khăn để cần tới các bước tinh chỉnhphức tạp. Các đánh giá còn tương đối chủ quan, cần có sự tham gia của các chuyêngia trong lĩnh vực giao thông để có thể tạo nên một hệ thống tham số đánh giá hợplý và có tính chất ứng dụng cao hơn nữa•Xây dựng môi trường phức tạp với quy mô lớn hơn, ví dụ như quy mô hệ thốngtrong một thành phố, hay một quận, huyện. Với môi trường này, ta có thể tậndụng được tính tối ưu của phương pháp học tăng cường. Khi đó, nhờ khả năngtối ưu hoá cho hàm mục tiêu chung trên toàn bộ môi trường, agent có thể họcđược những tính chất liên quan đến phân luông giao thông, đưa ra các quyếtđịnh tín hiệu đèn để tối ưu trên cả môi trường thay vì một cụm đèn duy nhất.•Thay đổi mô-đun cảm biến của môi trường bằng hình ảnh. Theo đó, ta có thểđể agent học từ đầu vào là ảnh, và từ đó ta có thể huấn luyện một mô hình đưara các quyết định đèn tín hiệu, hoặc thêm một mô-đun đánh giá lưu lượng xeở các hướng dựa vào ảnh. Từ đây, ta giảm được chi phí cho việc phải sử dụngmột số lượng cảm biến khá lớn trên đườnNhư vậy, trong nội dung báo cáo đồ án tốt nghiệp, em đã trình bày về các phươngpháp học tăng cường, quá trình phát triển môi trường cho bài toán hệ thống giaothông ở một ngã tư, và từ đó ứng dụng các phương pháp trên vào tìm giải pháp tốiưu cho bài toán. Những kết quả đạt được từ đồ án khá tốt và sẽ là tiền đề để em cóthể tiếp tục phát triển bài toán và tiến đến ứng dụng vào giải quyết vấn đề cho xãhội. Do thời gian thực hiện đồ án có hạn nên kết quả của đồ án còn có những hạnchế. Em mong có thể có được sự góp ý và hỗ trợ từ thầy cô, bạn vè để giúp em pháttriển đồ án thành một dự án lớn hơn, mang lại những giá trị cho xã h