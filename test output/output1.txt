1.1 Đặt vấn đề Hiện nay, trong thời đại cách mạng công nghiệp 4.0, sựphát triển và ứng dụng công nghệtrởthành một trong những yếu tốtiên quyết dẫn đến thành công của nhiều ngành nghềvà lĩnh vực. Trong sốđó, trí tuệnhân tạo (AI [1]) được xem là một trong những công nghệđược đánh giá cao, có nhiều ứng dụng và đã mang lại những cải thiện rất đáng kểcho nhiều lĩnh vực. Tuy nhiên, có một sốlĩnh vực mà không có nhiều ứng dụng nổi bật của AI, trong sốđó có thểkểđến giao thông.
Một hệthống giao thông tối ưu là nhu cầu quan trọng do đây là thứmà mỗi cá nhân chúng ta đều tương tác, tham gia và sửdụng mỗi ngày. Tuy nhiên hệthống giao thông mà hiện chúng ta vẫn đang tham gia và tương tác mỗi ngày vẫn còn nhiều điểm có thểcải tiến. Một phần chưa tối ưu trong hệthống này có thểkểđến các cụm đèn giao thông. Hiện nay, ta có thểdễdàng thấy rằng có nhiều trường hợp mà tín hiệu đèn giao thông không phản ánh đúng tình trạng giao thông thực tế. Điều này dẫn đến các quyết định tín hiệu đèn và thời gian đèn được đưa ra một cách không hợp lý, gây ra lãng phí thời gian, ảnh hưởng đến sức khoẻvà tâm lý của người tham gia giao thông.
Chính vì vậy, trong đồán này em đã chọn ứng dụng AI, mà cụthểlà phương pháp học tăng cường, đểtìm ra giải pháp tối ưu hoá những vấn đềcủa hệthống đèn giao thông hiện tại. Với đồán này, em hướng tới việc thực hiện nó như một bằng chứng khái niệm (Proof of concept), đểthểhiện sựkhảthi của việc ứng dụng học tăng cường trong giải quyết bài toán vềlĩnh vực giao thông, hướng tới một hệ thống giao thông thông minh, đem lại giá trịlớn cho xã hội.
1.2 Các giải pháp hiện tại và hạn chế Hiện nay, giải pháp đang được sửdụng ởngoài thực tếlà một hệthống đèn giao thông thay thay đổi tuần tự. Trong đó, các mốc thời gian tương ứng cho từng màu đèn, cũng như thứtựthay đổi màu của các đèn đều được xác định từtrước. Điều này giải quyết vấn đềtương đối tốt với những trường hợp giao thông xảy ra với đúng như những mức thời gian và những quy luật đã được đềra từtrước.
Tuy nhiên, những ước lượng và tính toán đểra được các quy luật trên đều được tính một cách trung bình với các mốc thời gian lớn, điều này dẫn tới sai sốđối với thực tế. Bởi vì bản chất của phương pháp trên không thểhiện được các thông tin vềtình trạng giao thông thực tếởngoài môi trường, do đó sẽkhông thểđưa ra các tín hiệu chuẩn xác và phù hợp nếu các ước lượng trên không được thoảmãn. Bên cạnh đó thì việc tính toán trên cũng phải diễn ra thường xuyên đểcó thểtheo kịp với những thay đổi ngoài thực tế, điều này cũng yêu cầu có một nguồn nhân lực và dữliệu lớn.
1.3 Mục tiêu và định hướng giải pháp Với các hạn chếđược nêu trong phần 1.2, em đã đi đến một hướng giải quyết mới cho bài toán. Với việc giảđịnh là các tuyến đường có thông tin từcảm biến được chuyển vềcho ta biết tại từng vịtrí có phương tiện đang chiếm hay không. Từ thông tin đó, mô hình sẽđưa ra cho ta các quyết định vềtín hiệu đèn giao thông phù hợp cho đúng thời điểm đó.
Giải pháp này hướng tới giải quyết các vấn đềcủa phương pháp truyền thống được nhắc đến trong phần 1.2. Với các thông tin môi trường được cập nhật liên tục, mô hình sẽđưa ra các quyết định phù hợp nhất với đúng thông tin tương ứng đó, từ đó có thểgiải quyết được vấn đềvềtín hiệu không tương ứng với thực tế. Bên cạnh đó, ta cũng có thểliên tục tiếp tục việc huấn luyện mô hình với các quyết định thực tếtrên, và sửdụng một sốhướng triển khai mô hình mới một cách an toàn, từđó có thểđảm bảo việc luôn luôn có một mô hình hoạt động hiệu quảvà phù hợp vưới môi trường đặt ra, không bịgiảm hiệu năng với các thay đổi vềmôi trường theo thời gian.
1.4 Đóng góp của đồán Đồán này có 3 đóng góp chính như sau:
1. Đềxuất một phương pháp tối ưu hoá hệthống đèn giao thông, sửdụng các thông tin của môi trường và đưa ra các tín hiệu đèn giao thông phù hợp với trình trang giao thông thực tếtrong thời gian thực.
2. Xây dựng một môi trường mô phỏng với bài toán đặt ra là tối ưu hoá hệthống đèn giao thông ởmột ngã tư đơn giản.
3. Thửnghiệm một sốphương pháp học tăng cường đểgiải quyết bài toán trên.
1.5 Bốcục đồán Phần còn lại của báo cáo đồán tốt nghiệp này được tổchức như sau.
Ởchương 2, em sẽgiới thiệu vềnhững kiến thức nền tảng cần liên quan đến bài toán đặt ra cũng như những lý thuyết được áp dụng trong giải pháp của bài toán.
Thông qua chương này, người đọc có thểhiểu rõ hơn vềnền tảng của giải pháp cần trình bày, có khảnăng hiểu các khái niệm, kiến thức, thuật toán được sửdụng trong đồán này. Đồng thời, em cũng trình bày một sốphương pháp đã và đang được phát triển đểgiải quyết bài toán, cùng với đó là những điểm mạnh và hạn chếcủa các phương pháp.
Chương 3 sẽbao gồm thông tin chi tiết vềgiải pháp mà em đưa ra và quá trình phát triển nó. Người đọc có thểhiểu được tư tưởng cốt lõi của giải pháp, đồng thời cũng có khảnăng kết hợp với lý thuyết đểcó thểtái tạo lại được các kết quảmà em đã đạt được.
Chương tiếp theo sẽbao gồm những kết quảthửnghiệm và đánh giá của em.
Trong chương này, em trình bày vềmột sốtham sốđánh giá được sửdụng và kết quảđạt được đối với việc thửnghiệm nhiều tham sốkhác nhau của môi trường và phương pháp học tăng cường.
Trong phần tổng kết em sẽtổng kết lại thông tin vềđồán, kết quảđạt được và trình bày một sốhướng phát triển trong tương lai.
Cuối báo cáo sẽlà danh sách các tài liệu tham khảo mà em sửdụng trong quá trình phát triển đồán cũng như thực hiện viết báo cáo cho đồán.
2.1 Tổng quan Đối với bài toán đã được giới thiệu trong Chương 1, đểgiải quyết được bài toán, trước hết ta cần tìm hiểu một sốnền tảng lý thuyết. Từnhững lý thuyết này, ta có thểhiểu hơn vềbài toán đặt ra cũng như có cái nhìn tổng quan các hướng đi cho việc giải quyết bài toán. Chương này sẽgiới thiệu các kiến thức từđơn giản tới nâng cao đểngười đọc có khảnăng hiểu được rõ vềgiải pháp được đềxuất trong đồán này.
2.2 Ngữcảnh của bài toán Như đã giới thiệu trong phần trước, bài toán mà ta đang hướng tới là giải quyết một môi trường ngã tư đơn giản bao gồm 4 hướng. Ởđây, đối tượng mà ta cần tối ưu là hệthống đèn giao thông, trong đó tại mỗi thời điểm đưa ra quyết định, ta có thểlựa chọn tiếp tục màu đèn cũ hoặc thay đổi màu đèn. Khi màu đèn được thay đổi, hệthống các phương tiện giao thông sẽtuân thủtheo tín hiệu đèn mà ta đưa ra.
Trong phạm vi của bài toán, đểđơn giản hoá quá trình này, ta ngầm hiểu hệthống phương tiện tham gia giao thông sẽtuân thủhoàn toàn theo đèn tín hiệu. Bên cạnh đó, đểcó thểđềxuất một sốgiải pháp tối ưu, ta cũng ngầm định việc có đủnguồn lực đểthay đổi một sốyếu tốmôi trường như việc thêm hệthống cảm biến dưới mặt đường.
2.3 Hệthống đèn giao thông hiện tại Hiện tại, hệthống đèn giao thông đang hoạt động theo phương pháp thay đổi tuần tựdựa trên một khung thời gian đã được cốđịnh từtrước. Khung thời gian này được tính toán thông qua việc lấy các thông tin vềmật độxe trong khu vực trong một thời gian dài và tính toán ra một giá trịtrung bình cho các thời gian đó. Từđó, sửdụng thông tin trên đểtìm ra được một bộsốthời gian phù hợp. Phương pháp này có điểm hạn chếlớn là không sửdụng các thông tin trong thời gian thực, do đó không thểtối ưu được cho từng thời điểm cụthểcủa môi trường. Bên cạnh đó, các thông tin vềgiao thông cũng thay đổi theo thời gian, điều này đặt ra yêu cầu về việc tính toán lại các giá trịbộsốtrên cho phù hợp với môi trường.
2.4 Các kết quảnghiên cứu tương tự Học tăng cường đã được ứng dụng trong kiểm soát tín hiệu đèn giao thông trong một sốnghiên cứu khoa học trước đây tuy nhiên còn tương đối đơn giản với môi trường mô phỏng thiếu thực tế. Học tăng cường đã bắt đầu được ứng dụng trong kiểm soát tín hiệu đèn giao thông từnhững năm của thập niên 90. Nghiên cứu [2] tổng hợp các phương pháp từ1997 tới năm 2010 sửdụng học tăng cường trong lĩnh vực này. Trong giai đoạn này, các phương pháp phần lớn bịgiới hạn ởQ-learning [3] dạng bảng với giá trịQ được tính toán bằng một hàm tuyến tính. Bên cạnh đó, do những giới hạn vềgiải thuật thời điểm đó, không gian trạng thái của môi trường thường được thu hẹp lại thành một vài giá trịnhư sốlượng xe đang chờđèn đỏ, số lượng xe ởmỗi làn đường,... Chính vì thế, thông tin của môi trường không được thể hiện một cách đầy đủ, dẫn tới hiệu năng không được nâng cao.
Với sựphát triển của học tăng cường trong các năm gần đây, các phương pháp mới và hiệu quảliên tục được giới thiệu. Do đó, một sốnghiên cứu cũng thửnghiệm ứng dụng học tăng cường vào kiểm soát tín hiệu đèn giao thông. Nghiên cứu [4] và [5] đã sửdụng mạng nơ-ron đểxấp sỉhàm giá trịQ đểvượt qua vấn đềvềkích thước không gian trạng thái của phương pháp Q-learning truyền thống, đạt được kết quảkhảquan hơn so với các phương pháp trước đó. Nghiên cứu [6] đã ứng dụng những phương pháp tiên tiến hơn của học tăng cường trong thời điểm đó như Double DQN [7] hay Prioritised Experience Replay (PER) [8].
Các phương pháp trên đều đạt được kết quảtốt, tuy nhiên vẫn tồn tại một số vấn đề. Các kết quảđạt được đều rất khó hoặc không thểtái tạo lại do sựthiếu hụt thông tin vềchi tiết triển khai thuật toán, phương pháp tính toán các độđo đánh giá và chi tiết phương pháp tạo ra hiệu năng cơ sở. Môi trường được sửdụng trong việc phát triển thuật toán và đánh giá kết quảcũng thiếu sựtùy biến, khó có thể thiết kếvà thu thập được thêm các thông tin mới nếu muốn phát triển bài toán. Bên cạnh đó, cùng với sựphát triển liên tục của lĩnh vực học tăng cường, các phương pháp mới được phát triển liên tục và đạt được kết quảcải thiện nhiều so với các phương pháp cũ. Do đó, việc ứng dụng các phương pháp này vào bài kiểm soát tín hiệu đèn giao thông, dưới một môi trường giảlập phù hợp với thực tếlà một việc quan trọng, là bước đầu cho quá trình ứng dụng học tăng cường vào vấn đềnày ở ngoài thực tế.
2.5 Học tăng cường Phương pháp học tăng cường (reinforcement learning [9]) là một nhánh con của học máy (machine learning [10]) mà bản chất hoạt động theo cơ chếthửnghiệm nhiều lần và rút ra kinh nghiệm sau mỗi lần thử. Gần đây, học tăng cường đang trở nên khá nổi tiếng nhờmột sốnhững kết quảđạt được như việc AlphaGo [11] thắng áp đảo các tuyển thủthếgiới trong cờvây, OpenAI [12] chiến thắng các đội mạnh nhất trong trò chơi DOTA2. Trong phần này, chúng ta sẽtìm hiểu vềcác kiến thức nền tảng của học tăng cường từđó đi đến một thuật toán nổi tiếng trong nhóm này là Proximal Policy Optimization [13].
2.5.1 Mô hình hoá bài toán Trong phần lớn các phương pháp, giải thuật trong trí tuệnhân tạo và đặc biệt là trong học máy, ta thường hay mô hình hoá bài toán bằng các mô hình toán học.
Do đó, đối với học tăng cường, việc mô hình hoá bài toán bằng toán học cũng là một hướng tiếp cận khảdĩ. Cụthểởđây, ta sửdụng Quá trình quyết định Markov (Markov Decision Process [14]) đểmô hình hoá bài toán.
a, Quá trình quyết định Markov Markov Decision Process (MDP) cung cấp một nền tảng toán học cho việc mô hình quá việc đưa ra quyết định trong các tình huống mà kết quảlà một phần ngẫu nhiên và một phần dưới sựđiều khiển của người ra quyết định. Tại mỗi bước thời gian, quá trình này ởtrong một trạng thái s, agent đưa ra quyết định có thểchọn bất kỳhành động a. MDP đáp ứng trong bước thời gian tiếp theo bằng việc đưa ta tới một trạng thái mới s′ và cho agent một điểm thưởng tương ứng Ra(s, s′).
Xác suất mà agent đi tới trạng thái s′ sau khi đưa ra hành động a được tính bởi hàm chuyển tiếp trạng thái Pa(s, s′). Do đó, trạng thái kếtiếp s′ phụthuộc vào trạng thái hiện tại s và hành động a.
trong đó:
• S là một dãy hữu hạn các trạng thái.
• A là một dãy hữu hạn các hành động.
• Pa(s, s′) là xác suất mà hành động a trong trạng thái s tại thời điểm t sẽdẫn đến trạng thái s′ trong thời điểm t + 1.
• Ra(s, s′) là điểm thưởng mà agent nhận được sau khi chuyển tiếp sang trạng thái s′ từtrạng thái s.
thưởng nhận ngay lập tức và điểm thưởng nhận trong tương lai.
tích luỹđiểm thưởng được tối ưu hoá.
X t=0 2.5.2 Policy Trong học tăng cường, điều tiên quyết mà ta hướng đến là tìm ra một policy tối ưu. Từđó, agent có thểbiết được nên đưa ra các hành động như thếnào trong từng trạng thái cụthểcủa môi trường.
mô hình trong các phương pháp học sâu được tham sốhoá.
Như đã đềcập, sau khi mô hình hoá bài toán bằng MDP, mục tiêu của ta là tìm ra policy mà đem vềcho ta lượng điểm thưởng tối đa:
max "X t r (st, at) # Đối với trường hợp ngẫu nhiên, policy sẽcho chúng ta một phân phối xác suất Trong phương pháp học tăng cường, ta muốn tìm được một chuỗi các hành động mà dẫn đến cho ta một lượng điểm thưởng kỳvọng lớn nhất hoặc là giảm tối đa phí (cost). Có nhiều cách đểgiải được vấn đềnày:
• Đánh giá mức độtốt khi đạt được từng trạng thái nhất định hoặc là thực hiện các hành động nhất định tại từng trạng thái, từđó chọn hành động tốt nhất (value learning).
• Sửdụng mô hình của môi trường đểtìm ra hành động trảvềđiểm thưởng lớn nhất (ví dụnhư việc tính trước các nước đi trong cờvua khi ta đã biết rõ luật) (model-based learning).
• Tìm ra một policy trực tiếp tối đa hoá lượng điểm thưởng có thểnhận được (policy gradient).
Sau đây ta sẽtìm hiểu sơ qua về3 phương pháp trên.
2.5.3 Học tăng cường dựa vào mô hình Trong bài toán này, ta biết trước được mô hình gồm các tính chất của môi trường, từđó, ta có thểdựđoán được trạng thái tiếp theo khi ta đưa ra một hành động. Một ví dụđiển hình cho bài toán này là cờvua. Trong cờvua, với mỗi trạng thái s là thế cờhiện tại, khi ta đưa ra hành động a, một nước đi nào đó, ta có thểbiết được trạng thái s′ tiếp theo.
Điểm cốt lõi của học tăng cường dựa và mô hình là sửdụng các tính chất đã biết của môi trường, cùng với đó là xây dựng các hàm giá trịđểtìm ra một chuỗi các 2.5.4 Học tăng cường theo giá trị Trong một sốtrường hợp, dù biết đầy đủcác tính chất của môi trường, ta vẫn không thểtựmình tìm được các hành động tối ưu. Đặc biệt là các trường hợp mà các khảnăng có thểxảy ra ởmỗi trạng thái rất lớn. Thay vì thế, ta có thểchuyển qua đánh giá giá trịcủa việc đạt được các trạng thái cụthể, từđó mà tìm ra các hành động tương ứng nhờvào mô hình của bài toán. Sựđánh giá này có thểthực a, Hàm giá trị thái st tại thời điểm t dưới policy hiện tại. Ta cũng có thểhiểu là đơn giản là đây là một hàm có tác dụng đánh giá và dựđoán lượng điểm thưởng mà ta có thểnhật khi đạt một trạng thái và tiếp tục sửdụng policy hiện tại cho các hành động và trạng thái sau đó.
T X t′=t h i Ví dụnhư trong cờvua, sẽcó những nước đi mà ởđó ta có lợi thếlớn so với khi ta chuẩn bịthua, thông thường khi theo một policy tối ưu thì hàm giá trịcho nước đi đó sẽtrảvềgiá trịlớn hơn so với khi ta sắp thua.
Khi ta có được một hàm giá trịhợp lý, có một sốcách đểta tìm được một policy tối ưu tương ứng. Tại mỗi bước thời gian, ta sửdụng V (s) đểtìm ra trạng thái tối ưu trong bước thời gian tiếp theo mà ta muốn đạt tới, sau đó ta sửdụng các thông tin tính chất của môi trường đểtừđó đưa ra được lựa chọn tốt nhất.
Đối với cách bài toán đã biết rõ mô hình thì đây là một điều dễdàng, tuy nhiên trong trường hợp ta không biết rõ các tính chất của môi trường, việc tìm ra lựa chọn tương ứng đểđưa ta tới trạng thái tối ưu tiếp theo mà ta tìm được là rất khó.
b, Tìm kiếm hàm giá trị Trong một sốmôi trường, sốlượng trạng thái là rất lớn. Điều đó dẫn đến việc lưu trữgiá trịcho từng trạng thái trởthành một việc bất khảthi. Đểgiải quyết vấn đềnày, ta có thểứng dụng các kết quảđã đạt được trong học giám sát (supervised learning) và huấn luyện một mô hình có khảnăng ước lượng được V .
X i Trong đó:
• y là giá trịmục tiêu và ta có thểdùng các phương pháp như Monte Carlo [15], Temoral Difference [16] đểtính toán.
c, Hàm giá trịhành động Vấn đềcủa hàm giá trịthông thường cho từng trạng thái là với việc tìm ra trạng thái chính xác, đôi khi ta vẫn không tìm được các hành động tương ứng dẫn ta tới trạng thái đó. Đặc biệt là ởtrong các môi trường mà ta không biết rõ các tính chất, điều là dường như là bất khảthi.
Đểgiải quyết vấn đềnày, ta có thểtính toán giá trịcho từng hành động Q(s, a) thay vì cho mỗi trạng thái của môi trường. Khi này, chúng ta sẽcần phải theo dõi lượng dữliệu lớn hơn (có nhiều action tại mỗi state), tuy nhiên, khi ta tìm được một hàm giá trịhành động đủtốt, ta có thểtừđó mà cho ra ngay lập tức hành động ta cần chọn. Điều này giúp cho ta không còn phụthuộc vào các tính chất của môi trường một cách hoàn toàn.
Khi ta đã có được một hàm Q đủtốt, ta tìm hành động tối ưu a∗= argmaxa Q(s, a) Như vậy, ta có thểnói phương pháp học hàm giá trịhành động là không phụ thuộc vào mô hình.
d, Hàm lợi thế Trong deep learning, gradient descent [17] thường hoạt động tốt nếu các giá trị của chúng ta được cân bằng hoá xung quanh 0. Ý tưởng này cũng được triển khai trong học tăng cường. Hàm lợi thếlà một hàm có tác dụng đánh giá lợi thếcủa một hành động a trong trạng thái s so với mức điểm thưởng kỳvọng thông thường ta có thểnhận được từtrạng thái s.
Trong đó:
hành động at trong trạng thái st và làm theo policy từđó trởđi.
e, Phương pháp Q-learning Epsilon-greedy [3] là một phương pháp giúp ta cân bằng được việc khám phá môi trường và sửdụng policy. Việc khám phá môi trường có mục đích đểcó thể bao quát được toàn bộkhông gian trạng thái và hành động, tránh cho việc agent bị overfit vào một ngữcảnh cụthể. Trong khi đó, việc sửdụng policy là mục đích sau cùng của ta khi đã có một policy tối ưu, từđó các hành động của agent không còn là các hành động ngẫu nhiên mà trên thực tếlà có giá trịgiúp cho ta đưa ra được lựa chọn sau cùng một cách hợp lý.
Cách hoạt động của phương pháp này khá đơn giản, tại mỗi bước thời gian t, agent sẽcó thểchọn giữa việc ra quyết định ngẫu nhiên (explore), hoặc sửdụng policy đểra quyết định (exploit). Điều này được quyết định thông qua hệsốϵ. Khi giá trịepsilon sẽquyết định việc agent chọn explore hay exploit môi trường. Cùng với đó giá trịnày cũng sẽthay đổi theo thời gian, từđó càng vềsau thì tỉlệexploit càng cao. Sau cùng agent sẽđưa ra quyết định hoàn toàn theo policy mà không tiến hành khám phá môi trường nữa.
Thuật toán Q-learning Với những môi trường mà không gian trạng thái và hành động nhỏ, ta có thểlưu tất cảcác giá trịQ cho từng cặp trạng thái và hành động và từđó tạo ra một bảng Q table lưu các giá trịnày. Khi đó, ta sửdụng thuật toán Q-learning [3] đơn giản với Q-table đểtính toán các giá trịQ, thuật toán được trình bày trong mục thuật toán 1.
Algorithm 1 Thuật toán Q-learning Khởi tạo Q(s, a)∀s ∈S+, a ∈A(s) for mỗi tập của môi trường do Khởi tạo s0 for t = 0 tới T do Với xác suất ϵ, chọn một hành động ngẫu nhiên at, hoặc chọn at = maxa(Q(st, a)) Thực hiện hành động at, theo dõi điểm thưởng rt, trạng thái mới st+1 t ←t + 1 end for end for Bằng việc liên tục tương tác với môi trường và lấy được nhiều mẫu, thuật toán giúp ta cập nhập liên tục được các giá trịtrên Q-table. Khi Q-table có các giá trị chính xác, ta có thểsửdụng nó đểgiúp ta đưa ra các hành động (ví dụởtrạng thái st, ta chọn hành động a∗ t trong bảng Q-table mà cho ta giá trịlớn nhất.
f, Phương pháp Deep Q Network Có những trường hợp môi trường có không gian trạng thái và hành động lớn, ta không thểcó đủkhông gian đểlưu được hết từng giá trịnày vào trong một bảng nào đó. Deep Q Network [18] cung cấp giải pháp là sửdụng một mạng neuron network đểthực hiện tính toán một cách gần đúng nhất các giá trịQ. Khi này, thuật toán Q-learning cần phải được thay đổi đểphù hợp, được thểhiện ởtrong mục thuật toán 2.
Algorithm 2 Deep Q-learning sửdụng kho kinh nghiệm Khởi tạo kho kinh nghiệm D với kích thước tối đa N Khởi tạo mạng giá trịhành động Q với tham sốngẫu nhiên.
for mỗi tập của môi trường do Khởi tạo s0 for t = 0 tới T do Với xác suất ϵ, chọn một hành động ngẫu nhiên at, Thực hiện hành động at, theo dõi điểm thưởng rt, và trạng thái mới st+1 Lưu trữbước chuyển (st, at, rt, st+1) vào D Gán st+1 = st Lấy mẫu một lượng nhỏcác tập ởtrong kho kinh nghiệm D Gán yj = ( rj for terminal st+1 for non-terminal st+1 end for end for 1. Lấy các mẫu hành động từmột trạng thái, có thểsửdụng các phương pháp khác nhau đểtăng tính ngẫu nhiên cho các mẫu, từđó giúp cân bằng được sự đánh đổi giữa khám phá và khai phá.
2. Theo dõi lượng điểm thưởng nhận được từmôi trường và trạng thái môi trường sau khi hành động được thực hiện.
3. Sửdụng hàm Q đểtìm hành động a′ đem lại cho ta giá trịQ lớn nhất.
Qk+1(s, a) = E h a′ Qk  St+1, a′ i 2.5.5 Policy Gradient Bên cạnh học tăng cường dựa vào mô hình và giá trị, còn một nhánh các nhóm phương pháp học tăng cường nữa là nhóm học tăng cường dựa vào policy. Các một cách gián tiếp thông qua việc tối ưu hàm giá trịhay dựa vào các bộluật cụthể của môi trường.
Hàm mục tiêu của các phương pháp học tăng cường dựa vào policy gradient [19] sẽlà " H X t=0 # = X Trong đó • PH t=0 R (st, ut) là tổng các điểm thưởng của từng cặp trạng thái và hành động có thểxảy ra.
• P ra.
max X a, Tối ưu hoá Việc tối ưu hoá bài toán trên cần ta phải sửdụng một sốbiến đổi toán học đã được chứng minh.
f(x) Ex∼p(x)[f(x)] = Z p(x)f(x)dx Bài toán tối ưu khi này trởthành "X t r (st, at) # Z Ta tính gradient đểsửdụng cho việc tối ưu Z Z lấy mẫu (sampling) đểtính toán xấp xỉgiá trịđó.
= p (s1) TY t=1 Lấy log 2 vế T X t=1 policy gradient trởthành N N X i=1 T X t=1 !  T X t=1 r (si,t, ai,t) ! Ta dùng policy gradient đã tính toán được này đểtối ưu hoá policy.
Policy Gradient sửdụng Monte Carlo Thuật toán REINFORCE [19] sửdụng Monte Carlo đểtính toán các giá trịđiểm thưởng một cách chính xác. Quá trình này được thực hiện qua việc chạy các chuỗi tương tác hoàn chỉnh đến khi môi trường đạt trạng thái kết thúc, sau đó mới tiến hành tính các giá trịđiểm thưởng trảvềthông qua Monte Carlo. Các giá trịnày được sửdụng đểtính toán policy gradient. Thuật toán được trình bày trong mục thuật toán 3.
Algorithm 3 REINFORCE Chọn sốlượng tập tối đa N for tập n tới N do for K bộcác tập của môi trường do for mỗi bước t trong tập môi trường do Tính giá trịđiểm thưởng chiết khấu Gt ←PT end for Tính giá trịmất mát policy cho toàn bộcác tập trong bộcác tập m T X t Gán n ←n + 1 end for end for Ta có thểsửdụng hàm lợịthếA giống ý tưởng khi sửdụng nó trong Q-learning.
Khi này policy gradient trởthành N N X i=1 T X t=1 Bằng cách này, ta cân bằng được các giá trịđiểm thưởng và giúp cho việc học của agent được dễdàng hơn.
b, Vấn đềcủa các thuật toán Policy Gradient Các phương pháp học tăng cường dựa vào policy gradient được gọi là các liệu không hiệu quảdo toàn bộlượng lớn dữliệu sẽchỉđược dùng cho một bước gradient descent duy nhất.
c, Importance Sampling Đểcải thiện vấn đềsửdụng dữliệu không hiệu quả, importance sampling [20] cho phép ta tính toán lượng điểm thưởng kỳvọng bằng một policy và sửdụng nó đểhọc cho một policy khác.
      Điều này hoàn toàn đúng với kết quảmà ta có được vềpolicy gradient thông thưởng ởtrên.
Khi này, policy gradient trởthành:
" T X t=1  tY t′=1 !  T X t′=t r (st′, at′) !# Như vậy, với hàm mục tiêu ban đầu T X t=1 T X t=1  Áp dụng importance sampling ta có:
= T X t=1    Nếu ta có thểràng buộc sựthay đổi của policy, ta có thểbỏqua phần Khi đó bài toán trởthành:
maximize ˆEt h i subject to Với việc thêm một ràng buộc rằng policy mới và policy cũ không được cách nhau quá xa, ta có thểsửdụng các thông tin thu thập được một cách hiệu quảhơn.
d, Proximal Policy Optimization Phương pháp Proximal Policy Optimization [13] (PPO) được phát triển bởi OpenAI và đạt được những kết quảrất ấn tượng, thểhiện được giá trịứng dụng của học tăng cường trong thực tế. Ý tưởng của phương pháp này là sửdụng importance sampling, nâng cao sựhiệu quảcủa việc sửdụng dữliệu.
là policy cuối cùng gần nhất mà ta dùng đểthu được các chuỗi dữliệu mà MDP trả về. Sửdụng importance sampling, ta tối ưu hoá policy mới bằng dữliệu thu được từpolicy cũ.
maximize ˆEt  ˆAt  Tuy nhiên, khi ta cập nhập policy mới trong mỗi bước tối ưu, khoảng cách giữa 2 policy ngày càng xa, sựước lượng của importance sampling ngày càng lớn. Do đó, sau một sốbước tối ưu hoá policy mới, ta lại phải cập nhật lại policy tương tác với môi trường đểlấy dữliệu:
Đểgiải quyết vấn đềnày, ta thêm vào bước giới hạn sựthay đổi policy. Ta giới hạn điểm lợi thếtính được nếu khoảng cách giữa 2 policy là quá xa. Khi đó, ta sử dụng một hàm mục tiêu mới:
LCLIP E " T X t=0  t  # Chi tiết thuật toán được trình bày trong mục thuật toán 4.
Algorithm 4 Proximal Policy Optimization với hàm mục tiêu bịchặn sốbước cập nhật policy với một bộcác kinh nghiệm thu được for k = 0, 1, 2, ... do t sửdụng các phương pháp tính toán lợi thế Tính toán cập nhật policy LCLIP bằng K bước cập nhật các bộkinh nghiệm thu được với LCLIP E " T X t=0  t  # end for Giải thích hành động hiện đang được chọn có xác suất được chọn cao hơn trên policy hiện tại so với policy cũ. Khi đó có một vài trường hợp mà ta cần chú ý.
năng được chọn cao hơn nhiều so với policy trước. Đây là điều tốt, tuy nhiên, vì ngay thời điểm này nó đã đang làm khá tốt rồi, ta không muốn model tiếp tục học theo hướng này quá nhiều vì điều đó có khảnăng dẫn đến việc bước quá xa và dễdẫn đến một bước cập nhật policy quá mức, dẫn đến hỏng quá trình huấn luyện. Do đó ta giới hạn giá trịnày lại ởmức (1 + ϵ).
policy mới của ta tiếp tục cần phải học theo hướng này (tăng khảnăng chọn hành động a trong trạng thái s hiện tại). Do đó hàm mục tiêu không bịchặn bước cập nhập policy được thực hiện trọn vẹn.
hướng, việc này giúp cho agent có thểtiếp tục cập nhập policy theo hướng đúng nhưng không bịquá mức.
policy hiện tại nên bước cập nhật policy nên khiến cho xác suất hành động này được chọn tại trạng thái hiện tại giảm đi. Tuy nhiên ởđây, ta thấy nó lại đang tăng lên so với bước cập nhật policy trước đó. Điều này chứng tỏbước cập nhật trước đã bịsai. Điểm hay ởđây là vì khi này giá trịlợi thếđang âm, do đó vếphải của hàm min sẽđược chọn, cho phép agent thực hiện một bước cập nhật policy theo hướng ngược lại với lần cập nhật trước với một giá trị không bịchặn, điều này phần nào hoạt động như một cơ chếsửa sai sau khi thực hiện một bước học sai.
Vậy là chỉbằng một hàm mục tiêu duy nhất, đơn giản, không có nhiều ràng buộc, phương pháp Proximal Policy Optimization giúp ta giải quyết được rất nhiều điểm yếu của các phương pháp phụthuộc vào policy gradient truyền thống. PPO hoạt động một cách hiệu quảvềmặt dữliệu, có khảnăng hội tụtốt và tránh được các vấn đềliên quan đến cập nhật policy sai và phá hỏng quá trình huấn luyện.
2.6 Unity3D 2.6.1 Engine trò chơi của Unity3D Unity [21] cung cấp một engine trò chơi và một IDE bao gồm hệthống tương tác với môi trường (editor), hệthống vật dụng (asset), hệthống xây dựng cảnh, hệ thống mạng và một sốyếu tốkhác. Và với những yếu tốtrên, tất nhiên Unity 3D có thểđược sửdụng đểlàm nhiều việc, không chỉbịgiới hạn ởviệc thiết kếtrò chơi.
Unity3D gồm 5 thánh phần chính là project view, scene view, game view, heirarchy view, inspector view.
a, Scene view Đây là phần chính mà ta sửdụng trong quá trình phát triển ứng dụng. Scene view cung cấp cho ta một cái nhìn tổng quát vềngữcảnh của môi trường, trong đó Hình 2.1: Scene View trong Unity Hình 2.2: Game View trong Unity ta có thểthấy các vật thểvà vịtrí của chúng. Scene view được thểhiện trong hình 2.1.
Scene view cho phép người dùng sửdụng các thao tác kéo thảđểdi chuyển và định hình các vật thểtrong môi trường được thực hiện thông qua một sốcông cụ như bộcông cụdịch chuyển (translate tool) dùng đểdịch chuyển các vật thể, bộ công cụxoay (rotation tool) dùng đểthay đổi độquay của vật thểtheo các trục, bộ công cụtăng giảm kích thước (dùng đểthay đổi kích thước của vật thểtheo các trục), và một sốcông cụkhác. Bên cạnh đó thì scene view cũng cho phép người dùng nhìn được toàn thểcác yếu tốcủa môi trường mà một sốtrong đó người chơi không thểnhìn được, ví dụnhư các thông tin vềbộkhung vật lý của vật thể, hay các thông tin phục vụcho việc sửa các lỗi trong quá trình phát triển trò chơi.
b, Game view Game view là phần mà người chơi sẽnhìn thấy khi tương tác với trò chơi được tạo ra. View này cho phép người dùng có thểxem trước được góc nhìn của người chơi, từđó có thểđiều chỉnh các hành động của trò chơi cũng như các vật thểtrong Hình 2.3: Heirarchy View trong Unity trò chơi một cách hiệu quả. Góc nhìn này được lấy thông qua camera chính của trò chơi được thiết lập vịtrí ởtrong scene view, qua đó ta có thểgiới hạn không gian mà người dùng có thểnhìn thấy trong sản phẩm hoàn thiện, thay vì cho họnhìn toàn bộkhung cảnh môi trường. Một ví dụcho game view được thểhiện trong hình 2.2.
c, Hierarchy view Hierarchy view cho phép ta theo dõi toàn bộcác vật thểđang hiện hữu trong môi trường theo dạng danh sách. Ởđây các vật thểđược tạo ra sẽcó dạng cha và con. Các vật thểcon sẽđược nằm trong cha và có thểđược mởđóng thông qua cha.
Trong heirachy view được thểhiện trong hình 2.3, ta có thểthấy tổng quát các vật thểtrong môi trường, trong đó các vật thểcha được gắn hình mũi tên ởbên trái, thể hiện nó có thểđược mởđểta thấy các vật thểcon trong đó.
Hình 2.4: Project View trong Unity d, Project view Project view 2.4 chứa toàn bộcác thông tin liên quan đến dựán hiện tại của ta, bao gồm các cảnh, đoạn mã nguồn, hay các mô hình đồhoạ, và toàn bộcác dữ liệu khác mà ta sửdụng trong dựán. Project view hoạt động gần tương tựnhư các phần mềm duyệt tập dữliệu trong máy tính của chúng ta, tuy nhiên một sốthông tin không cần thiết đã được lược bỏ. Do đó, view này cũng cho phép người dùng tạo, xoá tập dữliệu và quản lý toàn bộcác tập dữliệu này trong dựán một cách có quy củ.
e, Inspector view Inspector view 2.5 cho phép ta theo dõi toàn bộthông tin vềmột vật thể. Những thông tin này rất đa dạng tuỳthuộc vào từng kiểu vật thểmà ta muốn theo dõi, trong đó có những thông tin mà vật thểnào cũng có như vịtrí, độquay, tỉlệ. Một sốthông tin khác có thểcó liên quan đến các yếu tốvật lý, cấu tạo màu sắc của vật thể. Bên cạnh đó thì inspector view cũng cho ta thông tin và khảnăng thay đổi các thông tin được lấy từtrong các mã nguồn kịch bản gắn với vật thể.
f, Hệthống mã nguồn xây dựng hành động Đểcó thểsửdụng Unity như một công cụxây dựng môi trường, ta cần sửdụng hệthống mã nguồn đểlập trình từng hành động của các yếu tốtrong môi trường.
Các thao tác này được thực hiện nhờvào hệthống xây dựng các hành động của vật thểthông qua mã nguồn trong Unity.
Trong Unity, mỗi vật thểđều có thểđược gắn một thành phần cấu tạo là một hoặc nhiều tập mã nguồn C#. Mã nguồn này sẽquản lý các hành vi của vật trong Hình 2.5: Inspector View trong Unity Hình 2.6: Collider của một cá thểxe taxi môi trường theo yêu cầu của người lập trình. Trong đó, vật thểcủa ta sẽkếthừa lớp MonoBehavior, đây là một lớp được thiết kếđểthực hiện các công việc như hiển thịvật, thực hiện các sựthay đổi của vật do ta lập trình. Đểcho lập trình viên có khảnăng này, lớp này cũng có một sốhàm mà ta có thểghi đè đểtuỳbiến hành vi của vật thể.
• Update: Hàm được gọi liên tục sau đã hoàn thành các thao tác của lần gọi trước. Sốlần mà hàm được gọi trong một khoảng thời gian phụthuộc vào hiệu năng của máy và hiệu năng của phần chương trình được viết trong hàm.
• FixedUpdate: Hàm được gọi liên tục với một tần xuất cốđịnh. Giá trịmặc định cho tần xuất này là 50 lần một giây và có thểđược thay đổi bởi người dùng. Do đó hàm này có sốlần gọi cốđịnh trong một giây, điều này giúp ta có khảnăng tính toán một cách chính xác hơn.
• Start: Hàm được gọi khi object được khởi tạo, cho ta khảnăng thay đổi các thông tin trước khi hàm Update và FixedUpdate được gọi.
Bên cạnh đó thì Unity cung cấp một sốAPI hữu dụng khác đểthao tác với các object phục vụcác mục đích khác nhau như tạo vật thểvà xoá vật thểkhỏi môi trường.
2.6.2 Một sốcơ chếthường dùng trong Unity a, Nhận diện va chạm Nhận diện va chạm là nhận biết khi có hai vật thểlấn chiếm lên nhau. Điều này xảy ra khi hai vật có vịtrí gần nhau hơn là mức kích thước của chúng cho phép.
Trong Unity, cơ chếnày thường được lập trình theo hướng sửdụng hệthống khung Hình 2.7: Máy trạng thái hữu hạn cho hệthống đèn giao thông va chạm (collider) của Unity.
Khung va chạm (hình 2.6) có thểhiểu là một bộkhung do người dùng tựđịnh nghĩa cho mỗi vật thể, đây là yếu có tốthểthêm vào hoặc không tuỳthuộc vào mục đích của người lập trình viên đối với từng vật thể. Đểsửdụng cơ chếnày, Unity cung cấp API OnCollisionEnter trong mã nguồn của mỗi vật thể. Hàm này cho phép ta thao tác với khung va chạm của vật thểmà đang va chạm với khung của bản thân. Từđó, ta có thểlấy được hầu hết các thông tin của vật như vịtrí, độ quay, tỉlệvà các thuộc tích công khai khác của vật thểva chạm.
b, Máy trạng thái hữu hạn Máy trạng thái hữu hạn (Finite State Machine [22]) (FSM) là một cách thiết kế mô hình thểhiện cách mà một vật thểtrong môi trường sẽhoạt động. Trong đó, vật thểnày sẽhoạt động dựa vào một sốhữu hạn các trạng thái khác nhau, trong đó, tại mỗi khung thời gian, vật thểnày sẽđối chiếu giá trịtrạng thái hiện tại và với mỗi trạng thái sẽđưa ra hành động cụthể. Bên cạnh đó, việc xửlý thông tin đểthay đổi trạng thái hợp lý hoàn toàn được tách biệt với quá trình hành động.
FSM có thểđược áp dụng vào việc thiết kếhệthống lớn như quản lý môi trường, hay cũng có thểsửdụng trong từng object nhỏtrong môi trường như là mỗi xe. Một ví dụvềFSM được thểhiện trong hình 2.7 với các trạng thái là xanh, vàng và đỏ.
Trong đó, mỗi trạng thái sẽcó cơ chếchuyển trạng thái khác nhau, ví dụtừđèn xanh khi hết thời gian sẽchuyển sang vàng, khi hết thời gian vàng thì chuyển qua đỏ. Bên cạnh đó, một sốtrạng thái nhất định sẽchỉchuyển được tới một vài trạng thái nhất định khác, ví dụnhư đỏkhông thểchuyển thành vàng. Một điểm quan trọng khác là cơ chếtác động của đèn lên môi trường không phụthuộc vào phần cơ chếthay đổi trạng thái. Tại mỗi khung thời gian của môi trường, hệthống chỉkiểm tra trạng thái hiện tại và hoạt động theo logic của trạng thái đó.
c, Hệthống đếm giờ Hệthống đếm giờlà phương pháp được sửdụng khá nhiều trong Unity. Phương pháp này được sửdụng khi ta cần thực hiện một việc nào đó sau một khoảng thời gian cho trước. Các ví dụcó thểkểđến như các đồng hồđếm giờ, đồng hồđếm ngược, hay hệthống điểm thưởng theo giờ,... Chính vì việc có nhiều trường hợp sử dụng như vậy, Unity cung cấp cho ta một sốgiải pháp liên quan đến tính toán thời gian, một giải pháp được sửdụng nhiều nhất đó là API Time.deltaTime. Đây là một phương thức tĩnh của lớp Time, cho phép ta trảra khoảng cách giữa khung thời gian hiện tại và ngay trước đó, từđó ta có thểsửdụng giá trịnày phù hợp với mục đích của mình.
Với ví dụvềviệc sửdụng hệthống đếm giờtrong việc sinh xe trong một khoảng thời gian. Ta sửdụng một biến đểtích trữthời gian, khởi đầu là 0, sau đó tại mỗi khung thời gian ta tiến hành cộng nó với giá trịTime.deltaTime, và kiểm tra xem nó đã vượt qua khoảng thời gian mà ta mong muốn chưa, nếu đã vượt qua thì ta tiến hành thực hiện hành động cần thiết và chuyển lại giá trịtích trữvề0.
d, Hệthống Prefab Prefab là các vật thểđược tạo trước, chính vì vậy hệthống prefab ởđây là một hệ thống cho phép ta tạo và quản lý các vật thểmột cách bài bản. Có thểhiểu prefab như một bản mẫu cho một object được cấu tạo một cách hoàn chỉnh gồm bất kỳyếu tốnào mà lập trình viên mong muốn. Các prefab này sau đó được chuyển thành các vật thểtrong môi trường thông qua việc người dùng thêm chúng vào trong scene view hoặc thông qua việc tạo chúng ởtrong mã nguồn.
Đối với việc tạo vật thểtrong scene view, hệthống này cho ta quyền quản lý các vật thểnày thông qua prefab của chúng. Ta có thểthay đổi một sốthuộc tính trong mỗi vật thểtrong môi trường một cách khác nhau, khiến chúng khác đi so với prefab. Tuy nhiên khi cần, ta có thay đổi một sốthông tin trong prefab vào áp dụng chúng vào toàn bộcác vật thểđang có trong môi trường. Việc này giống với cơ chếkếthừa trong lập trình hướng đối tượng.
Đểtạo các vật thểtrong môi trường từnhững prefab này thông qua lập trình mã nguồn, Unity cung cấp API Instantiate. API này nhận vào một prefab và một sốthông tin như vịtrí tạo, độquay, và vật thểcha, sau đó tạo vật thểnày trong môi trường và trảvềchính nó. Từđó, ta có thểthay đổi một sốthuộc tính công khai của vật thểphù hợp với mục đích của mình. Cơ chếtạo vật thểcho phép ta tựđộng hoá quá trình tạo vật thểtrong môi trường, rất hữu dụng trong một sốtrường hợp cụ thể.
2.7 Unity ML-Agents Bộcông cụUnity ML-Agents [23] cung cấp một SDK được phát triển bởi Unity và được sửdụng trong các môi trường Unity đểtạo ra một môi trường học máy. Bộ công cụcho phép ta tạo ra một môi trường mô phỏng trong Unity có các yếu tốđể Hình 2.8: Sựtương tác của các thành phần trong bộcông cụML-Agents hỗtrợcho việc học máy, bên cạnh đó cũng cung cấp cho ta một hệthống giao tiếp với môi trường này thông qua các Python API. Các thành phần chính trong Unity ML-Agents là cảm biến, các chủthểhành động (agent), và một hệthống học viện (academy).
Unity ML-Agents biến một môi trường thông thường trong Unity trởthành một môi trường học máy. Trong đó, các agent hoạt động thông qua các bước thời gian.
Trong mỗi bước thời gian, từng agent sẽthu thập các thông tin vềmôi trường và sửdụng chúng và policy tương ứng đểtìm ra hành động tối ưu cho trạng thái hiện tại, đồng thời nhận vềđiểm thưởng cho từng hành động. Việc biến đổi các vật thể thông thường thành các agent được thực hiện thông qua việc kếthừa lớp Agent được cung cấp bởi bộcông cụ. Lớp Agent cung cấp cho ta các chức năng mà lớp MonoBehavior có, đồng thời cung cấp thêm các logic liên quan đến môi trường học máy và một sốphương thức giúp ta tuỳchỉnh phương thức tương tác của agent với môi trường. Một sốphương thức quan trọng mà ta cần thực hiện ghi đè đểhoàn thiện mã nguồn Agent gồm có:
• OnEpisodeBegin: Cho phép lập trình viên tuỳbiến các giá trịhay các hoạt động mỗi khi tập của môi trường được bắt đầu.
• ResetEnv: Cho phép ta kết thúc tập hiện tại, có thểsửdụng kèm một sốđiều kiện nhất định đểđạt được các cơ chếhoạt động mong muốn.
• CollectObservations: Cho phép ta thiết lập các thông tin mà agent nhận được trong mỗi lần yêu cầu hành động, ởđây ta có thểhiểu là tương ứng với trạng thái s.
• OnActionReceived: Trảvềcho ta hành động của agent và cho phép ta thực hiện thay đổi môi trường tương ứng với hành động đó.
Bên cạnh đó, ta cũng được quyền thay đổi một sốthông tin liên quan đến hành vi của agent thông qua một sốthuộc tính công khai của agent trong Inspector View 2.5. Trong đó có một sốthông tin quan trọng như:
• Kích thước sốchiều của các quan sát vềmôi trường.
• Kích thước sốchiều của hành động.
• Sốbước tối đa - sốbước thời gian tối đa trong một tập môi trường, khi bước thời gian đạt giới hạn này, môi trường sẽtựđộng thực hiện ResetEnv mà không cần chờđiều kiện mà lập trình viên thiết kế.
Hệthống học viện (academy) sẽlà thành phần thực hiện quản lý các agent. Các chức năng của học viện bao gồm theo dõi các bước thời gian cho từng agent và quản lý các thông tin liên quan đến agent. Trong quá trình huấn luyện, học viện sẽđóng vai trò giao tiếp với các mã nguồn huấn luyện trong Python và chạy một loạt các tập của môi trường khác nhau. Trong mỗi tập này sẽbao gồm nhiều bước thời gian, tại đó học viện tổng hợp các quan sát từagent và gửi qua mã nguồn huấn luyện, nhận lại các hành động tương ứng và gửi chúng lại cho từng agent.
Các mã nguồn huấn luyện trong Python không phải là một phần của môi trường.
Các mã nguồn này chứa các thuật toán học máy khác nhau đểcó thểhuấn luyện agent trông môi trường học. Python API là một giao thức giữa ứng dụng huấn luyện và hệthống giao tiếp. Hệthống giao tiếp nằm ởtrong môi trường và có tác dụng kết nối môi trường này tới Python API.
Các thành phần trong bộcông cụML-Agents được thểhiện trong hình 2.8.
2.8 Kết chương Trong chương này, em đã trình bày vềcác lý thuyết và kỹthuật cần có đểcó thểthực hiện giải pháp được đềxuất. Các phương pháp tối ưu dựa vào mô hình có điểm mạnh là sựđơn giản nhưng lại phụthuộc hoàn toàn vào môi trường bài toán. Phương pháp tiên tiến hơn là Deep Q Network đã khắc phục điểm yếu trên nhưng vẫn tồn tại một sốvấn đềvềtính tối ưu do không trực tiếp tối ưu hoá policy.
Phương pháp Proximal Policy Optimization thểhiện khảnăng tối ưu tốt và tốc độ hội tụnhanh, phù hợp với phần lớn các bài toán hiện nay. Bên cạnh đó, ta cũng tìm hiểu vềcác kĩ thuật cơ bản trong Unity và gói công cụUnity ML-Agent, từđó giúp ta có khảnăng phát triển một môi trường hoàn thiện và kết nối tới các thuật toán huấn luyện. Những kiến thức này sẽđược sửdụng trong chương tiếp theo đểphát triển và giải quyết bài toán đềra.
3.1 Tổng quan Với những kiến thức nền tảng được trình bày từChương 2, em tiến hành áp dụng chúng vào việc phát triển môi trường và sửdụng phương pháp Proximal Policy Optimization đểgiải bài toán trên môi trường đã được xây dựng. Trong chương này, em sẽtrình bày vềcác gói hỗtrợ, thư viện được sửdụng, quá trình phát triển môi trường và ứng dụng PPO đểtìm được một policy tối ưu cho môi trường đó.
3.2 Tổng quan giải pháp Trong đồán này, em đưa ra giải pháp sửdụng Unity và bộcông cụML-Agents đểxây dựng một môi trường học cho bài toán hệthống đèn giao thông thông minh.
Em sửdụng phương pháp học tăng cường và cụthểlà giải thuật Proximal Policy Gradient, tương tác với môi trường đã tạo thông qua các Python API được cung cấp, từđó xây dựng được một policy hợp lý và tối ưu, giúp cho agent đưa ra được các quyết định vềtín hiệu đèn giao thông một cách hợp lý.
3.3 Xây dựng ngữcảnh môi trường trong Unity 3.3.1 Các gói hỗtrợ a, Mô hình độhoạđơn giản Đểcó thểphát triển được một môi trường chân thực, giống với thực tế, em sử dụng gói đồhoạgiao thông đơn giản được mua ởtrên hệthống cửa hàng của Unity.
Gói này cung cấp cho ta các mô hình đồhoạchân thực liên quan đến giao thông như đường, vạch phân cách, xe cộ,... Với các mô hình đồhoạcó sẵn này, ta có thể bỏqua bước thiết kếđồhoạđểgiảm thiểu được thời gian phát triển môi trường, phù hợp với phạm vi của một đồán. Một sốcác mô hình đồhoạđược gói này hỗ trợđược thểhiện ởhình 3.1.
b, Hệthống tạo đường đi Đểcó thểđịnh hướng di chuyển của các phương tiện trong môi trường một cách chính xác và giống với thực tế, em sửdụng gói hỗtrợtạo đường đi là Path Creator. Tác dụng của gói hỗtrợnày là tạo cho ta các đường cong và cung cấp cho ta một sốAPI đểlấy được điểm nằm trên đường cong trên gần nhất với một điểm mà ta cần tìm. Từđó ta có thểsửdụng thông tin này vào việc di chuyển của các phương tiện, giúp các phương tiện của ta có thểdi chuyển trong môi trường một cách hợp lý, góp phần giúp cho môi trường trởnên thêm chân thực và thểhiện được đúng các tính chất của việc di chuyển ngoài đời thực.
Hình 3.1: Gói đồhoạgiao thông đơn giản Hình 3.2: Nền của môi trường ngã tư đơn giản Hình 3.3: Hệthống lộtrình 3.3.2 Nền của môi trường Với bài toán đặt ra là hệthống đèn giao thông tại một ngã tư đơn giản, em đã xây dựng một ngã tư có với 4 hướng đường hai chiều, trong đó mỗi chiều có 2 làn đường xe chạy. Cụthểphần nền của môi trường được trình bày như hình 3.2. Như có thểthấy trong hình, môi trường sẽbao gồm 4 hướng đi giao nhau tại một ngã tư.
Ởđây, mỗi hướng đi sẽcó một đèn giao thông tương ứng quyết định việc có cho xe di chuyển hay không. Hệthống bao gồm 4 đèn giao thông này sẽđóng vai trò quản lý việc lưu thông của các phương tiện giao thông trong ngã tư này, đồng thời cũng chính là đối tượng mà ta đang hướng tới đểtối ưu hoá các quyết định.
Trước hết, em sửdụng các mô hình đường đểtạo ra các làn và sau đó là thêm vào các hình giải phân cách đểphân chia làn đường. Tiếp đó là thêm vào phần vỉa hè đểhoàn thiện mô hình một làn đường đơn giản. Từđó ta có thểlặp lại các bước trên và xây dựng được mô hình nền đường hoàn chỉnh.
Đểcác xe có thểdi chuyển trên môi trường, ta tạo các đoạn đường lộtrình mà các xe sẽchọn đểđi theo, các đoạn đường này sẽđược lưu lại và trong thao tác tạo ra xe và thêm vào môi trường, các xe đồng thời cũng sẽđược gắn cho một lộtrình cụthểđểđi theo, từđó đảm bảo được tính đúng đắn của môi trường. Các đoạn đường được thêm vào môi trường được thểhiện bởi các đường màu xanh trong hình 3.3.
3.3.3 Hệthống phương tiện Đểtạo được một hệthống phương tiện hợp lý phù hợp với nhu cầu của bài toán, ta cần phải hiểu là đây không thểlà các object cốđịnh luôn hiện hữu trong môi trường được, ta cần liên tục sinh ra vật thểvà xoá chúng đi sau khi đã đi ra khỏi môi trường. Bên cạnh đó thì mỗi cá thểphương tiện sẽlà yếu tốchịu trách nhiệm cho việc đảm bảo các yêu cầu của môi trường giao thông, các cơ chếtránh va chạm hay dừng đèn đỏ.
a, Hệthống quản lý phương tiện Đểđảm bảo môi trường có tính bao quát, thểhiện được toàn bộcác tình huống của có thểxảy ra của môi trường thực tế, em sửdụng một sốthông sốđểkiểm soát môi trường.
• Giãn cách tạo xe: Khoảng cách giữa 2 lần tạo xe và thêm vào môi trường, việc này giúp ta có thểkiểm soát được mật độxe nói chung trên toàn bộmôi trường.
• Thứtựưu tiên của các làn: Độưu tiên dành cho từng làn đường khác nhau, làn nào có độưu tiên cao thì khảnăng xe được sinh ra trên làn đó càng cao. Điều này giúp ta có thểtạo được những tình huống khác nhau của giao thông hiện tại.
• Độchênh lệch của các mức ưu tiên: Bên cạnh thứtựưu tiên, ta cũng dùng các tham sốđểkiểm soát độchênh lệch của các mức ưu tiên với nhau, kết hợp với sựthay đổi thứtựưu tiên giúp ta tạo được một môi trường đa dạng, mô phỏng được các trường hợp có thểxảy ra ngoài thực tế.
Bằng việc tạo ra và kiểm soát, thay đổi các thông sốtrên liên tục, ta có thểtạo ra một bộmô phỏng hoàn thiện, ứng với môi trường đềra. Tương ứng với mỗi tập của môi trường học, hiện tại đang là 5 phút, các thông sốtrên sẽđược thay đổi sao cho vừa đủđểcó thểhoàn thành được một bộtrạng thái. Cụthể, với mỗi tập của môi trường:
• Giãn cách tạo xe sẽthay đổi từ0.5s tới 1.5s. Điều này khiến cho lượng xe trên toàn bộmôi trường được thay đổi. Ởtập tiếp theo của môi trường, sựthay đổi này sẽngược lại, tức là từ1.5s tới 0.5s. Điểu này là đểđảm bảo hoạt động tốt với các thông sốthứtựưu tiên lẫn mật độcủa các mức ưu tiên.
• Thứtựưu tiên giữnguyên đểtạo môi trường với ưu tiên đó, và thay đổi khi ta chuyển qua tập mới của môi trường.
• Độchênh lệch của các mức ưu tiên sẽliên tục chuyển từ(0.25, 0.25, 0.25, 0.25) thành (0.55, 0.2, 0.15, 0.10) trong quá trình 5 phút. Tức là trong 5 phút này ta liên tục có các độchênh lệch khác nhau của môi trường thoảmãn với một thứtựưu tiên cốđinh.
Với phương pháp trên, sau 24 tập của môi trường, ta có được đầy đủcác thứtự ưu tiên khác nhau của môi trường, và với mỗi thứtựưu tiên khác nhau, ta lại có các mật độkhác nhau cho các mức ưu tiên. Kết hợp với giãn cách tạo xe, sau 48 tập của môi trường, ta có thểcó được toàn bộcác trạng thái vềmặt lưu lượng ởtừng làn có thểxuất hiện trong môi trường.
Sau khi đảm bảo được tính bao quát trên, tiếp đến là bước thực hiện sinh xe. Với việc các mô hình xe khác nhau được thiết kếhoàn thiện, sau mỗi khoảng giãn cách được định nghĩa ởtrên, ta tiến hành tạo ra xe, đồng thời gắn cho mỗi xe một lộ trình. Việc lựa chọn lộtrình là một quá trình ngẫu nhiên có kiểm soát, được định đoạt thông qua các thông sốvềthứtựưu tiên và mật độcủa các mức ưu tiên. Việc lựa chọn lộtrình sẽgiúp cho ta có thểsửdụng được các thông sốtrên đểkiểm soát môi trường.
Đểđảm bảo không xảy ra hiện tượng các xe khi sinh ra bịchèn vào nhau, sau khi ta tìm được một lộtrình, ta sẽtạo ra một điểm ngẫu nhiên gần với đầu của lộ trình, điểm này sẽđược coi là điểm xuất phát. Tuy nhiên, ta cần kiểm tra xem khu vực xung quanh điểm xuất phát này có phương tiện nào đang lấn chiến không, nếu có ta sẽcần chọn lại một điểm xuất phát mới. Sau khi đã tìm được một điểm xuất phát đảm bảo có thểsinh xe an toàn, công việc sinh xe mới được diễn ra thông qua cơ chếInstantiate của Unity.
Bên cạnh việc liên tục tạo ra các xe, đểđảm bảo môi trường được diễn ra trơn tru, ta cũng cần tiến hành xửlý các vấn đềliên quan đến tương tác của các xe với nhau. Ởđây, đểđảm bảo được hiệu năng của môi trường, tránh việc mô phỏng quá nặng, sau khi xe đi hết khỏi phần đường trước đèn đỏ, các cơ chếtránh va chạm của xe sẽđược giảm thiểu. Điều này có thểđược thực hiện là do đối với bài toán hiện tại, ta chỉquan tâm vềphần di chuyển trước đèn đỏcủa xe. Thêm vào đó, ta sẽtiến hành xoá xe khỏi môi trường khi xe đi tới điểm kết thúc của lộtrình.
b, Hệthống hoạt động của các phương tiện Các xe trong môi trường hoạt động theo cơ chếđi theo một lộtrình đã có. Các lộtrình này đã được gán cho mỗi xe trong quá trình tạo ra xe trong môi trường, là yếu tốchính quyết định sựdi chuyển của xe. Bên cạnh đó, ta có thêm thông sốvề vận tốc, giúp ta kiểm soát được tốc độxe di chuyển theo lộtrình định sẵn.
Đểcho việc xe di chuyển trong môi trường một cách hợp lý và đồng thời có được các cơ chếtránh va chạm, các xe sẽhoạt động theo cơ chếmáy trạng thái hữu hạn (FSM). Các trạng thái của xe gồm có:
• DRIVE: di chuyển thông thường với vận tốc tối đa đã định nghĩa.
• RED_LIGHT_STOP: vào trạng thái dừng do gặp đèn đỏ.
• VEHICLE_COLLISION_STOP: vào trạng thái dừng do gặp phương tiện.
• ACCELERATE: vào trạng thái tăng tốc đểchuyển từtrạng thái dừng sang di chuyển thông thường.
Quá trình thay đổi trạng thái này được thực hiện thông qua một cảm biến được gắn ởđầu xe, cảm biến này cho ta biết trong đoạn thẳng trước mặt có vật thểnào không và đồng thời biết được phân loại của object đó. Việc này được thực hiện thông qua hệthống RaycastHit của Unity, cho phép ta kiểm tra thông tin trên một đoạn thẳng trước mặt. Trong trường hợp có vật cản được nhận diện trong khoảng đoạn thẳng trước mặt, ta có thểbiết được khoảng cách của vật cản, đồng thời thay đổi trạng thái của xe nếu các điều kiện tránh va chạm thoảmãn:
• Nếu khoảng cách đủnhỏvà phân loại của vật cảm phía trước là đèn giao thông thì ta chuyển trạng thái thành RED_LIGHT_STOP.
• Nếu khoảng cách đủnhỏvà phân loại của vật cản phía trước là phương tiện thì ta chuyển trạng thái thành VEHICLE_COLLISION_STOP.
• Nếu phía trước không có vật cản và trạng thái hiện tại đang là một trạng thái dừng, lúc này có nghĩa là xe đang dừng và cần bắt đầu di chuyển, ta chuyển trạng thái thành ACCELERATE.
• Nếu phía trước không có vật cản và vận tốc hiện tại đang đạt vận tốc tối đa, ta chuyển trạng thái hoặc giữnguyên trạng thái là DRIVE.
Với các cơ chếtrên, ta có thểđảm bảo xe lưu thông theo đúng những luật lệđề ra, phù hợp với thực tế. Đồng thời ta cũng có thểđảm bảo được quá trình dừng đèn đỏcủa xe khi kết hợp với việc bật tắt đèn đểtạo ra các vật cản ảo.
Khi xe dừng lại do đèn đỏhoặc do một xe khác đang chịu tác động của đèn đỏ, ta sẽlưu lại các giá trịvềthời gian dừng, giá trịnày sẽcó tác dụng đánh giá độhiệu quảcủa tín hiệu đèn giao thông, từđó đưa ra các mức điểm thưởng phù hợp.
3.3.4 Hệthống đèn giao thông Hệthống đèn giao thông cho môi trường một ngã tư đơn giản bao gồm 4 đèn giao thông tương ứng với 4 hướng đi của ngã tư, được kiểm soát bởi một vật thể cha đóng vai trò quản lý hoạt động của 4 đèn này.
Cách hoạt động của hệthống này là tại các vịtrí đèn đỏ, phần khung va chạm của vật thểđèn giao thông đó sẽđược chuyển trạng thái thành bật. Việc này sẽgóp phần tạo thành một vật cản vô hình mà các xe có thểthấy được thông qua cảm biến đặt phía trước xe. Đểđạt được điều này, ta thêm mã nguồn C# vào vật thểquản lý hệthống đèn, cho nó quyền thao tác với 4 đèn giao thông tương ứng. Trong đó, mỗi khi chuyển trạng thái đèn, ta cần thay đổi các thông sốcủa cả4 đèn trong hệ thống trên. Đồng thời, ta cần có thay đổi đểcó thểdễdàng quan sát tín hiệu đèn hơn, do khi đặt hệthống máy quay lên trên cao thì mô hình đèn trởnên khá nhỏvà khó quan sát. Đểkhắc phục vấn đềnày, ởmỗi đèn ta thêm một khối cầu đơn giản có tác dụng thểhiện màu đèn. Sựthay đổi màu của khối cầu cùng đồng thời được thực hiện cùng với việc thay đổi trạng thái đèn, đảm bảo cung cấp được các thông tin chính xác vềmôi trường.
Các logic trên được chuyển hoá thành một API, từđó bất kỳvật thểnào có quyền truy cập API này đều có khảnăng thay đổi trạng thái đèn. Điều này giúp cho agent của ta có khảnăng thực thi hành động, tạo ra các tác động trên môi trường, từđó có thểtạo thành một môi trường huấn luyện hoàn chỉnh.
3.4 Cấu hình Markov Decision Process cho môi trường Cấu hình Markov Decision Process cho môi trường hiện tại bao gồm một số bước cơ bản như định nghĩa một tập (episode) của môi trường, định nghĩa trạng thái (state) của môi trường, định nghĩa hành động (action) của agent, tạo quy luật điểm thưởng. Với nhu cầu của từng bài toán khác nhau, các định nghĩa và quy định trên có thểđược thay đổi đểphù hợp với bài toán đó. Do đó, việc định nghĩa một cách chi tiết các yếu tốcủa MDP trong bài toán hiện tại là việc quan trọng hàng đầu.
3.4.1 Tổng quan Đối với bài toán hiện tại, môi trường được đặt ra là một mô phỏng ngã tư đơn giản với 4 hướng đi, mỗi hướng đi bao gồm 2 làn đường. Giãn cách đưa ra quyết định của agent là 5 giây, giúp cho việc quy định ý nghĩa của hành động được rõ ràng hơn. Ví dụ, ta có thểthấy, nếu ta đưa ra quyết định mỗi khung thời gian với khoảng 50 khung thời gian một giây thì từng quyết định một sẽkhông đáng kểvà phải lặp rất nhiều lần đểtạo thành một chuỗi thời gian đèn xanh hoặc đỏ, điều này ảnh hưởng rất nhiều tới việc học của agent. Trước mỗi khi đưa ra quyết định, agent sẽnhận được thông tin trạng thái của môi trường. Ởtrong bài toán hiện tại, trạng thái của môi trường được thểhiện thông qua thông tin vềcác cảm biến đặt trên đường, chúng sẽtrảvề1 nếu có xe đang đứng ởtrên và 0 trong trường hợp ngược lại. Cùng với đó, thời gian dừng tích luỹcủa các phương tiện trong khoảng thời gian 5 giây trước đó được tính vào làm điểm thưởng cho agent cho hành động trước đó.
Đểthực hiện các cấu hình này, ta cần thêm vào vật thểquản lý hệthống đèn giao thông một tệp mã nguồn C# là TrafficLightAgent. Mã nguồn này sẽkếthừa lớp Agent được cung cấp bởi gói ML-Agents, giúp ta thực hiện việc quản lý môi trường và giao tiếp với agent từmã nguồn trong python.
3.4.2 Thiết kếtừng tập của môi trường Mỗi tập của môi trường sẽbao gồm 64 bước. Điều này giúp cho mỗi tập sẽ bao quát được một mức thứtựưu tiên của các phần đường, giúp cho việc quản lý môi trường dễdàng hơn. Mỗi tập sẽbắt đầu bằng một môi trường trống không có phương tiện, và từđây là các phương tiện sẽđược sinh ra theo quy luật đã được trình bày ởtrên. Ởđây, do đặc thù của bài toán, ta không có các bước thời gian kết thúc giúp ta kết thúc tập, mà chỉthực hiện làm mới môi trường mỗi khi đã đạt 64 bước thời gian. Khi được làm mới, môi trường sẽtiến hành xoá toàn bộcác phương tiện đang hoạt động trong đó, đồng thời đưa các giá trịtích luỹtrởvềgiá trịmặc định.
Ta tiến hành thay đổi một sốthông sốcủa TrafficLightAgent đểphù hợp với các thông tin trên. Vì bản thân gói ML-Agents không hỗtrợgiãn cách đưa quyết định của môi trường mức 5 giây, ta cần vào phần mã nguồn gốc của gói đểthay đổi. Sau đó ta chuyển giá trịDecisionPeriod (tương đương với sốkhung thời gian của môi trường giữa 2 lần yêu cầu quyết định từagent) thành 250. Ởđây, môi trường hoạt động với 50FPS, do đó 250 tương ứng với 5 giây thực tế.
Trong mã nguồn của agent, ta tính sốlần đưa ra quyết định của agent, sau khi sốnày đạt 64, ta thực hiện việc làm mới môi trường. Ởđây, ta lấy toàn bộcác vật thểnằm trong vật chứa và tiến hành xoá từng cá thể. Sau đó, ta chuyển các giá trị khác vềgiá trịkhởi tạo của chúng. Sau quá trình này, ta sẽcó một môi trường ở trạng thái bắt đầu, với các thông sốvềthứtựưu tiên, mật độưu tiên, mật độxe khác nhau.
3.4.3 Thiết kếthông tin trạng thái của môi trường Đểlấy được thông tin vềmật độxe của từng làn đường của môi trường, ta sử dụng các cảm biến được đặt dưới đường. Từng cảm biến này sẽcho ta thông tin về sựtồn tại của các phương tiện tại từng điểm trên môi trường. Ởmôi trường cơ bản, em sửdụng 252 cảm biến đặt ởkhắp các làn đường đi tới ngã tư (ởđây ta không cần quan tâm tới thông tin vềcác làn đường sau khi ra khỏi ngã tư). Các thông tin này được lấy thông qua việc kiểm tra sựhiện diện của vật thểởtrong một vùng, được cung cấp thông qua phương thức OverlapBox được Unity cung cấp. Các thông tin này sẽđược chuyển thành một vector rồi gửi tới cho agent đểxửlý.
3.4.4 Thiết kếhành động của agent Đối với môi tường đơn giản, hành động của agent chỉlà các quyết định gián đoạn (discrete) với lựa chọn một hành động tại một thời điểm, ởđây agent sẽđưa ra giá trị0 hoặc 1 tương ứng với việc cặp đèn 0 hoặc 1 chuyển thành màu đỏ. Khi đó, sửdụng API ta có từscript gốc của hệthống đèn giao thông, ta có thểthực hiện hành động này lên môi trường.
3.4.5 Thiết kếhệthống điểm thưởng cho từng hành động Điểm thưởng cho agent trong từng hành động sẽđược tích luỹtrong vòng 5 giây sau khi thực hiện hành động đó. Ởđây, điểm thưởng được sửdụng chính là âm của tổng thời gian mà từng phương tiện phải dừng lại từhệquảcủa đèn đỏ. Bên cạnh đó, đểđảm bảo tránh trường hợp một phương tiện bất kỳphải chờquá lâu, điểm thưởng này cũng được nhân lên cùng với sốthời gian mà xe đã chờ. Điều này giúp cho ta cân bằng được việc tối ưu tổng thời gian chờcho toàn bộxe cũng như không đểtừng xe phải chờquá lâu.
Đểtích luỹđược điểm thưởng cho toàn bộxe trong môi trường, việc cập nhật điểm thưởng phải diễn ra trên từng xe chứkhông phải trong bản thân agent. Do đó, trong mỗi khung thời gian, từng xe sẽcó cơ chếtích luỹthời gian chờvà thêm nó vào trong điểm thưởng của môi trường. Như vậy, con sốnày trong agent sẽđược tích luỹliên tục trong quá trình 5 giây, tăng được sựchính xác cho việc đánh giá.
3.5 Tạo cơ chếhành động cơ sởcủa bài toán Đểđánh giá hiệu năng của các phương pháp được sửdụng, trước hết ta cần có một bản mô phỏng hệthống đèn giao thông chưa tối ưu và đánh giá nó đểlàm mức hiệu năng gốc, từđó ta mới đánh giá hiệu năng của các phương pháp khác so với mức gốc đó. Như đã đềcập, nguyên lý hoạt động của hệthống đèn giao thông chưa tối ưu hiện tại là sửdụng các dữliệu vềlưu lượng giao thông trung bình trong thời gian dài đểtìm ra được một bộsốchia thời gian phù hợp với các tình huống trung bình của môi trường. Việc tính toán này có thểchia thành nhiều quãng thời gian trong ngày, ví dụnhư buổi sáng hoặc buổi chiều có thểcó các mốc thời gian khác nhau. Từcơ sởnày, em xây dựng hệthống mô phỏng lại nguyên lý hoạt động trên.
Do mỗi trường có sựthay đổi liên tục vềthứtựưu tiên của các làn cũng như mật độxe trên từng mức độưu tiên, đểviệc thiết lập một hiệu năng cơ sởđược hợp lý nhất, em đã xây dựng các luật chia thời gian cụthểcho từng giai đoạn của môi trường. Cụthể, đểtìm được giá trịtỉlệxe của 2 cặp hướng đi vuông góc nhau ở ngã tư, ta lấy giá trịtrung bình mật độcủa 2 cặp hướng đi chia cho nhau. Ví dụ với thứtựưu tiên là (0, 1, 2, 3) tương ứng với 4 hướng đi mà ởđây (0, 2) và (1, 3) vuông góc với nhau, ta lấy giá trịmật độưu tiên trung bình của 2 cặp này đểtìm ra tỉlệhợp lý là 0.55+0.25+0.15+0.25 0.20+0.25+0.10+0.25 = 1.5. Như vậy, trong trường hợp này, tỉlệmật độxe trung bình của hai phần đường tương ứng tín hiện giao thông ngược nhau là 1.5. Ta có thểsửdụng tỉlệnày như một sựtham khảo cho việc tạo ra một bộsố thời gian đèn một cách hợp lý. Tất nhiên, với chỉtỉlệtrên thì ta không đủđểxây dựng một bộhệsốtối ưu, do đó, em đã thửnhiều bộsốthời gian khác nhau đểtìm ra được bộsốtối ưu nhất cho các đoạn đường. Với bài toán này, em chọn quy luật phân chia thời gian như sau:
ứng với 7.5 giây đối với ví dụtrên do ratio ởđây là 1.
10s đối với ví dụtrên do ratio ởđây là 1.5.
3.6 Huấn luyện mô hình giải bài toán Sửdụng phương pháp Proximal Policy Gradient, em tiến hành huấn luyện agent đểtối ưu hoá policy của nó đểgiải được bài toán đềra.
Trong đó, policy của bài toán được thểhiện thông qua một mạng neuron cơ bản với đầu vào là 252 đơn vịtương ứng với 252 giá trịthông tin nhận được từcác cảm biến (ởđây chính là trạng thái s của môi trường). Từ252 node này, mạng sẽbao gồm 5 lớp ẩn với 128 đơn vịẩn ởmỗi lớp. Đầu ra của mạng là 2 đơn vịtương ứng với giá trịchọn từng hành động (ởđây chính là chọn tín hiệu cho từng cặp đèn đối xứng).
3.6.1 Tổng quan Trong quá trình huấn luyện, agent và môi trường sẽđược chạy liên tục nhiều lần đểtích luỹđược lượng dữliệu dưới policy hiện tại, lượng dữliệu này sẽđược lưu lại trong bộnhớ. Sau đó khi lượng dữliệu này đủlớn, ta tiến hành sửdụng các thông tin trên đểcập nhật policy hiện tại. Ởđây, như đã trình bày trong phần lý thuyết, thuật toán PPO cho phép ta sửdụng dữliệu này cho việc cập nhật policy này nhiều lần, miễn là sốlần thực hiện đảm bảo policy không thay đổi quá xa so với policy gốc. Với quá trình như vậy, sau một khoảng thời gian đủlớn và với những cấu hình phù hợp của môi trường cũng như những tham sốcủa việc huấn luyện, agent sẽhọc được một policy hợp lý và giải quyết được môi trường một cách tối ưu.
3.6.2 Quá trình tích luỹthông tin Ta tạo một bộnhớđểagent có thểlưu trữđược các thông tin trên. Trong đó, tại mỗi bước thời gian mà agent trải qua, ta lưu lại các giá trị:
• st - trạng thái của môi trường.
• at - hành động được lựa chọn.
• Rt - điểm thưởng nhận được do hệquảcủa hành động a.
• st+1 - trạng thái tiếp theo của môi trường Thông tin này sẽđược sửdụng trong việc tính ra tỉlệgiữa hai policy trong quá trình tính toán hàm mục tiêu của PPO.
• Giá trịthểhiện bước thời gian hiện tại có phải bước kết thúc hay không.
Các thông tin này sẽđược lưu lại dưới dạng một bộdữliệu, khi sốlượng đủlớn, ta tiến hành lấy mẫu các thông tin từđây theo từng bước thời gian đểsửdụng cho việc cập nhập mô hình.
3.6.3 Huấn luyện mô hình dựa trên dữliệu đã thu được Với các dữliệu đã thu được trong quá trình tương tác với môi trường, agent sẽ sửdụng các dữliệu này đểtiến hành cập nhật policy của mình. Các thông tin ta có là chuỗi các bước thời gian mà ởmỗi bước ta có trạng thái môi trường, hành động được lựa chọn, điểm thưởng cho hành động, trạng thái tiếp theo, xác suất chọn hành động với policy hiện tại.
Trước hết, sau khi có một chuỗi các bước thời gian, ta có thểsửdụng phương pháp Monte Carlo chuẩn hoá giá trịđiểm thưởng tại mỗi bước thời gian. Như đã trình bày trong phần lý thuyết, việc này sẽgiúp giá trịtại mỗi bước thời gian có tính liên kết hơn và tính toán một cách chính xác hơn cho MDP. Thông tin này sẽ được sửdụng thay cho các thông tin điểm thưởng thông thường của môi trường.
Sau đó, ta tiến hành cập nhật policy dựa vào các thông tin đã có. Ởđây, vì ta sửdụng phương pháp PPO, ta có thểthực hiện nhiều lần cập nhật với bộdữliệu.
Cụthể, tại mỗi lần thực hiện cập nhật (tương ứng với mỗi epoch), ta tiến hành tính toán:
1. Tính toán các giá trịxác suất chọn hành động at tại trạng thái st với policy mới nhất. Từđó tính được tỉlệgiữa policy cũ và mới.
2. Sửdụng hàm giá trịđểtính toán giá trịcủa các trạng thái trong tập dữliệu.
3. Tính toán giá trịlợi thếtại từng bước thời gian bằng cách lấy giá trịđiểm thưởng đã được chuẩn hoá (thông qua Monte Carlo) trừđi giá trịcủa trạng thái được tính từhàm giá trị.
4. Tính toán hàm mất mát của PPO và tiến hành tối ưu hoá policy.
Ởđây, cụthểtrong hàm mất mát của PPO sẽbao gồm 2 phần, mất mát cho hàm mục tiêu của PPO và mất mát cho hàm giá trị.
• Hàm mục tiêu của PPO là giá trịmin của hai phần là tỉlệgiữa 2 policy nhân với giá trịlợi thếvà phiên bản bịchặn của tích trên.
• Mất mát của hàm giá trịđược tính toán dựa trên mục tiêu là giá trịđiểm thưởng đã được chuẩn hoá (nhận được từMonte Carlo).
Sau khi có 2 hàm mất mát trên, ta sửdụng phương pháp tối ưu Adam đểtiến hành tối ưu hoá policy cũng như hàm giá trị.
3.7 Kết chương Trong chương này, em đã trình bày vềquá trình phát triển môi trường và huấn luyện mô hình giải quyết bài toán. Trong đó, em đã trình bày vềcác bước cơ sởnhư phát triển nền môi trường, thêm các logic vềhệthống đèn giao thông và hệthống quản lý phương tiện. Sau đó, môi trường được phát triển thành một quá trình quyết định Markov đóng vai trò là môi trường học cho agent. Từđó, ta tiến hành huấn luyện agent và tìm được policy tối ưu cho bài toán.
4.1 Tổng quan Với môi trường và thuật toán đã được phát triển trong chương 3, em tiến hành thửnghiệm huấn luyện mô hình với một sốcấu hình khác nhau của môi trường và giải thuật. Các thửnghiệm này được thực hiện theo hướng phát triển mô hình và tối ưu hoá sốlượng cảm biến và kích thước của mô hình, từđó giảm thiểu chi phí cho giải pháp. Trong chương này, em sẽtrình bày vềcác cấu hình được sửdụng cho việc thửnghiệm và các kết quảđạt được. Bên cạnh đó, em cũng đưa ra một số đánh giá vềcác kết quảvà giá trịcủa chúng trong việc giải quyết bài toán.
4.2 Mục tiêu đánh giá Đối với bài toán đặt ra và giải pháp đã trình bày, mục tiêu đánh giá của em trong đồán này như sau:
• Thửnghiệm các cấu hình khác nhau của môi trường.
• Thửnghiệm các độlớn khác nhau của mô hình.
• Đánh giá vai trò của phương pháp học tăng cường trong việc giải bài toán đề ra.
• Đánh giá khảnăng ứng dụng của giải pháp trong thực tế.
4.3 Các độđo đánh giá Đểđánh giá hiệu năng của các phương pháp trên với nhau cũng như với hiệu năng cơ sở, em sửdụng một sốđộđo đánh giá như sau:
• Thời gian chờtrung bình: Giá trịtrung bình mỗi xe chờđèn đỏ.
• Chiều dài hàng đợi: Chiều dài của một dãy xe chờtín hiệu đèn chuyển qua màu xanh.
Thời gian chờtrung bình cho phép ta nhìn vào hiệu năng của các phương pháp đối với một mục tiêu quan trọng của bài toán là giảm thiểu thời gian mà mọi người tham gia giao thông phải chờđợi tín hiệu đèn. độđo đánh giá giúp ta có thểước lượng được các giá trịmà giải pháp mang lại.
Chiều dài hàng chờcũng là một độđo đánh giá quan trọng. Khi chiều dài dãy các xe đang chờđèn đỏcàng lớn có nghĩa là sốlượng các xe cùng dừng tại một điểm đèn đỏcàng lớn. Điều này gây ra sựtrì hoãn khi chuỗi xe nhận được tín hiệu đèn xanh đểđi, do các xe đều có khoảng thời gian chờxe phía trước cách xa trước khi nhấn ga. Bên cạnh đó, khi chuỗi xe dài không được giải phóng cùng lúc sẽgây Hình 4.1: Hiệu năng cơ sở ra tình trạng tăng tốc rồi giảm tốc liên tục, kết hợp với sựtrì hoãn đã giải thích ở trên, khiến cho đoạn đường càng thêm tắc nghẽn, cùng với đó là thải ra nhiều loại khí thải có hại cho sức khoẻvà môi trường.
4.4 Phương pháp thí nghiệm Đểđánh giá hiệu năng của từng phương pháp, em cấu hình môi trường phù hợp với việc đánh giá. Một sốthông sốsau được cốđịnh:
• Tổng sốbước thời gian cho mỗi lần sinh giá trịđánh giá là 3072 bước. Điều này đảm bảo bao quát được hết toàn bộcác trường hợp của môi trường. Ởđây chiều tăng giảm của giãn cách tạo xe).
• Với mỗi phương pháp, thực hiện tính toán các giá trịđộđo đánh giá đối với bội sốđủlớn của 3072 bước thời gian đểtính toán giá trịtrung bình một cách chính xác.
• Môi trường sửdụng hạt giống cốđịnh cho các hàm sốngẫu nhiên. Điều này giúp ta đảm bảo được môi trường giống nhau giữa các lần đánh giá, tăng được sựcông bằng cho các phương pháp.
Bên cạnh đó, môi trường được xây dựng thành một ứng dụng, nâng cao hiệu năng và độchính xác cho đánh giá.
4.5 Hiệu năng cơ sở Với môi trường được thiết kếcho việc đưa ra các quyết định giống với cơ chế hiện tại ởngoài thực tế, sau quá trình đánh giá, hệthống đạt được hiệu năng không tốt và không ổn định được thểhiện trong hình 4.1. Như ta có thểthấy, tổng điểm thưởng trong mỗi tập của môi trường có sựkhác biệt khá nhiều. Cho thấy điểm hạn chếkhông thểthích ứng với các trạng thái khác nhau của môi trường, trong đó ta sẽcó những giai đoạn mà hệthống này giải quyết tốt, nhưng cũng có những giai đoạn mà hiệu năng của hệthống là rất tệ. Giá trịđiểm thưởng trung bình đạt được -5013.
4.6 Quá trình huấn luyện mô hình 4.6.1 Thiết lập siêu tham sốcho thuật toán PPO Đểhuấn luyện mô hình, trước tiên em thửmột hệthống siêu tham sốcơ bản cho agent và phát triển từđó đối với các kết quảđạt được khác nhau. Các hệsốcần chú ý gồm có:
• buffer_size = 512: Sốlượng bước thời gian cần thu thập trước khi thực hiện bước cập nhật policy.
• batch_size = 64: Sốlượng bước thời gian được sửdụng cho một lần tính toán gradient trong bước tối ưu hoá policy.
• learning_rate= 3e-5: Tốc độhọc, kiểm soát tốc độthay đổi của mô hình trong mỗi lần cập nhật.
• num_layers = 5: Sốlớp ẩn trong mạng neuron sửdụng.
• hidden_units = 128: Sốđơn vịẩn trong một lớp ẩn trong mạng neuron sử dụng.
• epsilon = 0.2: Giá trịϵ trong hàm mục tiêu của phương pháp PPO. Giá trị0.2 được tác giảkhuyến nghịcho hầu hết bài toán.
• gamma = 0.95: Hệsốchiết khấu, thểhiện sựthụt giảm giá trịđiểm thưởng nhận được trong tương lại, tính cho thời điểm hiện tại.
Với hệthống siêu tham sốnhư trên, ta có thểtiến hành huấn luyện mô hình và sửdụng kết quảhuấn luyện đểtiếp tục quá trình tối ưu hệthống tham số.
4.6.2 Huấn luyện mô hình với môi trường có hệthống cảm biến dày đặc Đểkhởi đầu quá trình phát triển, trước hết em sửdụng một hệthống môi trường có hệthống cảm biến rất dày, được đặt ởhầu hết các vịtrí trong môi trường. Trong đó, mỗi hướng đường bao gồm 63 cảm biến trong đoạn đường dài 100m và rộng 15m được thểhiện trong hình 4.2. Điều này đảm bảo agent có đủthông tin nhận Hình 4.2: Hệthống 252 cảm biến Hình 4.3: Kết quảvới hệthống môi trường nhiều cảm biến thức vềmôi trường do hệthống cảm biến rất dày và không bịmất mát trong các quãng giãn cách.
Với cấu hình môi trường này, em sửdụng một kiến trúc mạng neuron đơn giản với 5 lớp ẩn, trong đó mỗi lớp có 128 đơn vịẩn và với các giá trịsiêu tham sốcơ bản đã được chọn, quá trình huấn luyện diễn ra khá ổn định và hiệu quả, được thể hiện qua hình 4.3. Trong đó, ta có thểthấy mô hình học một cách tương đối ổn định và hội tụsau khoảng 25000 bước thời gian.
Với kết quảkhá tốt, đạt giá trịđiểm thưởng trung bình -2162. Giảm 57% so với hiệu năng cơ sở. Bên cạnh đó, sau khi hội tụ, lượng điểm thưởng nhận được của mô hình này cũng rất ổn định, thay đổi trong khoảng 216. Sựvượt trội này được thểhiện rõ trong hình 4.3 so sánh kết quảcủa hai hệthống. Những kết quảnày thể Hình 4.4: Hệthống 104 cảm biến hiện hiệu năng áp đảo của hệthống thông minh so với hệthống truyền thống về cảsốđiểm thưởng trung bình lẫn sựổn định. Với hệthống này, ta có thểđảm bảo được việc tối ưu các quyết định với mục đích hướng tới là sựthoải mái của người tham gia giao thông.
Với nhận định rằng các thông tin quan sát của agent vềmôi trường là tương đối đầy đủdo có sốlượng cảm biến khá dày. Ta thấy hệthống có thểđược cải tiến về mặt mô hình và thửnghiệm giảm sốlượng cảm biến đểtối ưu hoá hệthống, giảm thiểu chi phí.
4.6.3 Cắt giảm sốlượng cảm biến Đểgiảm thiểu một lượng lớn chi phí cho việc mua và lắp đặt các cảm biến, em thửnghiệm giảm lượng lớn các cảm biến trong môi trường. Cụthể, thay lắp đặt 3 dải cảm biến song song như trong hệthống ban đầu, ta có thểsửdụng hai dải cảm biến do trên môi trường hiện tại chỉcó 2 làn đường xe chạy. Đồng thời, ta cũng giảm bớt sốlượng cảm biến thông qua việc đặt chúng cách xa nhau hơn, làm tăng độthưa của chúng. Hệthống cảm biến mới được thểhiện ởhình 4.4, sốlượng cảm biến được giảm từ252 xuống còn 104.
Với sốlượng cảm biến giảm đáng kể, chi phí cho việc mua cũng như lắp đặt hệ thống giảm đáng kể. Trong khi đó, theo kết quảhuấn luyện mô hình được thểhiện trong hình, ta vẫn đạt được mức điểm thưởng tương đương như trong hệthống cảm biến dày đặc, được thểhiện trong hình 4.5 Chỉvới lượng cảm biến tương đối thưa vềmôi trường, agent vẫn có thểđưa ra được các quyết định hợp lý và đạt kết quả ổn định.
Khi giảm kích thước mô hình một cách tương đối nhiều với sốlượng lớp ẩn giảm từ5 xuống 2. Hiệu năng trên đã có giảm nhẹvà có sựthay đổi vềquá trình hội tụ Hình 4.5: Kết quảvới hệthống 104 cảm biến Hình 4.6: Hệthống 36 cảm biến của mô hình. Hình 4.5 thểhiện rõ sựso sánh giữa 3 hệthống thông minh với các cấu hình khác nhau và với kết quảcủa hệthống cơ sở. Khi giảm kích thước mô hình, hiệu năng của hệthống khi hội tụcó sựgiảm nhẹso với hai hệthống trước.
Điều này là do kích thước mô hình quá nhỏdẫn tới không thểhọc được đủcác đặc tính của môi trường. Từđó, trong một sốtrường hợp không hiểu rõ, hệthống có thể đưa ra một sốhành động không tối ưu nhất.
Tuy nhiên, với những kết quảtrên, ta vẫn có thểđánh giá rằng 104 cảm biến là một sốlượng đủđểagent có thểnắm bắt được các thông tin cần thiết vềlưu lượng giao thông trong thời gian thực trong môi trường. Do đó, ta có thểthửnghiệm tiếp tục giảm sốlượng cảm biến.
Hình 4.7: Kết quảcủa hệthống cảm biến tối giản 4.6.4 Hệthống cảm biến tối giản Đểcắt giảm sốlượng cảm biến trong môi trường. Ta tiến hình cắt giảm 2 dải cảm biến xuống một, từđó ta chọn điểm trung tâm 2 làn đường đểđặt cảm biến, từđó có thểnhận được thông tin vềcả2 làn đường. Hệthống được thểhiện trong hình 4.6.
Với sốlượng cảm biến giảm đáng kể, kết quảta đạt được thểhiện sựsụt giảm tương đối so với hệthống 104 cảm biến. Điểm thưởng trung bình đạt mức -2431 với độchênh lệch giữa các tập môi trường khá thấp. Nhìn vào hình 4.7, ta có thể thấy được một sựso sánh vềkết quảcủa các hệthống với cấu hình khác nhau. Ở đây, hệthống với lượng cảm biến tối giản vẫn đem lại một kết quảkhảquan khi chỉ thấp hơn hai hệthống tốn kém nhất và ngang bằng với hiệu năng của hệthống 104 cảm biến với mô hình nhỏ.
Bên cạnh đó, sựsụt giảm hiệu năng này có thểhiểu được do, do ta chỉsửdụng 9 cảm biến ởmỗi hướng đường, trong đó cảm biến được đặt ởgiữa 2 làn đường và không có sựphân biệt giữa việc có 1 xe hay 2 xe đang trong vùng cảm biến. Khi này, những quan sát vềmôi trường sẽquá thưa và có một chút sai lệch so với thực tế. Tuy nhiên, ta có thểthấy kết quảcủa mô hình vẫn khá ổn định qua các tập môi trường với các tình trạng môi trường khác nhau, thểhiện được sựthích nghi của mô hình với mọi tính huống trong môi trường. Những kết quảđạt được vẫn là tương đối tốt so với hệthống tốt nhất, giảm 12%. Và so với mức hiệu năng cơ sởthì ta vẫn đạt mức rất tốt, tăng 51%.
Hình 4.8: Giá trịđiểm thưởng trung bình cho các cấu hình khác nhau của hệthống 4.6.5 Đánh giá kết quảcác cấu hình hệthống Sau quá trình huấn luyện mô hình, em tiến hành đánh giá kết quảcủa các cấu hình hệthống khác nhau trên môi trường kiểm tra. Hình 4.8 thểhiện cái nhìn tổng quan vềkết quảcủa các cấu hình khác nhau của hệthống cảm biến cũng như kích thước mô hình sửdụng. Ta có thểthấy những kết quảcủa hệthống có sựkhác biệt giữa các cấu hình.
Hệthống 252 cảm biến cung cấp mức điểm thưởng trung bình cao nhất trong 3 cấu hình cảm biến. Trong đó, mức tối đa đạt được tại mô hình với 152283 tham số. Thực nghiệm cho thấy không có thay đổi đáng kểkhi tăng kích thước mô hình.
Tuy nhiên, hệthống này có những điểm hạn chếnhư yêu cầu quá nhiều cảm biến.
Đồng thời, do đầu vào của hệthống có nhiều cảm biến hơn nên sốtham sốcủa mô hình cũng tăng đáng kể.
Hệthống 104 cảm biến có sựgiảm vềđiểm thưởng so với hệthống cảm biến dày đặc trên. Điều này là do sựgiảm thiểu lớn vềđộdày đặc của những thông tin môi trường nhận được từcác cảm biến cùng với đó là sựgiảm thiểu kích thước mô hình. Tuy nhiên, kết quảnhận được vẫn rất khảquan và cách biệt không quá nhiều so với mức hiệu năng tối đa của hệthống cảm biến dày đặc. Bên cạnh đó, do số lượng cảm biến giảm đáng kể, với cùng một sốlượng đơn vịẩn trong mỗi lớp và sốlớp ẩn trong mô hình, lượng tham sốcủa mô hình cũng giảm đi đáng kể. Ởmột mức lượng tham sốkhá tương đồng, hệthống này đạt hiệu năng tương đương với Hình 4.9: Thời gian chờtrung bình hệthống cảm biến dày đặc với cùng kích thước mô hình.
Hệthống 36 cảm biến có lượng điểm thưởng thấp hơn so với hai hệthống kể trên. Mặc dù có sốlượng cảm biến khá ít và thông tin đầu vào từmôi trường rất thưa, bằng việc tăng kích thước mô hình, ta vẫn có thểxây dựng được hệthống này với kết quảtương đối tốt. Nếu so sánh với hệthống cơ sởthì ta vẫn có một sựkhác biệt lớn. Hơn nữa, việc giảm lượng cảm biến từ252 xuống 36 là sựgiảm thiểu rất lớn vềchi phí cần thiết cho giải pháp, từđó giúp cho tính ứng dụng của giải pháp được tăng cường, giúp giải quyết được bài toán thực tế.
Qua những đánh giá này, ta có thểthấy được một cái nhìn tổng quan vềquá trình huấn luyện mô hình và những kết quảđạt được với những cấu hình khác nhau của hệthống. Từđó, tuỳthuộc vào nhu cầu thực tếmà ta có thểsửdụng các cấu hình khác nhau cho các bài toán cụthể.
4.7 Đánh giá các hệthống qua các độđo đánh giá 4.7.1 Thời gian chờtrung bình Thời gian chờtrung bình là trọng tâm tối ưu hoá trong bài toán đặt ra. Đánh giá này thểhiện giá trịtrung bình mà mỗi xe xuất hiện trong môi trường phải chờ. Mục tiêu của bài toán là giảm giá trịnày tới mức tối thiểu, đồng nghĩa với việc người tham gia giao thông có thểlưu thông một cách nhanh và thoải mái hơn, đồng thời tiết kiệm được lượng thời gian tham gia giao thông hằng ngày. Sau quá trình đánh giá hệsốnày trên nhiều cấu hình hệthống và kích thước của mô hình, một sốkết Hình 4.10: Chiều dài hàng chờtrung bình quảtiêu biểu được thểhiện trong hình 4.9.
Ta có thểthấy, hàm mục tiêu của mô hình ta đặt ra có phần khác so với hệsố đánh giá này, trong đó hàm mục tiêu của ta có đưa thêm một trọng sốliên quan đến thời gian chờcủa xe (xe chờcàng lâu thì trọng sốđiểm thưởng âm càng lớn). Mặc dù vậy, hệthống được xây dựng từmô hình trên vẫn cho ta một kết quảvượt trội so với hệthống gốc. Hệthống tối giản với 36 cảm biến đạt mức thời gian chờtrung bình giảm 45% so với hệthống gốc. Và ởcác hệthống với nhiều cảm biến hơn con sốnày còn lớn hơn nữa.
4.7.2 Chiều dài hàng chờtrung bình Kết quảđánh giá các hệthống với độđo đánh giá chiều dài hàng chờtrung bình được thểhiện trong hình 4.10. Ởđây ta cũng thấy được sựvượt trội của hệthống tối ưu so với hệthống gốc. Các cấu hình hệthống tối ưu khác nhau không có quá nhiều sựchênh lệch vềhiệu năng, đều ởmức giảm 21% chiều dài hàng chờtrung bình so với hệthống gốc.
4.8 Đánh giá các mục tiêu • Với các cấu hình khác nhau của môi trường, em đã thực hiện đánh giá và rút ra kết quảcác mức hiệu năng khác nhau với từng cấu hình. Đồng thời, mỗi cấu hình cũng sẽphù hợp với từng điều kiện thực tếcủa bài toán như mức hiệu năng yêu cầu, mức chi phí.
• Với các độlớn khác nhau của mô hình, kết quảthửnghiệm cho thấy hiệu năng tăng lên khi ta tăng kích thước của mô hình. Trong đó, mức kích thước với 5 lớp ẩn gồm 128 đơn vịẩn là một mốc giới hạn mà từđó khi tăng kích thước mô hình hiệu năng không có nhiều thay đổi.
• Trong việc giải bài toán đềra, các phương pháp học tăng cường giúp tối ưu hoá các quyết định tín hiệu đèn giao thông một cách hiệu quảdựa trên chuỗi các kinh nghiệm tích luỹtrong quá khứ. Do đó, phương pháp này giúp cho các quyết định này không bịphụthuộc vào các thông tin nhãn từcon người - điều khó thực hiện trong bài toán mà ta đặt ra. Chính vì vậy, giải pháp giúp ta vượt qua được vật cản vềviệc tạo thông tin nhãn cho hành động, và đồng thời cung cấp một mô hình tối ưu dựa trên điều kiện thực tế, giúp ta nâng cao được giá trịứng dụng của nó.
• Các kết quảđạt được cho thấy, với các mức chi phí yêu cầu khác nhau, kết quả đạt được có sựthay đổi nhưng đều vượt trội so với hệthống không tối ưu hiện tại. Do đó, với lượng dữliệu thật ngoài thực tế, sau quá trình huấn luyện và thửnghiệm kết quả, hệthống có thểđược đem vào thí điểm tại một sốcụm đèn giao thông, và từđó đem vào sửdụng thực tếnếu đạt kết quảtốt. Từviệc có thểtiếp tục nhận các thông tin và đưa ra các hành động khi được đem vào thực tếsửdụng, hệthống có thểliên tục học và cập nhật policy của mình. Kết hợp với một phương pháp triển khai hệthống một cách an toàn, việc sửdụng và liên tục tối ưu hoá hệthống là khảquan.
4.9 Kết chương Như vậy, trong suốt quá trình thực hiện đồán, em đã xây dựng được một môi trường học tăng cường với bài toán tối ưu hoá các tín hiệu đèn giao thông tại một ngã tư đơn giản, xây dựng các hệthống cảm biến khác nhau và thửnghiệm phương pháp học tăng cường vào tối ưu hoá hệthống đèn giao thông. Em đã đưa ra kết quảkhảthi và đạt kỳvọng đặt ra khi hệthống tối ưu có hiệu năng vượt trội so với hệthống gốc. Kết quảnày khẳng định tính khảthi của giải pháp và khảnăng phát triển và ứng dụng vào thực tiễn trong tương lai.
5.1 Kết luận 5.1.1 Tổng kết kết quả Trong nội dung đồán, em đã đạt được một sốkết quảchính như sau:
• Xây dựng môi trường hệthống giao thông ởmột ngã tư đơn giản, hoạt động một cách hiệu quảvà có khảnăng tuỳbiến.
• Thửnghiệm phương pháp học tăng cường với các cấu hình khác nhau của môi trường và mô hình. Đạt kết quảtốt với mức thời gian chờtrung bình của các phương tiện giảm 45% và độdài hàng chờgiảm 21%.
5.1.2 Điểm mạnh Giải pháp đưa ra có hiệu năng tăng nhiều so với hiệu năng cơ sở, cùng với đó là khảnăng ứng dụng cao trong thực tế. Do đó, hệthống có thểđược phát triển để sửdụng ngoài đời thực. Bên cạnh đó, môi trường được xây dựng trong đồán có thểđược phát triển đểtăng độphức tạp của bài toán. Mô hình bài toán cũng là một điểm mạnh khi có thểáp dụng vào một sốbài toán khác.
5.1.3 Điểm hạn chế Điểm hạn chếcủa đồán là môi trường đang xét tới còn tương đối đơn giản, do đó việc học của agent chưa thực sựquá khó khăn đểcần tới các bước tinh chỉnh phức tạp. Các đánh giá còn tương đối chủquan, cần có sựtham gia của các chuyên gia trong lĩnh vực giao thông đểcó thểtạo nên một hệthống tham sốđánh giá hợp lý và có tính chất ứng dụng cao hơn nữa.
5.2 Hướng phát triển • Xây dựng môi trường phức tạp với quy mô lớn hơn, ví dụnhư quy mô hệthống trong một thành phố, hay một quận, huyện. Với môi trường này, ta có thểtận dụng được tính tối ưu của phương pháp học tăng cường. Khi đó, nhờkhảnăng tối ưu hoá cho hàm mục tiêu chung trên toàn bộmôi trường, agent có thểhọc được những tính chất liên quan đến phân luông giao thông, đưa ra các quyết định tín hiệu đèn đểtối ưu trên cảmôi trường thay vì một cụm đèn duy nhất.
• Thay đổi mô-đun cảm biến của môi trường bằng hình ảnh. Theo đó, ta có thể đểagent học từđầu vào là ảnh, và từđó ta có thểhuấn luyện một mô hình đưa ra các quyết định đèn tín hiệu, hoặc thêm một mô-đun đánh giá lưu lượng xe ởcác hướng dựa vào ảnh. Từđây, ta giảm được chi phí cho việc phải sửdụng một sốlượng cảm biến khá lớn trên đường.
5.3 Kết luận Như vậy, trong nội dung báo cáo đồán tốt nghiệp, em đã trình bày vềcác phương pháp học tăng cường, quá trình phát triển môi trường cho bài toán hệthống giao thông ởmột ngã tư, và từđó ứng dụng các phương pháp trên vào tìm giải pháp tối ưu cho bài toán. Những kết quảđạt được từđồán khá tốt và sẽlà tiền đềđểem có thểtiếp tục phát triển bài toán và tiến đến ứng dụng vào giải quyết vấn đềcho xã hội. Do thời gian thực hiện đồán có hạn nên kết quảcủa đồán còn có những hạn chế. Em mong có thểcó được sựgóp ý và hỗtrợtừthầy cô, bạn vè đểgiúp em phát triển đồán thành một dựán lớn hơn, mang lại những giá trịcho xã hội.
[1] S. Dick, “Artificial intelligence,” 2019.
[2] S. El-Tantawy, B. Abdulhai, and H. Abdelgawad, “Design of reinforcement learning parameters for seamless application of adaptive traffic signal control,” Journal of Intelligent Transportation Systems, vol. 18, no. 3, pp. 227–245, 2014.
[3] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no. 3, pp. 279–292, 1992.
[4] J. Gao, Y. Shen, J. Liu, M. Ito, and N. Shiratori, “Adaptive traffic signal control: Deep reinforcement learning algorithm with experience replay and target network,” arXiv preprint arXiv:1705.02755, 2017.
[5] C.-H. Wan and M.-C. Hwang, “Value-based deep reinforcement learning for adaptive isolated intersection signal control,” IET Intelligent Transport Systems, vol. 12, no. 9, pp. 1005–1010, 2018.
[6] X. Liang, X. Du, G. Wang, and Z. Han, “Deep reinforcement learning for traffic light control in vehicular networks,” arXiv preprint arXiv:1803.11115, 2018.
[7] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with double q-learning,” in Proceedings of the AAAI conference on artificial intelligence, vol. 30, 2016.
[8] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience replay,” arXiv preprint arXiv:1511.05952, 2015.
[9] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.
[10] T. M. Mitchell, Machine learning, 9. McGraw-hill New York, 1997, vol. 1.
[11] D. Silver, A. Huang, C. J. Maddison, et al., “Mastering the game of go with deep neural networks and tree search,” nature, vol. 529, no. 7587, pp. 484– 489, 2016.
[12] C. Berner, G. Brockman, B. Chan, et al., “Dota 2 with large scale deep reinforcement learning,” arXiv preprint arXiv:1912.06680, 2019.
[13] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” arXiv preprint arXiv:1707.06347, 2017.
[14] M. L. Puterman, Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.
[15] N. Metropolis and S. Ulam, “The monte carlo method,” Journal of the American statistical association, vol. 44, no. 247, pp. 335–341, 1949.
[16] G. Tesauro et al., “Temporal difference learning and td-gammon,” Communications of the ACM, vol. 38, no. 3, pp. 58–68, 1995.
[17] S. Ruder, “An overview of gradient descent optimization algorithms,” arXiv preprint arXiv:1609.04747, 2016.
[18] V. Mnih, K. Kavukcuoglu, D. Silver, et al., “Playing atari with deep reinforcement learning,” arXiv preprint arXiv:1312.5602, 2013.
[19] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient methods for reinforcement learning with function approximation,” Advances in neural information processing systems, vol. 12, 1999.
[20] S. T. Tokdar and R. E. Kass, “Importance sampling: A review,” Wiley Interdisciplinary Reviews: Computational Statistics, vol. 2, no. 1, pp. 54–60, 2010.
[21] J. K. Haas, “A history of the unity game engine,” 2014.
[22] X. Lin, “Multi-behaviors finite state machine,” in 2009 IEEE Youth Conference on Information, Computing and Telecommunication, 2009, pp. 201–203. DOI:
10.1109/YCICT.2009.5382390.
[23] A. Juliani, V.-P. Berges, E. Teng, et al., “Unity: A general platform for intelligent agents,” arXiv preprint arXiv:1809.02627, 2018.