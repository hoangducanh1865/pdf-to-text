For me, finishing this project signifies not just a milestone but the start of anexciting new journey. While conducting this thesis, I have often felt lucky. Thiswork would not have been possible without the invaluable guidance and mentorshipof Associate Professor Huỳnh Thị Thanh Bình. I am also grateful to my mentor,Long Đoàn, for our weekly discussions and his thoughtful feedback. I would liketo extend my thanks to the members of the MSO Lab, who offered their time toenrich this work in countless ways. Last but not least, I want to express my heartfeltappreciation to my beloved family and best friends, who have always been mysource of emotional support and encouragement during the most challenging times.Thank you all from the bottom of my heartAutomatic Heuristic Design (AHD) is an active research area due to its utility insolving complex search and NP-hard combinatorial optimization problems in thereal world. Recent advances in Large Language Models (LLMs) introduce new pos-sibilities by coupling LLMs with Evolutionary Computation (EC) to automaticallygenerate heuristics, known as LLM-based Evolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained great performance on varioustasks, there is still a gap in understanding the properties of heuristic search spacesand achieving a balance between exploration and exploitation, which is a criticalfactor in large heuristic search spaces. This research addresses this gap by propos-ing two diversity measurement metrics and performing an analysis of previousLLM-EPS approaches, including FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal that while EoH demonstrates higher diversity than Fun-Search and ReEvo, its objective score is unstable. Conversely, ReEvo’s reflectionmechanism yields good objective scores but fails to optimize diversity effectively.In light of these findings, HSEvo was introduced as an adaptive LLM-EPS frame-work that strikes a balance between diversity and convergence through the use ofa harmony search algorithm. Experimentation demonstrated that HSEvo achievedhigh diversity indices and good objective scores while remaining cost-effective.These results underscore the importance of balancing exploration and exploitationand understanding heuristic search spaces in designing frameworks in LLM-E2.4 Exploring the correlation between objectNP-hard Combinatorial Optimization Problems (COPs) frequently arise in real-world scenarios. Examples include groups of problems such as routing and logis-tics, scheduling and planning, network design, and others. To address these prob-lems, heuristic methods are among the most commonly employed approaches dueto their superior efficiency in terms of time and resources needed to find solutionsthat approximate the optimal. Over the past few decades, significant efforts havebeen dedicated to designing effective search methods, resulting in techniques suchas simulated annealing, tabu search, and iterated local search, among many oth-ers. These manually crafted methods have been successfully applied in numerouHowever, due to the diverse nature of practical problems, each application comeswith its constraints and objectives, often requiring customization or the selection ofspecific search methods tailored to the situation at hand. Manually creating, tuning,and configuring search methods for particular problems is not only labor-intensivebut also demands deep expertise. This represents a bottleneck in many applicationdomains. Consequently, AHD has emerged as a promising solution, aiming to auto-mate the selection, tuning, or construction of efficient search methods for specifiThe goal of AHD is to address the difficulties involved in the creation of manualheuristics by utilizing computational methods to automatically generate or enhanceheuristics. Among the various approaches within AHD, both Hyper-Heuristics(HHs) [1], [2] and Neural Combinatorial Optimization (NCO) [3]–[5] have gainedsubstantial attention for their potential to address the limitations of traditionaHHs operate within predefined heuristic spaces curated by human experts, se-lecting or generating heuristics from these established sets. While HHs have demon-strated success, their performance is inherently limited by the scope and quality ofthe predefined heuristic space, often restricting the discovery of novel or more efOn the other hand, NCO employs neural networks to learn patterns and predictsolutions for COPs. By leveraging data-driven models, NCO seeks to generalizeacross problem instances, offering a more dynamic approach to heuristic devel-opment. Despite its promise, NCO faces significant challenges. Effective general-ization across diverse problem instances requires robust inductive biases [6], whicare difficult to design and implement. Additionally, the interpretability and explain-ability of the learned models remain pressing concerns [5], making it challengingto validate or trust the solutions produced. These limitations highlight the need forNCO frameworks to evolve further, incorporating mechanisms that enhance theiThe constraints associated with HHS and NCO underscore the need for moreflexible, adaptive, and innovative methods in AHD. Addressing these limitationscan unlock the full potential of heuristic design, enabling the development of sys-tems that not only adapt to complex and evolving problem landscapes but also ex-pand the horizons of heuristic discovery beyond current boundaries. Such advance-ments will play a crucial role in shaping the future of optimization and decisionRecently, the rise of LLMs has opened up new possibilities for AHD. It is be-lieved that LLMs [7], [8] could be a powerful tool for generating new ideas andheuristics. However, standalone LLMs with prompt engineering can be insufficientfor producing novel and useful ideas beyond existing knowledge [9]. Some attemptshave been made to couple LLMs with EC to automatically generate heuristics,known as LLM-EPS [10]–[12]. Initial works such as FunSearch [13] and subse-quent developments like Evolution of Heuristic (know as EoH) [14] and ReflectiveEvolution (know as ReEvo) [15] have demonstrated significant improvements overprevious approaches, generating quality heuristics that often surpass current meth-ods. Even so, ReEvo yields state-of-the-art and competitive results compared witA key difference between LLM-EPS and classic AHD lies in the search spacesof heuristics. Classic AHD typically operates within well-defined mathematicalspaces such as Rn, whereas LLM-EPS involves searching within the space of func-tions, where each function represents a heuristic as a program. This functionalsearch space introduces unique challenges and opportunities, necessitating a deeperunderstanding of the properties and characteristics of heuristics within this domain.Despite the advancements made by LLM-EPS frameworks, there remains a signif-icant gap in foundational theories and principles that underpin AHD within theThis research focuses on the intersection of EC and LLMs, exploring their po-tential synergy in AHD. History has demonstrated that evolutionary algorithmsexcel in optimization tasks ranging from combinatorial problems and image enhancement to industrial scheduling thanks to their population-based search mech-anisms that balance exploration and exploitation. Simultaneously, LLMs such asChatGPT, Claude, and Gemini have achieved remarkable advancements in natu-ral language processing and code generation, showcasing their ability to producThe primary objective of this research is to explore how these two fields cancollaborate to design heuristics for solving COPs. While current research has madeprogress in metrics such as solution quality and runtime efficiency, maintainingand leveraging diversity among candidate solutions remains underexplored. Thisoversight may limit the range of potential solutions, ultimately hindering opportuThus, the scope of this study aims at three objectives. First, "tools" should be de-veloped to measure the diversity level promoted by various methods within LLM-EPS frameworks. This quantification will enable straightforward comparison andselection of suitable diversity-enhancing methods, while also serving as a founda-tion for assessing the balance between exploration and exploitation. Second, thestudy seeks to analyze how varying levels of diversity impact the search quality ofdifferent LLM-EPS frameworks. Finally, this work aims to design and evaluate anew LLM-EPS framework that balances exploration and exploitation, incorporat-ing insights drawn from the second objective through the tools developed in thefirst objective. This framework will be tested on AHD problems, with results com-pared against existing LLM-EPS methods. This comparison will help evaluate theproposed framework and provide evidence of its potential contributions based This thesis makes three key contributions to the field of AHD:•Two new metrics are the Shannon-Wiener Diversity Index (SWDI) and theCumulative Diversity Index (CDI) proposed to evaluate and track the diversityof heuristic populations in LLM-EPS frameworks. SWDI evaluates diversitybased on the distribution of heuristic clusters, encouraging a balance betweenexploration and exploitation. CDI assesses the overall spread and uniformityof heuristics in the search space by analyzing their distribution through a min-imum spanning tree. These metrics address challenges in quantifying diver-sity in function-based search spaces. Both metrics offer practical insights intopopulation dynamics, revealing how diverse heuristics contribute to escapi•A comprehensive analysis of heuristic search spaces has been conducted. Thisinvestigation explores the relationship between the proposed diversity metricsand the performance of various LLM-EPS frameworks across different opti-mization problems. The analysis highlights that while higher diversity gener-ally supports exploration, excessive diversity without proper optimization cation risk premature convergence. These findings establish the significance o•Introduce a novel framework called HSEvo. HSEvo integrates the HarmonySearch (HS) algorithm to optimize diversity and convergence within LLM-EPS frameworks. It refines traditional evolutionary phases such as initial-ization, crossover, and mutation by incorporating advanced techniques likeflash reflection and elitist mutation. Flash reflection uses LLM-driven analy-ses to guide heuristic improvements, while elitist mutation focuses on refiningtop-performing heuristics. The HS component tunes heuristic parameters tdemonstrate that HSEvo outperforms existing frameworks by achieving highdiversity indices and superior objective scores, providing a cost-effective aThe remainder of this thesis is organized as follows.Chapter 1 offers a thorough review of relevant literature, focusing on the foun-dational concepts of EC and LLMs. It also explores the emerging field of LLM-EPS and emphasizes the significance of diversity in evolutionary algorithms. Thechapter concludes by identifying gaps in current research and laying the groundChapter 2 introduces two novel diversity metrics SWDI and CDI to measureand analyze the diversity of heuristic populations in LLM-EPS frameworks. Ad-ditionally, it explores the correlation between these diversity metrics and the ob-jective scores of various optimization problems, providing insights into the role oChapter 3 explains the proposed HSEvo framework, which integrates the HS al-gorithm with EC to balance exploration and exploitation in heuristic search spaces.It provides detailed descriptions of HSEvo, including population initializatioChapter 4 presents the experimental setup, benchmark datasets, and results ofapplying HSEvo to various AHD problems, including Bin Packing Online, Trav-eling Salesman Problem, and Orienteering Problem. After that, compares the per-formance of HSEvo with previous LLM-EPS frameworks and includes an ablationstudy to evaluate the effectiveness of the proposed components.Finally, Conclusions summarizes the key findings of the thesis, highlighting theimportance of diversity in LLM-EPS frameworks and the effectiveness of the pro-posed HSEvo framework. It also suggests potential directions for future researchArtificial Intelligence (AI) is evolving at an extraordinary pace, inspiring inno-vative methods that blend disparate computational fields to tackle some of the mostdemanding challenges. In this rapidly shifting landscape, two particular paradigmsstand out for their strengths yet have traditionally advanced on separate tracks: ECand LLMs. EC, inspired by natural evolution, explores populations of candidatesolutions to discover high-quality outcomes, whereas LLMs are adept at generat-ing coherent, context-aware outputs ranging from textual prose to executable code.As AI applications become more ambitious, a growing area of research, known asLLM-EPS, seeks to harness the best of both worlds: leveraging the adaptive searchprowess of EC and the creative and generative capabilities of LLMs. This emergingfusion offers a novel route for solving challenges in AHD.Initial research indicates that aligning the iterative adjustments of EC with theadaptable generation abilities of LLMs can lead to significant advancements, es-pecially in the creation of heuristic programs, code fragments, or even written so-lutions. Nevertheless, an essential question persists: How can diversity be ensuredthroughout the evolutionary process? This is particularly important when candi-date solutions are no longer merely simple numeric vectors but are instead diverseforms of code or text. Preserving diversity is paramount to avoid converging tooquickly on suboptimal solutions and to maintain an expansive search of the so-lution landscape. The subsequent sections provide an overview of EC principles,discuss the capabilities and constraints of modern LLMs, and survey recent stridesin LLM-EPS research, with special attention to how future investigations migAlthough the convergence of EC and LLMs for optimization and AI is still arelatively new domain, it builds upon two longstanding bodies of research thathave more often evolved in isolation. EC traditionally tackles numerical or com-binatorial challenges, while LLMs excel at text-oriented tasks like language trans-lation, content generation, and summarization. Current work in LLM-EPS illus-trates how these fields can be merged: LLMs become generators of candidate solu-tions whether code snippets, pseudo-code, or textual explanations, and evolutionRecent studies show this collaboration. In code generation, studies [7], [16], [17]showcase evolutionary strategies that refine LLM suggested programs for enhancquality and resilience. In text generation [18], [19] illustrates how evolutionaryrefinement can produce more precise and diverse linguistic outputs. Together, theseworks point to tangible gains, including improved error resolution, larger solutioNevertheless, most existing research [13]–[15] focuses on performance indica-tors without explicitly examining how population diversity can be steered or main-tained when solutions take the form of human-readable code or text. This missingpiece is critical for keeping evolutionary searches from prematurely collapsing onlocal optima and for fostering creative, wide-ranging solution sets. Future efforts,therefore, might delve deeper into incorporating diversity metrics to ensure thatLLM-driven evolutionary processes not only excel in performance but also yieEC [20] is one computational intelligence model used to mimic the biologi-cal evolution phenomenon. Currently, EC includes four algorithms: Genetic Al-gorithm (GA), Evolutionary Programming (EP), Evolution Strategies (ES), andGenetic Programming (GP). GA was proposed by the American scholar Hollandin the 1950s in a study of self-adapting control. To study the finite-state machine ofAI, the American scholar Fogel proposed EP in the 1960s. At nearly the same time,the German scholars Rechenberg and Schwefel proposed ES to solve numerical op-timization. In the 1990s, based on the GA, the American scholar Koza proposedGP to study the automatic design of computer programs.Although the four algorithms were proposed by different scholars for differentpurposes, their computing processes are similar and can be described as follows.1. One group of initial feasible solutions are created;2. The properties of the initial solutions are evaluated;3. The initial solutions are selected according to their evaluation results;4. The selected solutions are conducted by the evolutionary operations and th5. If the feasible solutions obtained during the above step can meet the require-ments, then the computation will stop. Otherwise, the feasible solutions ob-tained during the above step are taken as the initial solutions, and the compuGenerally, as one global optimization method, EC has the following characteris-tics: (i) The search process begins from one group and not from one point; (ii) onthe objective function is used in the search process; and (iii) the random method isused in the search process. Therefore, this method has the following advantages: (i)Highly versatile and can be used for different problems; (ii) it can solve problemsthat are highly nonlinear and nonconvex; and (iii) the model’s plasticity is verThe three evolutionary computation algorithms related to the research on thisLLM-EPS include GA, EP, and GP. In the next section of the chapter, these algAs the most widely used EC method, GA has a solid biologic foundation. Itsbasic principles are from Darwin’s evolution theory and Mendel’s genetic theor1.Individual encoding. According to Mendel’s genetic theory, the individualmust be represented by its genotype. Therefore, the binary encoding methodis generally applied, which is one string of numbers 0 and 1. However, for acomplicated engineering problem, the real encoding method is always us2.Crossover operation. This operation is completed to mimic the genetic re-combination during biological mating. Generally, random pairing is applied.For different encoding methods, the crossover operation is different. For bi-nary encoding, the point crossover is utilized. For real encoding, the discretecrossover and arithmetic crossover are always used. Moreover, this operatio3.Mutation operation. This operation is completed to mimic the gene mutationduring the genetic process. Generally, for binary encoding, the simple pointmutation is utilized. For real encoding, the uniform mutation and non-uniformmutation are always used. Moreover, this operation is conducted with a littl4.Selection operation. This operation is completed to mimic the environmentselection of Darwin’s evolution theory. Therefore, it is also called the repro-duction operation. Generally, roulette wheel selection is widely used. How-ever, stochastic tournament selection is a better operation for complicated en5.Termination criterion. Generally, the termination criterion specifies that thedifference between the maximum fitness value and average fitness value ofone group is less than one error ϵ. To avoid infinite iteration, a maximum num-ber of evolutionary generations is also specified. Moreover, there are someparameters to be determined: the number of individuals, crossover probabil-ity, mutation probability, ϵand maximum number of evolutionary generations.Generally, these parameters are determined based on experience and a testThe main operations of EP are as follow [22], [23].1.Mutation operation. This operation is the sole optimization operation; this isthe specialty of the EP. Generally, Gauss random mutation is utilized.2.Testing feasibility of the individual. The fundamental nature of mutation in-volves a type of random alteration to the original individual, which meansit cannot ensure that the newly mutated individual remains within the searchspace; in other words, a non-feasible individual could be generated. The exis-tence of a non-feasible individual can not only make the result incorrect butalso make the efficiency low. To overcome this problem, a simple and easymethod is proposed [23]. If the new mutated individual is non-feasible, it willbe replaced by a randomly created feasible individual. However, this techniqmay have a very low efficiency for certain problems.3.Selection operation. The selection operation is a stochastic tournament model,where individuals are probabilistically chosen based on their fitness, ensurin4.Termination condition. This is the same as that of the GA. Moreover, thereare some parameters to be determined: the number of individuals, ϵand max-imum number of evolutionary generations. Thus, the number of parametersfor EP is less than that for the GA. Generally, these parameters can also Because GP is proposed based on the basic GA principles, the procedures oftwo algorithms are nearly the same [24]. The main operations of GP are as follows.1.Individual encoding. The tree encoding method is applied. In other words,the layering tree structure expression is applied, which is shown in top of2.Crossover operation. Generally, random pairing and point crossover are uti-lized. This operation is also conducted with a large probability.3.Mutation operation. Generally, simple point mutation is utilized. This ope5.Termination criterion. This is the same as that of GA. Moreover, the paraLLMs are sophisticated AI systems that specialize in understanding, generating,and predicting human-like text. By analyzing extensive datasets, these models havemade remarkable strides in various natural language processing tasks. Their capa-bilities include text generation, translation, and summarization, enhancing commuThe evolution of language models has progressed from statistical methods t•Statistical language models: Early models, such as n-grams, estimated theprobability of a word based on its preceding words but struggled with d•Neural network models: The introduction of neural networks allowed folanguage models. Recurrent neural networks and long short-term memory nemechanisms to process input data in parallel, enabling the capture of long-range dependencies more effectively. This architecture underpins many modLLMs are built upon several key concepts. Fristly, LLMs assign probabilities tosequences of words, modeling the likelihood of a word following a given contextP(w1, w2, . . . , w n) =nYi=1P(wi|w1, w2, . . . , w i−1) (1.1)where P(wi|w1, w2, . . . , w i−1)represents the conditional probability of word wSecondly, LLMs employ deep neural networks with multiple layers and param-eters to learn complex language patterns. The training process involves adjustingthese parameters to minimize a loss function, typically the cross-entropy loswhere Nis the number of words in the training corpus.Transformers use self-attention mechanisms to weigh the importance of differentwords in a sequence, enabling the model to capture contextual relationships ewhere Q(queries), K(keys), and V(values) are matrices derived from the inpuTraining LLMs involves several key steps. First, data collection is essential, asit requires aggregating extensive text data from diverse sources to capture a wirange of language patterns. Once the data is collected, model training begins, uti-lizing high-performance computing resources to optimize the model’s parametersthrough techniques such as gradient descent. Additionally, scaling laws have beenempirically observed, which suggest that increasing the model size, training data,and computational resources leads to improved performance. One notable scaliwhere Cis the training cost in FLOPs, Nis the number of model parameters, Dis the size of the training dataset in tokens, and C0is a constant.Despite their capabilities, LLMs face with several limitations. LLMs may gen-erate factually incorrect or contextually misleading outputs colloquially known as“hallucinations.” In addition, their large, black-box structures can complicate in-terpretability, making it challenging to understand precisely why a particular tokenor code snippet is produced. Moreover, controlled generation, where specific con-straints or style guides must be applied, can be non-trivial to implement. Regard-less, their unprecedented ability to generate complex textual or code-based artifactshas spurred interest in integrating LLMs into evolutionary workflows, particularlyfor tasks that benefit from a human-like creative sp···LLM-based EPS: better heuristics are created via prompting LLMsover program codes.evolve···𝑝𝑜𝑝𝑢𝑙𝑎𝑡𝑖𝑜𝑛0exchange sub-parts···𝑝𝑜𝑝𝑢𝑙𝑎𝑡𝑖𝑜𝑛1selectionGenetic Programming: better heuristics are created through genetic operations over trees.···Function setadd  subfor  sum  ⋯⋯Terminal setbins itemstrue false⋯⋯specified byhuman expertsdefinitial_heuristic(item: float, bins: list) -> list:```Define priority with which we assign an item to each bin.:param item: Size of item to be added to the bin.:param bins: List of capacities for each bin.:return: a list showing priority score of each bin.```return[0] * len(binLLM-EPS combines the adaptive search capabilities of evolutionary algorithmswith the generative power of LLMs, unlocking new possibilities for AHD. Unliketraditional optimization methods that produce numeric solutions, LLM-EPS gener-ates interpretable programs or code snippets, allowing for seamless interaction and1.3 illustrates the fundamental differences between GP and LLM-EPSFunSearch, short for "search in the function space," represents a groundbreak-ing approach in leveraging LLMs for program synthesis and optimization. Unliketraditional methods, which often face limitations due to hallucinations or verifi-cation challenges. FunSearch innovatively combines an LLM with a systematicevaluator. This evaluator rigorously scores candidate programs, ensuring correct-ness and quality, and plays a pivotal role in evolving low-performing solutions inttionary methodology, which starts with a user-defined "skeleton" program. Thisskeleton encapsulates known functional structures and isolates the critical logicfor optimization, thereby reducing the search space and improving accuracy. Thesystem employs an island-based model to maintain a diverse pool of candidate prgrams it has generated so far (retrieved from the programs database), and asked to generatean even better one. The programs proposed by the LLM are automatically executed, andevaluated. The best programs are added to the database, for selection in subsequent cycles.The user can at any point retrieve the highest-scoring programs discovered so far.through genetic-like operations, FunSearch achieves robust exploration across The distributed architecture of FunSearch enhances scalability and efficiency.The system comprises three main components: a programs database, samplers,and evaluators. The programs database stores correct and promising programs,while samplers use LLMs to generate new variations based on prompts constructedfrom high-scoring programs. Evaluators assess these programs by executing themagainst a predefined "evaluate" function, which measures the quality of outputsprompting ensures that high-quality programs guide the evolution process, whileclustering within islands helps preserve program diversity. The inclusion of Boltmann selection mechanisms and fitness-based sampling further refines the evolu-tionary trajectory, favoring both performance and interpretability. The system’s dis-tributed nature supports scalability, enabling it to tackle large, complex problemsacross a variety of domains, from mathematical discovery to algorithm design.This architecture not only delivers impressive results but also highlights Fun-Search’s adaptability. By systematically blending the creativity of LLMs with rig-orous evaluation and evolutionary improvements, FunSearch sets a new benchmarkfor program synthesis, pushing the boundaries of what LLMs can Algorithm Mutation Prompt EngineeringAlgorithm Crossover Prompt EnAnother noteworthy approach is Evolution of Heuristics (EoH), aimed explicitlyat AHD while minimizing manual labor. EoH represents heuristics both as naturallanguage “thoughts” and as executable code, mirroring the way human expertsconceptualize and then implement strategies. By employing a population-basalgorithm, EoH evolves these heuristics over successive generations with selectionEoH employs a population-based evolutionary model where heuristics evolveover generations. Selection, crossover, and mutation operations enhanced by LLMsheuristics (E1) or exploring variations based on common ideas (E2); and Modi-fication: focuses on refining heuristics for improved performance (M1), adjustingparameters (M2), or simplifying structures by eliminating redundancies (M3).This method requires fewer queries to LLMs compared to previous techniquessuch as FunSearch. Additionally, it eliminates the need for custom-crafted featuresor additional training. The upshot is a more streamlined method that outperformsmanually created heuristics across diverse optimization benchmarks. As EoH re-duces the overhead In pursuit of state-of-the-art results, Reflective Evolution (ReEvo) integratesLLMs with a GP foundation to tackle NP-hard combinatorial optimization tasks.ReEvo employs two types of LLMs: a Generator LLM, which produces initial codesnippets or heuristics, and a Reflector LLM, which provides both short-term steps: (i) Population Initialization: Prompts generate initial heuristic candidates.(ii) Selection: Parent heuristics are chosen based on their performance. (iii) Short-term Reflection: Comparative analysis of parent performance informs offspringgeneration. (iv) Crossover: Generates offspring by combining parent features. (v)Long-term Reflection: Consolidates insights from iterations to improve heuristicgeneration. (vi) Elitist Mutation: Refines the best heuristic further using accumuBy synthesizing immediate feedback (short-term reflection) and broader ining frameworks like EoH and FunSearch. Its verbal “gradient,” derived from reflec-tive insights, leads to a gentler fitness landscape and boosts optimization stability,making it a potent approach for real-world combinatorial problemsDiversity is a cornerstone of successful evolutionary algorithms, especially inscenarios with multiple objectives [29]. A population rich in diverse solutionsmaintains broad coverage of the search space, mitigating the dangers of stagna-tion in local optima and supporting adaptability to non-static or convoluted fitnesslandscapes. Various methods clustering, niching, or entropy-based measures havebeen proposed to safeguard diversity, with Shannon entropy being a popular metricfor assessing how “spread out” a set of solutions is [30], [31].However, embedding these diversity concepts into LLM-EPS raises fresh chal-lenges. Populations are not numeric vectors but heuristic code snippets or descrip-tive text, which calls for alternative ways to quantify “distance” or “difference.”What strategies can be employed to evaluate the semantic gap between two heuris-tic code snippets or two text proposals while effectively capturing their functionaldifferences? These inquiries highlight a pressing need for innovative strategies iCrucially, diversity acts not only as a buffer against premature convergence, butalso as a catalyst for creative or unconventional strategies, a vital ingredient in ad-vanced heuristic design. By developing refined diversity metrics and preservingvariability throughout the evolutionary process, researchers can unlock a broaderrange of potential solutions, leading to improved performance and more generalizThis chapter has charted the emerging partnership between EC and LLMs, show-casing how their combined strengths can transform AHD. In LLM-EPS, LLMscontribute a wellspring of fresh heuristic ideas, and evolutionary methods systeatically sharpen and validate these concepts. Yet, to extract maximum value fromthis synergy, careful attention must be paid to diversity the balancing force thaFrom traditional GA to newer frameworks like FunSearch, EoH, and ReEvo,the literature consistently highlights the importance of extensive exploration toavoid becoming trapped in suboptimal solutions. Diversity is the crucial compo-nent that integrates a genuinely adaptive search process, facilitating breakthroughsin problem-solving. In the following chapter, advanced methods for incorporatThis chapter introduces a method for encoding the LLM-EPS population andproposes two metrics for measuring diversity: SWDI and CDI. Following this, adiversity analysis will be conducted on previous LLM-EPS frameworks, includiOne particular problem in measuring diversity in LLM-EPS is how to encodethe population. While each individual in the traditional evolutionary algorithm isencoded as a vector, in LLM-EPS they are usually presented as a string of codesnippets/programs. This poses a challenge in applying previous diversity metricsto the population of LLM-EPS frameworks. To address this challenge, an encod-ing method is proposed that involves three steps: (i) removing comments and doc-strings using abstract-syntax tree, (ii) standardizing code snippets into a commoncoding style (e.g., PEP81), (iii) converting code snippets to vector representatioInspired by ecological studies, [32] provides a quantitative measure of speciesdiversity within a community. In the context of search algorithms, this index aimsto quantify the diversity of the population at any given time step based on clustersof individuals. To compute the SWDI for a specific set of individuals within acommunity, or archive, it is first necessary to determine the probability distributionof individuals across the clusters. This is represented awhere Ciis a cluster of individuals and Mrepresents the total number of individu-als across all clusters {C1, . . . , C N}. The SWDI, H(X), is then calculated using This index serves as a crucial metric in maintaining the balance between explo-ration and exploitation within heuristic search spaces. A higher index score sug-gests a more uniform distribution of individuals across the search space, thus pmoting exploration. Conversely, a lower index score indicates concentration aroundspecific regions, which may enhance exploitation but also increase the risk of preTo obtain the SWDI score, at each time step t, all individuals Ri={r1, . . . , r n}generated during that time step are added to an archive. The population encodingmethod described in Section 2.1 is utilized to derive vector representations V={v1, . . . , v n}. Following this, cosine similarity is used to compute the similarTo identify clusters in the archive, each embedding vector viis analyzed. Next,viwill be assigned to a cluster Ciif the similarity between viand all members ofCiexceeds a specified threshold α. In mathematical terms, viis assigned to Ciif:similarity (vi, vk)≥α∀k∈ {1, . . . ,|Ci|} (2.4)If no cluster Ciin {C1, . . . , C N} meets the condition, a new cluster CN+1willbe created and viwill be assigned to CN+1. Finally, the SWDI computed score isdetermined by calculating Equations (2.1) and (2.2) across the identified clustersWhile SWDI focuses on diversity of different groups of individuals, the CDIplays a crucial role in understanding the spread and distribution of the whole popu-lation within the search space [33], [34]. In the context of heuristic search, the CDImeasures how well a system’s energy, or diversity, is distributed from a centralizeMany entropy interpretations have been suggested over the years. The best knownare disorder, mixing, chaos, spreading, freedom and information [35]. The first de-scription of entropy was proposed by Boltzmann to describe systems that evolvefrom ordered to disordered states. Spreading was used by Guggenheim to indicatethe diffusion of a energy system from a smaller to a larger volume. Lewis statedthat, in a spontaneous expansion gas in an isolated system, information regardingparticles locations decreases while, missing information or uncertainty increases.In graph theory, each edge can be assigned a weight representing, for example,the energy required to traverse that edge. A graph with a complex structure andvarying edge weights exhibits higher entropy, indicating a more disordered systeConversely, a simpler structure with uniformly distributed weights corresponds tAn Minimum Spanning Tree (MST) of a connected, edge-weighted graph is asubset of the edges that connects all vertices without any cycles and with the min-imum possible total edge weight. Constructing an MST can be seen as a processof transitioning from a high-entropy state (the original graph with all its edges andweights) to a lower-entropy state. This transition reflects the idea of energy distri-bution from a "better located" state to a "more distributed" one, as the MST repre-sents the most efficient way to connect all vertices, minimizing the total "energyConsider a connected, undirected graph G= (V, E)with a weight function w:E→R+. The entropy Hof the graph can be associated with the distribution ofedge weights. One way to define this entropywhere p(e)is probability-like measure of edge ebased on its weight.The MST, T= (V, ET), is a subgraph where ET⊆Eand the total weightPe∈ETw(e)is minimized. By selecting the subset of edges that connect all ver-tices with the minimal total weight, the MST reduces the system’s complexity andTo calculate the CDI, let’s consider all individuals within an archive, representedby their respective embeddings. By constructing a MST that connects all individu-als within the archive A, where each connection between individuals is calculatedusing Euclidean distance. This MST provides a structure to assess the diversity ofthe population. Let direpresent the distance of an edge within the MST, wherei∈ {1,2, . . . , #A−1}. The probability distribution of these distances is given bThe cumulative diversity is then calculated using the Shannon entrThis method enables the collection of the complete diversity present in thesearch space, offering valuable information about the range of solutions. HigherCDI values indicate a more distributed and diverse population, which is essentiaAn interesting point in Shannon Diversity Index (SDI) theory is that normalizingthe SDI with the natural log of richness is equivalent to the evenness valueHwhere H′represents the richness of SDI and Sis the total number of possiblecategories [36]. The significance of evenness is to demonstrate how the proportionNow, consider normalizing with the two current diversity indices. Firstly, in thecase of SWID, if there are Nclusters and the count of individuals in each clusterfrom C1toCNis uniform, then the evenness will consistently equal 1, regardlessof the value of N. This is not the preferred outcome since the number of clustersshould be considered as a factor of diversity. Likewise, normalizing the CDI willalso overlook the influence of the number of candidate heuristics, which corre-spond to the nodes in the MST. This leads to the proposal not to normalize th2.4 Exploring the correlation between objective score and diversity measureTo investigate the relationship between the two diversity measurement metricsand the objective score, three experimental runs were performed using ReEvo onthree distinct AHD problems, Bin Packing Online (BPO), Traveling SalesmanProblem (TSP) with Guided Local Search (GLS) [37], and Orienteering Prob-lem (OP) with Ant Colony Optimization (ACO) solver [38]. Details of the experi-ments can be found in Section 4.1. The diversity evaluation process was conductedwith the code embedding model used is CodeT5+2[39], and similarity thresholdα= 0.95where α∈[0; 1]. The maximum budget for each run is 450K tokens. Theobjective scores and the two diversity metrics from these runs on BPO, TSP, andFrom figure 2.1, there are a few observations that can be drawn. First, the two di-versity measurement metrics have a noticeable correlation with thof the problem. When the objective score is converged into a local optima, theframework either tries to focus on the exploration by increasing the diversity of thepopulation, through the increase of SWDI in the first and second runs or tries tofocus on the exploitation of the current population, thus decrease the SWDI in thethird run. Focusing on exploration can lead to considerable gains in the objectivescore, whereas concentrating on exploitation may cause the population to remainstuck in local optima for an extended period. However, if the whole population isnot diverse enough, as can be seen with the low CDI of the second run, the pop-ulation might not be able to escape the local optima. These observations aboutthe correlation between the objective score and diversity measureTo analyze the overall diversity of previous LLM-EPS frameworks, experimentswere conducted on three distinct LLM-EPS frameworks: FunSearch [13], ReEvo[15], and EoH [14], focusing on three separate AHD problems (BPO, TSP, OP).The details of each experiment are presented ilighted in the previous section, SWDI focuses on understanding the diversity ofthe population during a single run. As such, it may not help quantify the diversityacross multiple experiment runs. Fig. 2.4 presents the exIn BPO and TSP, EoH obtained the highest CDI but got the worst objectivescore. This implies that EoH does not focus enough on exploitation to optimizethe population better. In contrast, while ReEvo and FunSearch obtain lower CDIthan EoH on BPO and TSP, they achieve better objective performance on all threeproblems. The experiments conducted on the BPO and TSP problems illustratethe inherent trade-off between diversity and objective performance in LLM-EPSframeworks. However, for the OP problem, it is evident that a high CDI is necessaryto achieve a better objective score, which aligns with the findings discussed iIn conclusion, this chapter has introduced a comprehensive approach to mea-suring diversity within LLM-EPS frameworks, focusing on SWDI and CDI. Theproposed population encoding method addresses the unique challenge of repre-senting code snippets as vectors, enabling the application of these diversity met-rics. Through extensive experiments on various AHD problems, the correlatibetween diversity metrics and objective scores has been explored, revealing thedelicate balance between exploration and exploitation in heuristic search spaces.The analysis of previous LLM-EPS frameworks, including FunSearch, ReEvo, andEoH, underscores the trade-offs between diversity and performance, highlightingthe importance of maintaining an optimal level of diversity to avoid premature cThis chapter introduces a novel LLM-EPS framework called Harmony SearchEvolution (HSEvo). HSEvo aims to enhance the diversity of the population whileimproving optimization performance and relieving the trade-off between these twoaspects through an individual tuning process based on harmony search. Addition-ally, the framework aims to minimize costs associated with LLMs by incorporatiIndividual encoding. In HSEvo, each individual within the evolutionary pro-rect inheritance from earlier LLM-EPS such as EoH and ReEvo. This code snippet,generated by a LLMs, embodies a specific heuristic function tailored to the targetoptimization problem. This encoding method is deliberately flexible, eschewingany predefined format or structure, which allows for the generation of diverse andcomplex heuristics. Unlike traditional methods that might use fixed-length vectorsor predefined data structures. HSEvo’s code-based approach enables the expres-sion of a wide range of algorithmic strategies, including conditional logic, loops,and function calls, making it highly adaptable to various problem domains.The choice of code as the encoding mechanism also facilitates direct evaluationof the generated heuristics. Each code snippet is directly executable, either within asandboxed environment or through a standard Python subprocess, simplifying theevaluation process and eliminating the need for intermediate translation steps. Thisdirect executability is particularly advantageous for AHD problems, where the goaInitialization. HSEvo initializes a heuristic population by prompting the gener-ator LLM with task specifications that describe the problem and detail the signatureof the heuristic function to be searched. Additionally, to leverage existing knowl-edge, the framework can be initialized with a seed heuristic function and/or exter-nal domain knowledge. To promote diversity, prompts are created with various roleinstructions (e.g., "You are an expert in the domain of optimization heuristics...","You are Albert Einstein, developer of relativity theory..."), details instructions inthe next Section 3.2.1. This approach aims to enrich the heuristic generationof heur isticx NSelec tionCr osso v erFlash Reflec tionElitist MutationHar m on y Se aHeur isticDeep analy sisLLMElitist Heur isticReflec tionFlo w dir ec tionSt opThe LasSelection. In HSEvo, the selection of parent pairs for crossover is performedusing a random selection strategy. This approach involves choosing two individu-als from current population with equal probability, without any bias towards theirfitness or performance. The primary goal of this random selection is to maintain abalance between exploration and exploitation during optimization process. By notfavoring the best-performing individuals, HSEvo ensures that search space is notprematurely narrowed, allowing for the discovery of potentially novel and effectiveheuristics that might be overlooked by more deterministic selection methods.Furthermore, random selection mechanism is designed to counteract prema-ture convergence, a phenomenon where the population becomes homogeneous tooquickly, hindering further exploration. This is particularly important in the contextof LLM-EPS, where the initial population might be biased towards certain types ofsolutions. The random selection helps to maintain diversity within the population,preventing the search from getting stuck in local optima and promoting a morethorough exploration of the heuristic space. This is supported by observations ofthe SWDI trajectory in Section 2.4, which suggests that random selection can helpto maintain a broader search width, thus avoiding premature convergence.Flash reflection. Reflections can provide LLMs with reinforcement learningreward signals in a verbal format for code generation tasks, as discussed by [40].Later, [15] also proposed integrating Reflections into LLM-EPS in ReEvo as anapproach analogous to providing “verbal gradient” information within heuristicspaces. HSEvo argue that reflecting on each pair of heuristic parents individuallyis not generalized and wastes resources. To address these issues flash reflectiproposed to alternative Reflection method for LLM-EPS. The flash reflection pro•First, at each time step t, the individuals selected during the selection phaseare grouped, and any duplicate individuals are removed. This ensures that theanalysis is performed on a unique set of heuristics. The LLM then undertakesa deep analysis of the performance ranking of these individuals. This take in-puts as mixed ranking pairs (i.e., one good performance parent and one worseperformance), good ranking pairs (i.e., both good performance), and worseranking pairs (i.e., both worse performance), then return a comprehensive de-scription on how to improve as a text string.•The second step, flash reflection involves comparing the current analysis attime step twith the previous analysis from time step t−1. This comparison isperformed by the LLM, which then generates guide information based on thedifferences and similarities between the two analyses. This guide informationis designed to be used in subsequent stages of the evolutionary process (e.g.crossover and mutation) to further refine the heuristics. By incorporating thistemporal aspect, flash reflection allows LLM to learn from its past experi-ences and adapt its code generation strategy over time. This iterative processof analysis and comparison enables LLM to continuously improve its under-standing of the heuristic space and generate increasingly effective solutions.In essence, flash reflection in HSEvo acts as a form of meta-learning, where theLLM not only generates heuristics but also learns how to generate better heuristicsover time. By providing a more generalized and resource-efficient approach to re-flection, HSEvo aims to overcome the limitations of previous methods and achievemore effective and efficient heuristic discovery. The textual feedback generated byflash reflection provides a rich source of information that guides the LLM towardsgenerating more promising solutions, ultimately leading to improved performancCrossover. In this stage, new offspring algorithms are generated by combiningelements of the parent algorithms. The goal is to blend successful attributes fromtwo parents via guide result guide information part of flash reflections. Throughthis step, HSEvo hopes to produce offspring that inherit the strengths of both,which can potentially lead to better-performing heuristics. The prompt includestask specifications, a pair of parent heuristics, guide information part of flash reElitist mutation. HSEvo employs an elitist mutation strategy, focusing its muttion efforts on the "elite" individual, which represents the best-performing heuris-tic found so far. This approach leverages the generator LLM to mutate this eliteindividual, incorporating insights derived from the LLM-analyzed flash reflec-tions. The mutation process is not random; instead, it is guided by the accumu-lated knowledge and feedback from the flash reflection, aiming to enhance theelite heuristic’s performance. Each mutation prompt includes detailed task specifi-cations, the code of the elite heuristic, the deep analysis from the flash reflections,and specific generation instructions, providing the LLM with the necessary contexThis elitist mutation strategy is designed to ensure continuous improvement inperformance while preserving the quality of the top solutions. By focusing on thebest-performing heuristic, HSEvo aims to refine and enhance its capabilities, ratherthan randomly exploring the search space. The incorporation of insights from theflash reflections ensures that the mutation process is not arbitrary but rather di-rected towards making meaningful improvements based on the accumulated knowl-edge of the evolutionary process. This approach allows HSEvo to effectively exploitthe best solutions found so far, while still allowing for exploration through otheHarmony Search. From the analysis in Section 2.4 and 2.5. The hypothesisis that if the population becomes too diverse, individual heuristics within it aremore likely to be unoptimized, which can negatively impact the overall optimiza-tion process. To mitigate this, HSEvo employs HS to fine-tune the parameters (e.g.,thresholds, weights, etc.) of the best-performing individuals in the population. Th•First, once an individual is selected, the LLM is tasked with extracting param-eters from the best individual (i.e., code snippets, programs) of the population•Following the parameter extraction, HSEvo employs the HS algorithm, a meta-heuristic optimization technique inspired by the improvisation process of mu-sicians. The HS algorithm iteratively adjusts the extracted parameters withintheir defined ranges, aiming to find a combination that maximizes the heuris-tic’s performance. This process involves creating a "harmony memory," whichstores a set of parameter combinations, and iteratively generating new har-monies by adjusting existing ones or creating new ones randomly. The per-formance of each harmony is evaluated, and the harmony memory is updatedwith better-performing combinations. This iterative process continues forpredefined number of iterations, allowing the HS algorithm to explore param-eter space and identify optimal or near-optimal parameter settings.•After the parameter optimization is complete, the optimized individual is markedto prevent it from being optimized again in future time steps. This ensures thatthe HS algorithm is not repeatedly applied to the same individuals, allowingthe framework to focus on optimizing other promising heuristics. The opti-mized individual is then added back to the population, contributing to theoverall diversity and performance of the population. This integration of HSinto the evolutionary process allows HSEvo to fine-tune the best-performingheuristics, enhancing their performance and mitigating the negative effects oTo sum up, the HS component in HSEvo is a critical mechanism for enhancingthe performance of the best-performing heuristics by fine-tuning their parameters.This process involves using the LLMs to extract parameters and define their ranges,followed by the application of the HS algorithm to optimize these parameters. Theoptimized individuals are then added back to the population, contributing to theoverall performance and diversity of the population. This strategic integration ofHS allows HSEvo to effectively balance exploration and exploitation, leading tAn overview of the flow of HSEvo is also described through pseudo-algorithm1, where function_evals <max_fe serves as the stopping condition. Here,function_evals is a counter variable tracking the number of evaluated indi-viduals, and max_fe is the maximum number of individuals HSEvo will evalInitialization : Load configuration, initialize LLM prompts, and geReflection : Perform flash and comprehensive reflection using LLMCrossover : Generate and evaluate new individuals by crossing parentMutation : Generate and evaluate new individuals by mutating the bExtract parameters and code for harmony seaCheck and update reflection lists based on elitist chaThis section synthesizes the prompts used in the HSEvo framework. The HSEvoprompt system includes four stages: initialization, population, flash reflection, crossov{role_init} helping to design heuristics that can effectively solve optimizaYour response outputs Python code and nothing else. Format your code as{role_init} Your task is to write a {function_name} function for {proIn this work, the variable role_init is utilized to assign various roles to theLLMs, allowing the creation of distinct personas [41] that can provide differentperspectives and promote population diversity. The roles are chosen in a round•You are an expert in the domain of optimization hcorrespond to the heuristic function name, the specific problem description, andthe function description, respectively, which will be detailed in the sectiRefer to the format of a trivial design above. Be very creative and give ‘{func-tion_name}_v2’. Output code only and enclose your code with Python codHere, task_description refer to Prompt 2, seed_ function is a trYou are an expert in the domain of optimization heuristics. Your task is toprovide useful advice based on analysis to design better heuristicPrompt 5: User prompt for flash reflection pharse Below is a list of design heuristics ranked from best to wors- Keep in mind, list of design heuristics ranked from best to worst. Meaningthe first function in the list is the best and the last function in the list is th- The response in Markdown style and nothing else has the following st+ Meticulously analyze comments, docstrings and source code of severalpairs (Better code - Worse code) in List heuristics to fill values for **AnalyExample: “Comparing (best) vs (worst), I see ...; (second best) vs (secondworst) ...; Comparing (1st) vs (2nd), I see ...; (3rd) vs (4th) ...; Comparin+ Self-reflect to extract useful experience for design better heuristics and filI’m going to tip $999K for a better heuristics! Let’s think step by step.list_ranked_heuristics is a list of heuristic functions obtained by group-ing parent pairs from selection, removing duplicates, and ranking based on Prompt 6: User prompt for flash reflection pharse 2.Your task is to redefine ‘Current self-reflection’ paying attention to avoid allthings in ‘Ineffective self-reflection’ in order to come up with ideas to Response ( <100 words) should have 4 bullet points: Keywords, AdviceI’m going to tip $999K for a better heuristics! Let’s think step by step.current_reflection is the output of the flash reflection step 1 from thecurrent generation. good_reflection is the output of the flash reflection step2 from previous generations where a new heuristic was discovered. Conversely,bad_reflection is the output of the flash reflection step 2 from pYour task is to write an improved function ‘{func_name}_v2‘ by COMBIN-ING elements of two above heuristics base Analyze & experience.Output the code within a Python code block: “‘python ... “‘, has commentand docstring ( <50 words) to description key idea of heuristics design.I’m going to tip $999K for a better heuristics! Let’s think step by stepnature_worse ,code_worse are the function signatures and code of the bettand worse parent individuals, respectively. flash_refection is the output ofthe flash reflection step 2 from the current generNow, think outside the box write a mutated function ‘{func_name}_v2‘ bet-ter than current version. You can using some hints if needOutput code only and enclose your code with Python code block: “‘pythoI’m going to tip $999K for a better solutionnatures and code of elitist individuals of current generatioYou are an expert in code review. Your task extract all threshold, weight orhardcode variable of the function make it become default parameterNow extract all threshold, weight or hardcode variable of the function makeit become default parameters and give me a ’parameter_ranges’ dictionaryrepresentation. Key of dict is name of variable. Value of key is a tuple inPython MUST include 2 float elements, first element is begin value, secon- Output code only and enclose your code with Python code block- Output ’parameter_ranges’ dictionary only and enclose your code witelitist_code is the snippet/string code of the elitist individual in the cur-rent population that has not yet undergone harmony searBPO Solving online Bin Packing Problem (BPP). BPP requires packing aset of items of various sizes into the smallest number of fixed-sizedbins. Online BPP requires packing an item as soon as it is received.TSP Solving Traveling Salesman Problem (TSP) via guided local search.TSP requires finding the shortest path that visits all given nodes anOP Solving a black-box graph combinatorial optimization problem vBPO The priority function takes as input an item and an array ofbins_remain_cap (containing the remaining capacity of each bin) andreturns a priority score for each bin. The bin with the highest prioritTSP The ‘update_edge_distance’ function takes as input a matrix of edgedistances, a locally optimized tour, and a matrix indicating the num-ber of times each edge has been used. It returns an updated matrix ofedge distances that incorporates the effects of the local optimizationand edge usage. The returned matrix has the same shape as the in-put ‘edge_distance’ matrix, with the distances adjusted based on thOP The ‘heuristics’ function takes as input a vector of node attributes(shape: n), a matrix of edge attributes (shape: n by n), and a constraintimposed on the sum of edge attributes. A special node is indexedby 0. ‘heuristics’ returns prior indicators of how promising it is toinclude each edge in a solution. The return is of the same sThe best heuristics generated by HSEvo for all problem settings are presentedTo summarize, this chapter presented HSEvo, an innovative LLM-EPS frame-work that transforms heuristic optimization by integrating population diversity andperformance through advanced techniques such as flash reflection and harmonysearch. HSEvo utilizes code-based individual encoding to enhance its adaptabilityin addressing a wide range of intricate optimization challenges. Key components,such as the random selection mechanism, flash reflection, and elitist mutation, en-sure thorough exploration of the heuristic space while preventing premature con-vergence. By incorporating the HS algorithm for fine-tuning parameters, HSEvofurther optimizes its best-performing heuristics, achieving a synergy between ex-ploration and exploitation. Along with this, detailed descriptions of prompts, pseu-docode, an2 """Returns priority with which I want to add item to each 5 item: Size of item to be added to the b9 Arrselection in online BPP with a more holistic approach.""11 # Avoid division by zero by adjusting remainin22 # Enhanced overflow penalty: stronger influence for 25 # Logarithmic penalty for remaining cap8 # Comprehensive scoring integrating all metrics for a13 prioritized_scores = (scores - np. min(scores)) / (max_score15 # Invert scores for selecting t9 Update edge distances using adaptive penalties and bonuses10 while ensuring nuanced pe6 boost = bonus_factor * (1 + (boo16 # Update the distance with 16 # Normalize node attributes (maintain zero div29 # Multi-dimensional scaling based o4 # Layered logic for combined node influence using8 ) ** 1.5 # Further amplify the i15 penalty_factor = 1 / (1 + penalty_bTo unveil the potential of the HSEvo framework. This chapter presents a seriesof experiments designed to benchmark its performance against existing LLM-EPSmethodologies. By tackling a range of optimization challenges, including BPO,TSP, and OP, this thesis examines how effective HSEvo is in improving heuristBenchmarks: To evaluate the diversity and objective scores of HSEvo in com-parison to earlier LLM-EPS frameworks, the same benchmarks outlined in Section2.4 were employed to conduct experiments on three distinct AHD problems: BP•BPO: packing items into bins of fixed capacity in real-time without priorknowledge of future items. In this benchmark, LLM-EPS frameworks nee•TSP: find the shortest possible route that visits each city exactly once andreturns to the starting point. In this setting, LLM-EPS frameworks need todesign heuristics to enhance the perturbation phase for the GLS solver.•OP: find the most efficient path through a set of locations, maximizing thetotal score collected within a limited time or distance. This setting requiresLLM-EPS frameworks to design herustics used by the ACO solver.Experiment settings: All frameworks were executed under identical environ-EPS frameworks and the evaluations of heuristics were conducted using a sinDefinition. The objective of this problem is to effectively assign a diverse col-lection of items, each with its own specific size and weight, into the smallest pos-sible number of containers, each having a fixed capacity denoted by C. This chal-lenge is particularly focused on the online packing scenario, where items are in-troduced one at a time and must be packed immediately upon arrival without priorknowledge of future items. This contrasts with the offline scenario, wher# of islands, # of samples per promDataset generation. Following [13], this experiment randomly generates fiveWeibull instances of size 5k with a capacity of C= 100 . The objective score inof five instances, where lbrepresents the lower bound of theoptimal number of bins computed [44] and nis the number of bins used to pack alSolver. LLM-EPS is used to design heuristic functions that are able to solve thDefinition. The TSP is a well-known combinatorial optimization issue in thefield of computer science and operations research. It involves a scenario where asalesman must visit a given set of cities, with the objective of finding the shortestpossible route that allows him to visit each city exactly once before returning to hisstarting point, which is often referred to as the origin city.Dataset generation. Following [37], this experiment generates a set of 64 TSPinstances with 100 nodes (TSP100). The node locations in these instances are ran-domly sampled from [0,1]2. This means that the nodes are positioned within asquare area bounded by (0, 0) and (1, 1). The average gap from the optimal solu-tion, generated by Concorde [45], is used as the objective score.Solver. GLS was used as the solver for this benchmark. GLS explores the so-lution space using local search operations guided by heuristics. The idea behindusing this solver is to explore the potential of search penalty heuristics foEPS. In this experiment, the traditional GLS algorithm was modified by includingperturbation phases [46], where edges with higher heuristic values are given prioDefinition. In the OP at hand, the primary aim is to maximize the cumulativescore achieved by visiting a series of designated nodes. This objective is subject tothe condition that the total length of the tour does not exceed a specified maximumlimit. The nodes represent key points of interest, each contributing a certain score,and the challenge lies in strategically planning the route to optimize the overallscore while adhering to the constraints of the maximum allowable tour length.Dataset generation. Following the process of DeepACO [38] during the gen-eration of this synthetic dataset. In each problem instance, uniform distribution isused to sample 50 nodes (OP50), including the depot node, from the unit interval[0,1]2. This means that the nodes are positioned within a square area bounded by(0, 0) and (1, 1). Especially, a challenging prize distribution [47] also where d0irepresents the distance between the depot and node i, and the maxiSolver. For this problem, ACO is used as a solver for this benchmark. ACO isan evolutionary algorithm that integrates solution sampling with pheromone trailupdates. It employs stochastic solution sampling, which is biased towards morepromising solution spaces based on heuristics. The population size is set at 2CDI (↑) Obj. (↓) CDI (↑) Obj. (↓) CDI (↑) Objble, it was observed that while HSEvo still hasn’t obtained better CDI than EoH,HSEvo is able to achieve the best objective score on all tasks. On BPO, HSEvooutperforms both FunSearch and ReEvo by a huge margin. This highlights the im-portance of my findings from the analysis in Section 2.5, where it is crucial toimprove diversity in order to optimTo investigate the impact of my framework on the diversity of the population,1Extending FunSearch to solve the OP problem with an ACO solver causedscore of HSEvo through different runs on BPO, TSP, OP problems, respectively.It can draw a similar observation with findings in Section 2.4, where with a highSWDI and CDI, the objective score can be optimized significantly. One thing to2.1, both have the SWDI decreasing over time. However, in HSEvo, the SWDI isat around 3.0 when the objective score improves significantly, then only decreasesmarginally after that. In ReEvo, the objective score improves when SWDI is ataround 3.0 and 2.7, and the magnitude of the improvement is not as large as HSEvo,which implies the importance of diversity in the optimization of the problem aTo gain a better understanding of novel framework HSEvo, The experiment ex-tends to conduct ablation studies on proposed components, the harmony searcAs harmony search is a vital component in HSEvo framework, it will not beeliminated from HSEvo; instead, an experimental setup will incorporate harmonysearch into the ReEvo framework. The results of the experiment concerning the OPboth ReEvo and the variant of ReEvo that includes harmony search, in terms objective score and CDI. Here, notice that harmony search can only marginallyimprove ReEvo. This can be explained that ReEvo does not have any mechanismto promote diversity, therefore it does not benefit from the advantage of harmoAn additional experiment was conducted in which the reflection componentused in ReEvo was replaced with a flash reflection component. Since flash re-flection is more cost-effective than the original reflection, reducing the number oftokens utilized for optimization to 150K tokens serves as a method to validate this.a smaller number of the timestep, ReEvo with flash reflection mechanism can out-perform HSEvo in optimization performance while obtaining comparable resultson CDI. However, when running with a larger number of tokens, ReEvo with flashreflection cannot improve on both objective score and CDI, while HSEvo improvesboth metrics to 5.67 and -14.62, respectively. This implies that without a diversity-promoting mechanism, flash reflection is not enough to improve the optimizatioThe experiments confirm that HSEvo consistently achieves superior objectivescores across BPO, TSP, and OP benchmarks while maintaining strong diversityindices. Additionally, the ablation studies highlight the critical roles of harmonysearch and flash reflection in enhancing HSEvo’s performance. These results val-idate HSEvo’s effectiveness and its potential to advance LLM-EPS frameworks iTo sum up, experimental findings presented in this chapter underscore the sig-nificant advantages of the HSEvo in addressing COPs. By systematically bench-marking HSEvo against established LLM-EPS methodologies across three distinctAHD problems. The results reveal HSEvo’s ability to consistently achieve superiorobjective scores while maintaining robust diversity indices. These outcomes val-idate the hypothesis that diversity-driven optimization strategies, as embodied inHSEvo, are instrumental in advancing AHD and evolutionary processeFurthermore, the ablation studies emphasize the pivotal contributions of the har-mony search and flash reflection components in enhancing performance of HSEvo.While harmony search bolsters the optimization process by promoting diversity,flash reflection offers a cost-effective yet impactful mechanism for refining solu-tions. Together, these components elevate HSEvo’s capabilities, establishing it as arobust framework for optimizing heuristic design. The results of this chapter lay asolid foundation for future exploration of HSEvo’s potential in broader optimiza-tion domains, reinforcing its role as a transformative advancement in LLM-EPThis thesis provides the pivotal role of population diversity in enhancing theperformance of LLM-EPS for AHD. Two novel diversity measurement metrics,the SWDI and CDI, were introduced to analyze and monitor population diversityquantitatively. A comprehensive review of existing LLM-EPS frameworks revealsa common oversight: the failure to adequately balance population diversity witIn response to existing boundaries, the introduction of HSEvo a new state-of-the-art LLM-EPS framework marks a notable advancement in this field. HSEvoleverages a diversity-driven harmony search and genetic algorithm to maintainan optimal balance between exploration and exploitation within heuristic searchspaces. By incorporating mechanisms like flash reflection and elitist mutation,HSEvo achieves high diversity indices and competitive objective scores across var-ious optimization problems, such as the BPO, TSP, and OP. Ablation studies alsoconfirmed the effectiveness of HSEvo’s components, underscoring its capability tThis work also establishes a new standard in the domain of LLM-EPS, high-lighting the important relationship between diversity and optimization. The resultsnot only fill the voids in current frameworks but also create opportunities for fu-ture studies to expand on the suggested metrics and methods. By enhancing thecomprehension of heuristic search spaces and promoting automated design tech-niques. This research offers significant insights and resources for tackling intricatFurthermore, creating effective stopping criteria for the search process is a promis-ing area for research. By integrating diversity metrics and evaluating objectivescores. It may be possible to reduce the computational costs inherent in LLM opHowever, the thesis acknowledges certain limitations, particularly in evaluatingLLM-EPS applications to AHD. Currently, experiments are only confined to GPT-4o-mini model due to the high costs associated with utilizing closed-source APIs.Expanding evaluations to incorporate additional models such as Qwen, LLaMA,Gemini, and others could significantly reinforce the robustness of the findings ofthis thesis. Another area of concern is parameter sensitivity. The harmony searcthat influence performance. This is a gap that persists across the LLM-EPS, icluding in prior works like FunSearch, EoH, and ReEvo. Addressing parametParts of the work presented in this thesis have been accepted at the 39th Annua1.Pham Vu Tuan Dat , Long Doan, and Huynh Thi Thanh Binh, "HSEvo: Eland Genetic Algorithm Using LLMs", The 39th Annual AAAI Conference onArtificial Intelligence , (AAAI-25), 2025. (accepted, rank A* )