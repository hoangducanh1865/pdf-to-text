For me, finishing this project signifies not just a milestone but the start of an exciting new journey. While conducting this thesis, I have often felt lucky. This work would not have been possible without the invaluable guidance and mentorship of Associate Professor Huỳnh Thị Thanh Bình. I am also grateful to my mentor, Long Đoàn, for our weekly discussions and his thoughtful feedback. I would like to extend my thanks to the members of the MSO Lab, who offered their time to enrich this work in countless ways. Last but not least, I want to express my heartfelt appreciation to my beloved family and best friends, who have always been my source of emotional support and encouragement during the most challenging times.Automatic Heuristic Design (AHD) is an active research area due to its utility in solving complex search and NP-hard combinatorial optimization problems in the real world. Recent advances in Large Language Models (LLMs) introduce new pos- sibilities by coupling LLMs with Evolutionary Computation (EC) to automatically EPS). While previous LLM-EPS studies obtained great performance on various tasks, there is still a gap in understanding the properties of heuristic search spaces and achieving a balance between exploration and exploitation, which is a critical factor in large heuristic search spaces. This research addresses this gap by propos- ing two diversity measurement metrics and performing an analysis of previous LLM-EPS approaches, including FunSearch, EoH, and ReEvo. Results on black- box AHD problems reveal that while EoH demonstrates higher diversity than Fun- Search and ReEvo, its objective score is unstable. Conversely, ReEvo’s reflection mechanism yields good objective scores but fails to optimize diversity effectively. In light of these findings, HSEvo was introduced as an adaptive LLM-EPS frame- work that strikes a balance between diversity and convergence through the use of high diversity indices and good objective scores while remaining cost-effective. These results underscore the importance of balancing exploration and exploitation and understanding heuristic search spaces in designing frameworks in LLM-EPS.2.4 Exploring the correlation between objective score and diversity meaworld scenarios. Examples include groups of problems such as routing and logis- tics, scheduling and planning, network design, and others. To address these prob- lems, heuristic methods are among the most commonly employed approaches due to their superior efficiency in terms of time and resources needed to find solutions that approximate the optimal. Over the past few decades, significant efforts have been dedicated to designing effective search methods, resulting in techniques such as simulated annealing, tabu search, and iterated local search, among many oth- ers. These manually crafted methods have been successfully applied in numerous However, due to the diverse nature of practical problems, each application comes with its constraints and objectives, often requiring customization or the selection of specific search methods tailored to the situation at hand. Manually creating, tuning, and configuring search methods for particular problems is not only labor-intensive but also demands deep expertise. This represents a bottleneck in many application domains. Consequently, AHD has emerged as a promising solution, aiming to auto- mate the selection, tuning, or construction of efficient search methods for specific The goal of AHD is to address the difficulties involved in the creation of manual heuristics by utilizing computational methods to automatically generate or enhance (HHs) [1], [2] and Neural Combinatorial Optimization (NCO) [3]–[5] have gained substantial attention for their potential to address the limitations of traditional HHs operate within predefined heuristic spaces curated by human experts, se- lecting or generating heuristics from these established sets. While HHs have demon- strated success, their performance is inherently limited by the scope and quality of the predefined heuristic space, often restricting the discovery of novel or more ef- On the other hand, NCO employs neural networks to learn patterns and predict solutions for COPs. By leveraging data-driven models, NCO seeks to generalize across problem instances, offering a more dynamic approach to heuristic devel- opment. Despite its promise, NCO faces significant challenges. Effective general- ization across diverse problem instances requires robust inductive biases [6], whichare difficult to design and implement. Additionally, the interpretability and explain- ability of the learned models remain pressing concerns [5], making it challenging to validate or trust the solutions produced. These limitations highlight the need for NCO frameworks to evolve further, incorporating mechanisms that enhance their The constraints associated with HHS and NCO underscore the need for more flexible, adaptive, and innovative methods in AHD. Addressing these limitations can unlock the full potential of heuristic design, enabling the development of sys- tems that not only adapt to complex and evolving problem landscapes but also ex- pand the horizons of heuristic discovery beyond current boundaries. Such advance- ments will play a crucial role in shaping the future of optimization and decision- Recently, the rise of LLMs has opened up new possibilities for AHD. It is be- lieved that LLMs [7], [8] could be a powerful tool for generating new ideas and heuristics. However, standalone LLMs with prompt engineering can be insufficient for producing novel and useful ideas beyond existing knowledge [9]. Some attempts have been made to couple LLMs with EC to automatically generate heuristics, known as LLM-EPS [10]–[12]. Initial works such as FunSearch [13] and subse- quent developments like Evolution of Heuristic (know as EoH) [14] and Reflective Evolution (know as ReEvo) [15] have demonstrated significant improvements over previous approaches, generating quality heuristics that often surpass current meth- ods. Even so, ReEvo yields state-of-the-art and competitive results compared with A key difference between LLM-EPS and classic AHD lies in the search spaces spaces such as Rn, whereas LLM-EPS involves searching within the space of func- tions, where each function represents a heuristic as a program. This functional search space introduces unique challenges and opportunities, necessitating a deeper understanding of the properties and characteristics of heuristics within this domain. Despite the advancements made by LLM-EPS frameworks, there remains a signif- icant gap in foundational theories and principles that underpin AHD within these This research focuses on the intersection of EC and LLMs, exploring their po- tential synergy in AHD. History has demonstrated that evolutionary algorithms excel in optimization tasks ranging from combinatorial problems and image en-hancement to industrial scheduling thanks to their population-based search mech- anisms that balance exploration and exploitation. Simultaneously, LLMs such as ChatGPT, Claude, and Gemini have achieved remarkable advancements in natu- ral language processing and code generation, showcasing their ability to produce The primary objective of this research is to explore how these two fields can collaborate to design heuristics for solving COPs. While current research has made progress in metrics such as solution quality and runtime efficiency, maintaining oversight may limit the range of potential solutions, ultimately hindering opportu- Thus, the scope of this study aims at three objectives. First, "tools" should be de- veloped to measure the diversity level promoted by various methods within LLM- selection of suitable diversity-enhancing methods, while also serving as a founda- tion for assessing the balance between exploration and exploitation. Second, the study seeks to analyze how varying levels of diversity impact the search quality of different LLM-EPS frameworks. Finally, this work aims to design and evaluate a ing insights drawn from the second objective through the tools developed in the first objective. This framework will be tested on AHD problems, with results com- pared against existing LLM-EPS methods. This comparison will help evaluate the proposed framework and provide evidence of its potential contributions based on This thesis makes three key contributions to the field of AHD: •Two new metrics are the Shannon-Wiener Diversity Index (SWDI) and the Cumulative Diversity Index (CDI) proposed to evaluate and track the diversity based on the distribution of heuristic clusters, encouraging a balance between exploration and exploitation. CDI assesses the overall spread and uniformity of heuristics in the search space by analyzing their distribution through a min- imum spanning tree. These metrics address challenges in quantifying diver- sity in function-based search spaces. Both metrics offer practical insights into•A comprehensive analysis of heuristic search spaces has been conducted. This and the performance of various LLM-EPS frameworks across different opti- mization problems. The analysis highlights that while higher diversity gener- tion risk premature convergence. These findings establish the significance of •Introduce a novel framework called HSEvo. HSEvo integrates the Harmony Search (HS) algorithm to optimize diversity and convergence within LLM- EPS frameworks. It refines traditional evolutionary phases such as initial- flash reflection and elitist mutation. Flash reflection uses LLM-driven analy- ses to guide heuristic improvements, while elitist mutation focuses on refining diversity indices and superior objective scores, providing a cost-effective and Chapter 1 offers a thorough review of relevant literature, focusing on the foun- dational concepts of EC and LLMs. It also explores the emerging field of LLM- EPS and emphasizes the significance of diversity in evolutionary algorithms. The chapter concludes by identifying gaps in current research and laying the ground- Chapter 2 introduces two novel diversity metrics SWDI and CDI to measure and analyze the diversity of heuristic populations in LLM-EPS frameworks. Ad- ditionally, it explores the correlation between these diversity metrics and the ob- jective scores of various optimization problems, providing insights into the role of Chapter 3 explains the proposed HSEvo framework, which integrates the HS al- gorithm with EC to balance exploration and exploitation in heuristic search spaces.Chapter 4 presents the experimental setup, benchmark datasets, and results of applying HSEvo to various AHD problems, including Bin Packing Online, Trav- eling Salesman Problem, and Orienteering Problem. After that, compares the per- formance of HSEvo with previous LLM-EPS frameworks and includes an ablation Finally, Conclusions summarizes the key findings of the thesis, highlighting the importance of diversity in LLM-EPS frameworks and the effectiveness of the pro- posed HSEvo framework. It also suggests potential directions for future research inArtificial Intelligence (AI) is evolving at an extraordinary pace, inspiring inno- vative methods that blend disparate computational fields to tackle some of the most demanding challenges. In this rapidly shifting landscape, two particular paradigms stand out for their strengths yet have traditionally advanced on separate tracks: EC and LLMs. EC, inspired by natural evolution, explores populations of candidate solutions to discover high-quality outcomes, whereas LLMs are adept at generat- ing coherent, context-aware outputs ranging from textual prose to executable code. As AI applications become more ambitious, a growing area of research, known as LLM-EPS, seeks to harness the best of both worlds: leveraging the adaptive search prowess of EC and the creative and generative capabilities of LLMs. This emerging fusion offers a novel route for solving challenges in AHD. Initial research indicates that aligning the iterative adjustments of EC with the adaptable generation abilities of LLMs can lead to significant advancements, es- pecially in the creation of heuristic programs, code fragments, or even written so- lutions. Nevertheless, an essential question persists: How can diversity be ensured throughout the evolutionary process? This is particularly important when candi- date solutions are no longer merely simple numeric vectors but are instead diverse forms of code or text. Preserving diversity is paramount to avoid converging too quickly on suboptimal solutions and to maintain an expansive search of the so- lution landscape. The subsequent sections provide an overview of EC principles, discuss the capabilities and constraints of modern LLMs, and survey recent strides in LLM-EPS research, with special attention to how future investigations might Although the convergence of EC and LLMs for optimization and AI is still a relatively new domain, it builds upon two longstanding bodies of research that have more often evolved in isolation. EC traditionally tackles numerical or com- binatorial challenges, while LLMs excel at text-oriented tasks like language trans- lation, content generation, and summarization. Current work in LLM-EPS illus- trates how these fields can be merged: LLMs become generators of candidate solu- tions whether code snippets, pseudo-code, or textual explanations, and evolution- Recent studies show this collaboration. In code generation, studies [7], [16], [17] showcase evolutionary strategies that refine LLM suggested programs for enhancedquality and resilience. In text generation [18], [19] illustrates how evolutionary refinement can produce more precise and diverse linguistic outputs. Together, these works point to tangible gains, including improved error resolution, larger solution tors without explicitly examining how population diversity can be steered or main- tained when solutions take the form of human-readable code or text. This missing piece is critical for keeping evolutionary searches from prematurely collapsing on local optima and for fostering creative, wide-ranging solution sets. Future efforts, therefore, might delve deeper into incorporating diversity metrics to ensure that LLM-driven evolutionary processes not only excel in performance but also yield EC [20] is one computational intelligence model used to mimic the biologi- cal evolution phenomenon. Currently, EC includes four algorithms: Genetic Al- Genetic Programming (GP). GA was proposed by the American scholar Holland in the 1950s in a study of self-adapting control. To study the finite-state machine of AI, the American scholar Fogel proposed EP in the 1960s. At nearly the same time, the German scholars Rechenberg and Schwefel proposed ES to solve numerical op- timization. In the 1990s, based on the GA, the American scholar Koza proposed Although the four algorithms were proposed by different scholars for different purposes, their computing processes are similar and can be described as follows. 3. The initial solutions are selected according to their evaluation results; 4. The selected solutions are conducted by the evolutionary operations and the 5. If the feasible solutions obtained during the above step can meet the require- ments, then the computation will stop. Otherwise, the feasible solutions ob- tained during the above step are taken as the initial solutions, and the compu- Generally, as one global optimization method, EC has the following characteris- tics: (i) The search process begins from one group and not from one point; (ii) onlythe objective function is used in the search process; and (iii) the random method is used in the search process. Therefore, this method has the following advantages: (i) Highly versatile and can be used for different problems; (ii) it can solve problems that are highly nonlinear and nonconvex; and (iii) the model’s plasticity is very The three evolutionary computation algorithms related to the research on this LLM-EPS include GA, EP, and GP. In the next section of the chapter, these algo- As the most widely used EC method, GA has a solid biologic foundation. Its basic principles are from Darwin’s evolution theory and Mendel’s genetic theory must be represented by its genotype. Therefore, the binary encoding method is generally applied, which is one string of numbers 0 and 1. However, for a complicated engineering problem, the real encoding method is always used,2.Crossover operation. This operation is completed to mimic the genetic re- For different encoding methods, the crossover operation is different. For bi- nary encoding, the point crossover is utilized. For real encoding, the discrete crossover and arithmetic crossover are always used. Moreover, this operation 3.Mutation operation. This operation is completed to mimic the gene mutation during the genetic process. Generally, for binary encoding, the simple point mutation is utilized. For real encoding, the uniform mutation and non-uniform mutation are always used. Moreover, this operation is conducted with a little 4.Selection operation. This operation is completed to mimic the environment selection of Darwin’s evolution theory. Therefore, it is also called the repro- duction operation. Generally, roulette wheel selection is widely used. How- ever, stochastic tournament selection is a better operation for complicated en- difference between the maximum fitness value and average fitness value of one group is less than one error ϵ. To avoid infinite iteration, a maximum num- ber of evolutionary generations is also specified. Moreover, there are some parameters to be determined: the number of individuals, crossover probabil- Generally, these parameters are determined based on experience and a test. The main operations of EP are as follow [22], [23]. 1.Mutation operation. This operation is the sole optimization operation; this is the specialty of the EP. Generally, Gauss random mutation is utilized. 2.Testing feasibility of the individual. The fundamental nature of mutation in- volves a type of random alteration to the original individual, which means it cannot ensure that the newly mutated individual remains within the search space; in other words, a non-feasible individual could be generated. The exis- tence of a non-feasible individual can not only make the result incorrect but also make the efficiency low. To overcome this problem, a simple and easy method is proposed [23]. If the new mutated individual is non-feasible, it will be replaced by a randomly created feasible individual. However, this technique3.Selection operation. The selection operation is a stochastic tournament model, where individuals are probabilistically chosen based on their fitness, ensuring 4.Termination condition. This is the same as that of the GA. Moreover, there are some parameters to be determined: the number of individuals, ϵand max- imum number of evolutionary generations. Thus, the number of parameters for EP is less than that for the GA. Generally, these parameters can also be Because GP is proposed based on the basic GA principles, the procedures of two algorithms are nearly the same [24]. The main operations of GP are as follows. 1.Individual encoding. The tree encoding method is applied. In other words, the layering tree structure expression is applied, which is shown in top of 2.Crossover operation. Generally, random pairing and point crossover are uti- lized. This operation is also conducted with a large probability. 3.Mutation operation. Generally, simple point mutation is utilized. This oper- 5.Termination criterion. This is the same as that of GA. Moreover, the param- LLMs are sophisticated AI systems that specialize in understanding, generating, and predicting human-like text. By analyzing extensive datasets, these models have made remarkable strides in various natural language processing tasks. Their capa- The evolution of language models has progressed from statistical methods to •Statistical language models: Early models, such as n-grams, estimated the probability of a word based on its preceding words but struggled with data•Neural network models: The introduction of neural networks allowed for language models. Recurrent neural networks and long short-term memory net- mechanisms to process input data in parallel, enabling the capture of long- LLMs are built upon several key concepts. Fristly, LLMs assign probabilities to sequences of words, modeling the likelihood of a word following a given context. where P(wi|w1, w2, . . . , w i−1)represents the conditional probability of word wi Secondly, LLMs employ deep neural networks with multiple layers and param- eters to learn complex language patterns. The training process involves adjusting these parameters to minimize a loss function, typically the cross-entropy loss: where Nis the number of words in the training corpus. Transformers use self-attention mechanisms to weigh the importance of different words in a sequence, enabling the model to capture contextual relationships effec- where Q(queries), K(keys), and V(values) are matrices derived from the input Training LLMs involves several key steps. First, data collection is essential, as it requires aggregating extensive text data from diverse sources to capture a widerange of language patterns. Once the data is collected, model training begins, uti- through techniques such as gradient descent. Additionally, scaling laws have been empirically observed, which suggest that increasing the model size, training data, and computational resources leads to improved performance. One notable scaling where Cis the training cost in FLOPs, Nis the number of model parameters, D is the size of the training dataset in tokens, and C0is a constant. Despite their capabilities, LLMs face with several limitations. LLMs may gen- erate factually incorrect or contextually misleading outputs colloquially known as “hallucinations.” In addition, their large, black-box structures can complicate in- terpretability, making it challenging to understand precisely why a particular token or code snippet is produced. Moreover, controlled generation, where specific con- straints or style guides must be applied, can be non-trivial to implement. Regard- less, their unprecedented ability to generate complex textual or code-based artifacts has spurred interest in integrating LLMs into evolutionary workflows, particularly···LLM-based EPS: better heuristics are created via prompting LLMsover program codes.evolve···𝑝𝑜𝑝𝑢𝑙𝑎𝑡𝑖𝑜𝑛0exchange sub-parts···𝑝𝑜𝑝𝑢𝑙𝑎𝑡𝑖𝑜𝑛1selectionGenetic Programming: better heuristics are created through genetic operations over trees.···𝑝𝑜𝑝𝑢𝑙𝑎𝑡𝑖𝑜𝑛𝑛 Function setadd  subfor  sum  ⋯⋯Terminal setbins itemstrue false⋯⋯specified byhuman experts definitial_heuristic(item: float, bins: list) -> list:```Define priority with which we assign an item to each bin.:param item: Size of item to be added to the bin.:param bins: List of capacities for each bin.:return: a list showing priority score of each bin.```return[0] * len(bins) with the generative power of LLMs, unlocking new possibilities for AHD. Unlike ates interpretable programs or code snippets, allowing for seamless interaction and FunSearch, short for "search in the function space," represents a groundbreak- ing approach in leveraging LLMs for program synthesis and optimization. Unlike traditional methods, which often face limitations due to hallucinations or verifi- cation challenges. FunSearch innovatively combines an LLM with a systematic ness and quality, and plays a pivotal role in evolving low-performing solutions into tionary methodology, which starts with a user-defined "skeleton" program. This skeleton encapsulates known functional structures and isolates the critical logic for optimization, thereby reducing the search space and improving accuracy. The system employs an island-based model to maintain a diverse pool of candidate pro-grams it has generated so far (retrieved from the programs database), and asked to generate an even better one. The programs proposed by the LLM are automatically executed, and evaluated. The best programs are added to the database, for selection in subsequent cycles. The user can at any point retrieve the highest-scoring programs discovered so far. The system comprises three main components: a programs database, samplers, and evaluators. The programs database stores correct and promising programs, while samplers use LLMs to generate new variations based on prompts constructed from high-scoring programs. Evaluators assess these programs by executing them against a predefined "evaluate" function, which measures the quality of outputs on prompting ensures that high-quality programs guide the evolution process, while clustering within islands helps preserve program diversity. The inclusion of Boltz-mann selection mechanisms and fitness-based sampling further refines the evolu- tionary trajectory, favoring both performance and interpretability. The system’s dis- tributed nature supports scalability, enabling it to tackle large, complex problems across a variety of domains, from mathematical discovery to algorithm design. This architecture not only delivers impressive results but also highlights Fun- Search’s adaptability. By systematically blending the creativity of LLMs with rig- orous evaluation and evolutionary improvements, FunSearch sets a new benchmark for program synthesis, pushing the boundaries of what LLMs can achieve in struc- Algorithm Mutation Prompt EngineeringAlgorithm Crossover Prompt Engineeringdefname (input1, …, inputI) Another noteworthy approach is Evolution of Heuristics (EoH), aimed explicitly at AHD while minimizing manual labor. EoH represents heuristics both as natural language “thoughts” and as executable code, mirroring the way human expertsalgorithm, EoH evolves these heuristics over successive generations with selection, over generations. Selection, crossover, and mutation operations enhanced by LLMs heuristics (E1) or exploring variations based on common ideas (E2); and Modi- fication: focuses on refining heuristics for improved performance (M1), adjusting This method requires fewer queries to LLMs compared to previous techniques such as FunSearch. Additionally, it eliminates the need for custom-crafted features or additional training. The upshot is a more streamlined method that outperforms manually created heuristics across diverse optimization benchmarks. As EoH re- duces the overhead of domain-specific engineering, it underscores the breadth of LLMs with a GP foundation to tackle NP-hard combinatorial optimization tasks. ReEvo employs two types of LLMs: a Generator LLM, which produces initial code snippets or heuristics, and a Reflector LLM, which provides both short-term and(ii) Selection: Parent heuristics are chosen based on their performance. (iii) Short- generation. (iv) Crossover: Generates offspring by combining parent features. (v) generation. (vi) Elitist Mutation: Refines the best heuristic further using accumu- ing frameworks like EoH and FunSearch. Its verbal “gradient,” derived from reflec- tive insights, leads to a gentler fitness landscape and boosts optimization stability, Diversity is a cornerstone of successful evolutionary algorithms, especially in scenarios with multiple objectives [29]. A population rich in diverse solutions maintains broad coverage of the search space, mitigating the dangers of stagna- tion in local optima and supporting adaptability to non-static or convoluted fitness been proposed to safeguard diversity, with Shannon entropy being a popular metric for assessing how “spread out” a set of solutions is [30], [31]. However, embedding these diversity concepts into LLM-EPS raises fresh chal- lenges. Populations are not numeric vectors but heuristic code snippets or descrip- tive text, which calls for alternative ways to quantify “distance” or “difference.” What strategies can be employed to evaluate the semantic gap between two heuris- tic code snippets or two text proposals while effectively capturing their functional differences? These inquiries highlight a pressing need for innovative strategies in Crucially, diversity acts not only as a buffer against premature convergence, but also as a catalyst for creative or unconventional strategies, a vital ingredient in ad- vanced heuristic design. By developing refined diversity metrics and preserving variability throughout the evolutionary process, researchers can unlock a broader range of potential solutions, leading to improved performance and more generaliz- This chapter has charted the emerging partnership between EC and LLMs, show- casing how their combined strengths can transform AHD. In LLM-EPS, LLMs contribute a wellspring of fresh heuristic ideas, and evolutionary methods system-atically sharpen and validate these concepts. Yet, to extract maximum value from this synergy, careful attention must be paid to diversity the balancing force that From traditional GA to newer frameworks like FunSearch, EoH, and ReEvo, the literature consistently highlights the importance of extensive exploration to avoid becoming trapped in suboptimal solutions. Diversity is the crucial compo- nent that integrates a genuinely adaptive search process, facilitating breakthroughs in problem-solving. In the following chapter, advanced methods for incorporatingThis chapter introduces a method for encoding the LLM-EPS population and proposes two metrics for measuring diversity: SWDI and CDI. Following this, a diversity analysis will be conducted on previous LLM-EPS frameworks, including One particular problem in measuring diversity in LLM-EPS is how to encode the population. While each individual in the traditional evolutionary algorithm is encoded as a vector, in LLM-EPS they are usually presented as a string of code snippets/programs. This poses a challenge in applying previous diversity metrics to the population of LLM-EPS frameworks. To address this challenge, an encod- ing method is proposed that involves three steps: (i) removing comments and doc- strings using abstract-syntax tree, (ii) standardizing code snippets into a common coding style (e.g., PEP81), (iii) converting code snippets to vector representations Inspired by ecological studies, [32] provides a quantitative measure of species diversity within a community. In the context of search algorithms, this index aims to quantify the diversity of the population at any given time step based on clusters of individuals. To compute the SWDI for a specific set of individuals within a community, or archive, it is first necessary to determine the probability distribution where Ciis a cluster of individuals and Mrepresents the total number of individu- als across all clusters {C1, . . . , C N}. The SWDI, H(X), is then calculated using the This index serves as a crucial metric in maintaining the balance between explo- ration and exploitation within heuristic search spaces. A higher index score sug- gests a more uniform distribution of individuals across the search space, thus pro-moting exploration. Conversely, a lower index score indicates concentration around specific regions, which may enhance exploitation but also increase the risk of pre- To obtain the SWDI score, at each time step t, all individuals Ri={r1, . . . , r n} generated during that time step are added to an archive. The population encoding method described in Section 2.1 is utilized to derive vector representations V= {v1, . . . , v n}. Following this, cosine similarity is used to compute the similarity To identify clusters in the archive, each embedding vector viis analyzed. Next, viwill be assigned to a cluster Ciif the similarity between viand all members of Ciexceeds a specified threshold α. In mathematical terms, viis assigned to Ciif: If no cluster Ciin {C1, . . . , C N} meets the condition, a new cluster CN+1will be created and viwill be assigned to CN+1. Finally, the SWDI computed score is determined by calculating Equations (2.1) and (2.2) across the identified clusters. While SWDI focuses on diversity of different groups of individuals, the CDI plays a crucial role in understanding the spread and distribution of the whole popu- lation within the search space [33], [34]. In the context of heuristic search, the CDI measures how well a system’s energy, or diversity, is distributed from a centralized Many entropy interpretations have been suggested over the years. The best known are disorder, mixing, chaos, spreading, freedom and information [35]. The first de- scription of entropy was proposed by Boltzmann to describe systems that evolve from ordered to disordered states. Spreading was used by Guggenheim to indicate the diffusion of a energy system from a smaller to a larger volume. Lewis stated that, in a spontaneous expansion gas in an isolated system, information regarding In graph theory, each edge can be assigned a weight representing, for example, the energy required to traverse that edge. A graph with a complex structure and varying edge weights exhibits higher entropy, indicating a more disordered system.Conversely, a simpler structure with uniformly distributed weights corresponds to An Minimum Spanning Tree (MST) of a connected, edge-weighted graph is a subset of the edges that connects all vertices without any cycles and with the min- imum possible total edge weight. Constructing an MST can be seen as a process of transitioning from a high-entropy state (the original graph with all its edges and weights) to a lower-entropy state. This transition reflects the idea of energy distri- bution from a "better located" state to a "more distributed" one, as the MST repre- sents the most efficient way to connect all vertices, minimizing the total "energy" Consider a connected, undirected graph G= (V, E)with a weight function w: E→R+. The entropy Hof the graph can be associated with the distribution of where p(e)is probability-like measure of edge ebased on its weight. The MST, T= (V, ET), is a subgraph where ET⊆Eand the total weightP e∈ETw(e)is minimized. By selecting the subset of edges that connect all ver- tices with the minimal total weight, the MST reduces the system’s complexity and, To calculate the CDI, let’s consider all individuals within an archive, represented by their respective embeddings. By constructing a MST that connects all individu- als within the archive A, where each connection between individuals is calculated using Euclidean distance. This MST provides a structure to assess the diversity of the population. Let direpresent the distance of an edge within the MST, where i∈ {1,2, . . . , #A−1}. The probability distribution of these distances is given by: The cumulative diversity is then calculated using the Shannon entropy:This method enables the collection of the complete diversity present in the search space, offering valuable information about the range of solutions. Higher CDI values indicate a more distributed and diverse population, which is essential An interesting point in Shannon Diversity Index (SDI) theory is that normalizing the SDI with the natural log of richness is equivalent to the evenness valueH′ where H′represents the richness of SDI and Sis the total number of possible categories [36]. The significance of evenness is to demonstrate how the proportions Now, consider normalizing with the two current diversity indices. Firstly, in the case of SWID, if there are Nclusters and the count of individuals in each cluster from C1toCNis uniform, then the evenness will consistently equal 1, regardless of the value of N. This is not the preferred outcome since the number of clusters should be considered as a factor of diversity. Likewise, normalizing the CDI will also overlook the influence of the number of candidate heuristics, which corre- spond to the nodes in the MST. This leads to the proposal not to normalize the 2.4 Exploring the correlation between objective score and diversity measure- To investigate the relationship between the two diversity measurement metrics and the objective score, three experimental runs were performed using ReEvo on three distinct AHD problems, Bin Packing Online (BPO), Traveling Salesman Problem (TSP) with Guided Local Search (GLS) [37], and Orienteering Prob- lem (OP) with Ant Colony Optimization (ACO) solver [38]. Details of the experi- ments can be found in Section 4.1. The diversity evaluation process was conducted with the code embedding model used is CodeT5+2[39], and similarity threshold α= 0.95where α∈[0; 1]. The maximum budget for each run is 450K tokens. The objective scores and the two diversity metrics from these runs on BPO, TSP, and From figure 2.1, there are a few observations that can be drawn. First, the two di- versity measurement metrics have a noticeable correlation with the objective scoresof the problem. When the objective score is converged into a local optima, the framework either tries to focus on the exploration by increasing the diversity of the population, through the increase of SWDI in the first and second runs or tries to focus on the exploitation of the current population, thus decrease the SWDI in the third run. Focusing on exploration can lead to considerable gains in the objective score, whereas concentrating on exploitation may cause the population to remain stuck in local optima for an extended period. However, if the whole population is not diverse enough, as can be seen with the low CDI of the second run, the pop- ulation might not be able to escape the local optima. These observations about the correlation between the objective score and diversity measurement metrics are To analyze the overall diversity of previous LLM-EPS frameworks, experiments were conducted on three distinct LLM-EPS frameworks: FunSearch [13], ReEvo [15], and EoH [14], focusing on three separate AHD problems (BPO, TSP, OP). The details of each experiment are presented in Section 4.1. Note that, as high-lighted in the previous section, SWDI focuses on understanding the diversity of the population during a single run. As such, it may not help quantify the diversity across multiple experiment runs. Fig. 2.4 presents the experiment results. In BPO and TSP, EoH obtained the highest CDI but got the worst objective score. This implies that EoH does not focus enough on exploitation to optimize the population better. In contrast, while ReEvo and FunSearch obtain lower CDI than EoH on BPO and TSP, they achieve better objective performance on all three problems. The experiments conducted on the BPO and TSP problems illustrate the inherent trade-off between diversity and objective performance in LLM-EPS frameworks. However, for the OP problem, it is evident that a high CDI is necessary to achieve a better objective score, which aligns with the findings discussed in In conclusion, this chapter has introduced a comprehensive approach to mea- suring diversity within LLM-EPS frameworks, focusing on SWDI and CDI. The proposed population encoding method addresses the unique challenge of repre- senting code snippets as vectors, enabling the application of these diversity met- rics. Through extensive experiments on various AHD problems, the correlationbetween diversity metrics and objective scores has been explored, revealing the delicate balance between exploration and exploitation in heuristic search spaces. The analysis of previous LLM-EPS frameworks, including FunSearch, ReEvo, and the importance of maintaining an optimal level of diversity to avoid premature con-This chapter introduces a novel LLM-EPS framework called Harmony Search Evolution (HSEvo). HSEvo aims to enhance the diversity of the population while improving optimization performance and relieving the trade-off between these two aspects through an individual tuning process based on harmony search. Addition- ally, the framework aims to minimize costs associated with LLMs by incorporating Individual encoding. In HSEvo, each individual within the evolutionary pro- rect inheritance from earlier LLM-EPS such as EoH and ReEvo. This code snippet, generated by a LLMs, embodies a specific heuristic function tailored to the target any predefined format or structure, which allows for the generation of diverse and complex heuristics. Unlike traditional methods that might use fixed-length vectors or predefined data structures. HSEvo’s code-based approach enables the expres- sion of a wide range of algorithmic strategies, including conditional logic, loops, and function calls, making it highly adaptable to various problem domains. The choice of code as the encoding mechanism also facilitates direct evaluation of the generated heuristics. Each code snippet is directly executable, either within a sandboxed environment or through a standard Python subprocess, simplifying the evaluation process and eliminating the need for intermediate translation steps. This direct executability is particularly advantageous for AHD problems, where the goal Initialization. HSEvo initializes a heuristic population by prompting the gener- ator LLM with task specifications that describe the problem and detail the signature of the heuristic function to be searched. Additionally, to leverage existing knowl- edge, the framework can be initialized with a seed heuristic function and/or exter- nal domain knowledge. To promote diversity, prompts are created with various role instructions (e.g., "You are an expert in the domain of optimization heuristics...", "You are Albert Einstein, developer of relativity theory..."), details instructions in the next Section 3.2.1. This approach aims to enrich the heuristic generation pro-of heur isticx NSelec tionCr osso v erFlash Reflec tion Heur isticDeep analy sisLLMElitist Heur isticReflec tionFlo w dir ec tionSt opThe Last Selection. In HSEvo, the selection of parent pairs for crossover is performed using a random selection strategy. This approach involves choosing two individu- als from current population with equal probability, without any bias towards their fitness or performance. The primary goal of this random selection is to maintain a balance between exploration and exploitation during optimization process. By not favoring the best-performing individuals, HSEvo ensures that search space is not prematurely narrowed, allowing for the discovery of potentially novel and effective heuristics that might be overlooked by more deterministic selection methods. ture convergence, a phenomenon where the population becomes homogeneous too quickly, hindering further exploration. This is particularly important in the context of LLM-EPS, where the initial population might be biased towards certain types of solutions. The random selection helps to maintain diversity within the population, preventing the search from getting stuck in local optima and promoting a more thorough exploration of the heuristic space. This is supported by observations of the SWDI trajectory in Section 2.4, which suggests that random selection can help to maintain a broader search width, thus avoiding premature convergence. reward signals in a verbal format for code generation tasks, as discussed by [40]. Later, [15] also proposed integrating Reflections into LLM-EPS in ReEvo as an spaces. HSEvo argue that reflecting on each pair of heuristic parents individually is not generalized and wastes resources. To address these issues flash reflectionproposed to alternative Reflection method for LLM-EPS. The flash reflection pro- •First, at each time step t, the individuals selected during the selection phase are grouped, and any duplicate individuals are removed. This ensures that the analysis is performed on a unique set of heuristics. The LLM then undertakes a deep analysis of the performance ranking of these individuals. This take in- puts as mixed ranking pairs (i.e., one good performance parent and one worse performance), good ranking pairs (i.e., both good performance), and worse ranking pairs (i.e., both worse performance), then return a comprehensive de- •The second step, flash reflection involves comparing the current analysis at time step twith the previous analysis from time step t−1. This comparison is performed by the LLM, which then generates guide information based on the differences and similarities between the two analyses. This guide information is designed to be used in subsequent stages of the evolutionary process (e.g. crossover and mutation) to further refine the heuristics. By incorporating this temporal aspect, flash reflection allows LLM to learn from its past experi- ences and adapt its code generation strategy over time. This iterative process of analysis and comparison enables LLM to continuously improve its under- standing of the heuristic space and generate increasingly effective solutions. In essence, flash reflection in HSEvo acts as a form of meta-learning, where the LLM not only generates heuristics but also learns how to generate better heuristics over time. By providing a more generalized and resource-efficient approach to re- flection, HSEvo aims to overcome the limitations of previous methods and achieve more effective and efficient heuristic discovery. The textual feedback generated by flash reflection provides a rich source of information that guides the LLM towards Crossover. In this stage, new offspring algorithms are generated by combining elements of the parent algorithms. The goal is to blend successful attributes from two parents via guide result guide information part of flash reflections. Through this step, HSEvo hopes to produce offspring that inherit the strengths of both, which can potentially lead to better-performing heuristics. The prompt includes task specifications, a pair of parent heuristics, guide information part of flash re- Elitist mutation. HSEvo employs an elitist mutation strategy, focusing its muta-tion efforts on the "elite" individual, which represents the best-performing heuris- tic found so far. This approach leverages the generator LLM to mutate this elite tions. The mutation process is not random; instead, it is guided by the accumu- lated knowledge and feedback from the flash reflection, aiming to enhance the elite heuristic’s performance. Each mutation prompt includes detailed task specifi- cations, the code of the elite heuristic, the deep analysis from the flash reflections, and specific generation instructions, providing the LLM with the necessary context This elitist mutation strategy is designed to ensure continuous improvement in performance while preserving the quality of the top solutions. By focusing on the best-performing heuristic, HSEvo aims to refine and enhance its capabilities, rather than randomly exploring the search space. The incorporation of insights from the flash reflections ensures that the mutation process is not arbitrary but rather di- rected towards making meaningful improvements based on the accumulated knowl- edge of the evolutionary process. This approach allows HSEvo to effectively exploit the best solutions found so far, while still allowing for exploration through other Harmony Search. From the analysis in Section 2.4 and 2.5. The hypothesis is that if the population becomes too diverse, individual heuristics within it are more likely to be unoptimized, which can negatively impact the overall optimiza- tion process. To mitigate this, HSEvo employs HS to fine-tune the parameters (e.g., thresholds, weights, etc.) of the best-performing individuals in the population. The •First, once an individual is selected, the LLM is tasked with extracting param- eters from the best individual (i.e., code snippets, programs) of the population •Following the parameter extraction, HSEvo employs the HS algorithm, a meta- heuristic optimization technique inspired by the improvisation process of mu- sicians. The HS algorithm iteratively adjusts the extracted parameters within their defined ranges, aiming to find a combination that maximizes the heuris- tic’s performance. This process involves creating a "harmony memory," which stores a set of parameter combinations, and iteratively generating new har- monies by adjusting existing ones or creating new ones randomly. The per- formance of each harmony is evaluated, and the harmony memory is updatedpredefined number of iterations, allowing the HS algorithm to explore param- •After the parameter optimization is complete, the optimized individual is marked to prevent it from being optimized again in future time steps. This ensures that the HS algorithm is not repeatedly applied to the same individuals, allowing the framework to focus on optimizing other promising heuristics. The opti- mized individual is then added back to the population, contributing to the overall diversity and performance of the population. This integration of HS into the evolutionary process allows HSEvo to fine-tune the best-performing heuristics, enhancing their performance and mitigating the negative effects of To sum up, the HS component in HSEvo is a critical mechanism for enhancing the performance of the best-performing heuristics by fine-tuning their parameters. This process involves using the LLMs to extract parameters and define their ranges, followed by the application of the HS algorithm to optimize these parameters. The optimized individuals are then added back to the population, contributing to the overall performance and diversity of the population. This strategic integration of HS allows HSEvo to effectively balance exploration and exploitation, leading to An overview of the flow of HSEvo is also described through pseudo-algorithm 1, where function_evals <max_fe serves as the stopping condition. Here, function_evals is a counter variable tracking the number of evaluated indi- viduals, and max_fe is the maximum number of individuals HSEvo will evaluate.Crossover : Generate and evaluate new individuals by crossing parents Mutation : Generate and evaluate new individuals by mutating the bestThis section synthesizes the prompts used in the HSEvo framework. The HSEvo prompt system includes four stages: initialization, population, flash reflection, crossover, {role_init} helping to design heuristics that can effectively solve optimiza- Your response outputs Python code and nothing else. Format your code as a {role_init} Your task is to write a {function_name} function for {prob- In this work, the variable role_init is utilized to assign various roles to the LLMs, allowing the creation of distinct personas [41] that can provide different perspectives and promote population diversity. The roles are chosen in a round- •You are an expert in the domain of optimization heuristics, correspond to the heuristic function name, the specific problem description, and the function description, respectively, which will be detailed in the section 3.2.7.Refer to the format of a trivial design above. Be very creative and give ‘{func- tion_name}_v2’. Output code only and enclose your code with Python code Here, task_description refer to Prompt 2, seed_ function is a triv- You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.Below is a list of design heuristics ranked from best to worst. - Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the - The response in Markdown style and nothing else has the following struc- + Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analy- Example: “Comparing (best) vs (worst), I see ...; (second best) vs (second worst) ...; Comparing (1st) vs (2nd), I see ...; (3rd) vs (4th) ...; Comparing + Self-reflect to extract useful experience for design better heuristics and fill I’m going to tip $999K for a better heuristics! Let’s think step by step. list_ranked_heuristics is a list of heuristic functions obtained by group- ing parent pairs from selection, removing duplicates, and ranking based on theYour task is to redefine ‘Current self-reflection’ paying attention to avoid all things in ‘Ineffective self-reflection’ in order to come up with ideas to design Response ( <100 words) should have 4 bullet points: Keywords, Advice, I’m going to tip $999K for a better heuristics! Let’s think step by step. current_reflection is the output of the flash reflection step 1 from the current generation. good_reflection is the output of the flash reflection step 2 from previous generations where a new heuristic was discovered. Conversely, bad_reflection is the output of the flash reflection step 2 from previous gen- Your task is to write an improved function ‘{func_name}_v2‘ by COMBIN- ING elements of two above heuristics base Analyze & experience. Output the code within a Python code block: “‘python ... “‘, has comment and docstring ( <50 words) to description key idea of heuristics design. I’m going to tip $999K for a better heuristics! Let’s think step by step. nature_worse ,code_worse are the function signatures and code of the betterand worse parent individuals, respectively. flash_refection is the output of Now, think outside the box write a mutated function ‘{func_name}_v2‘ bet- ter than current version. You can using some hints if need: Output code only and enclose your code with Python code block: “‘python You are an expert in code review. Your task extract all threshold, weight or hardcode variable of the function make it become default parameters. Now extract all threshold, weight or hardcode variable of the function make it become default parameters and give me a ’parameter_ranges’ dictionary representation. Key of dict is name of variable. Value of key is a tuple in Python MUST include 2 float elements, first element is begin value, second - Output code only and enclose your code with Python code block: - Output ’parameter_ranges’ dictionary only and enclose your code with elitist_code is the snippet/string code of the elitist individual in the cur-BPO Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received. TSP Solving Traveling Salesman Problem (TSP) via guided local search. TSP requires finding the shortest path that visits all given nodes and BPO The priority function takes as input an item and an array of returns a priority score for each bin. The bin with the highest priority TSP The ‘update_edge_distance’ function takes as input a matrix of edge distances, a locally optimized tour, and a matrix indicating the num- ber of times each edge has been used. It returns an updated matrix of edge distances that incorporates the effects of the local optimization and edge usage. The returned matrix has the same shape as the in- put ‘edge_distance’ matrix, with the distances adjusted based on the OP The ‘heuristics’ function takes as input a vector of node attributes (shape: n), a matrix of edge attributes (shape: n by n), and a constraint imposed on the sum of edge attributes. A special node is indexed by 0. ‘heuristics’ returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape asThe best heuristics generated by HSEvo for all problem settings are presented To summarize, this chapter presented HSEvo, an innovative LLM-EPS frame- work that transforms heuristic optimization by integrating population diversity and performance through advanced techniques such as flash reflection and harmony search. HSEvo utilizes code-based individual encoding to enhance its adaptability in addressing a wide range of intricate optimization challenges. Key components, such as the random selection mechanism, flash reflection, and elitist mutation, en- sure thorough exploration of the heuristic space while preventing premature con- vergence. By incorporating the HS algorithm for fine-tuning parameters, HSEvo further optimizes its best-performing heuristics, achieving a synergy between ex- ploration and exploitation. Along with this, detailed descriptions of prompts, pseu- docode, and the best heuristics on various benchmarks using HSEvo are provided2 """Returns priority with which I want to add item to each bin 5 item: Size of item to be added to the bin. 9 Array of same size as bins_remain_cap with priority scor11 # Avoid division by zero by adjusting remaining capacities8 # Comprehensive scoring integrating all metrics for a robust 13 prioritized_scores = (scores - np. min(scores)) / (max_score - 15 # Invert scores for selecting the highest priority bi16 # Update the distance with adjustment and ensure non4 # Layered logic for combined node influence using non 15 penalty_factor = 1 / (1 + penalty_base ** 2) #To unveil the potential of the HSEvo framework. This chapter presents a series of experiments designed to benchmark its performance against existing LLM-EPS methodologies. By tackling a range of optimization challenges, including BPO, TSP, and OP, this thesis examines how effective HSEvo is in improving heuristic Benchmarks: To evaluate the diversity and objective scores of HSEvo in com- parison to earlier LLM-EPS frameworks, the same benchmarks outlined in Section 2.4 were employed to conduct experiments on three distinct AHD problems: BPO •BPO: packing items into bins of fixed capacity in real-time without prior knowledge of future items. In this benchmark, LLM-EPS frameworks need •TSP: find the shortest possible route that visits each city exactly once and returns to the starting point. In this setting, LLM-EPS frameworks need to design heuristics to enhance the perturbation phase for the GLS solver. •OP: find the most efficient path through a set of locations, maximizing the total score collected within a limited time or distance. This setting requires LLM-EPS frameworks to design herustics used by the ACO solver. EPS frameworks and the evaluations of heuristics were conducted using a single Definition. The objective of this problem is to effectively assign a diverse col- lection of items, each with its own specific size and weight, into the smallest pos- sible number of containers, each having a fixed capacity denoted by C. This chal- lenge is particularly focused on the online packing scenario, where items are in- troduced one at a time and must be packed immediately upon arrival without prior knowledge of future items. This contrasts with the offline scenario, where all itemsWeibull instances of size 5k with a capacity of C= 100 . The objective score is nof five instances, where lbrepresents the lower bound of the optimal number of bins computed [44] and nis the number of bins used to pack all Solver. LLM-EPS is used to design heuristic functions that are able to solve this Definition. The TSP is a well-known combinatorial optimization issue in the field of computer science and operations research. It involves a scenario where a salesman must visit a given set of cities, with the objective of finding the shortest possible route that allows him to visit each city exactly once before returning to his starting point, which is often referred to as the origin city. Dataset generation. Following [37], this experiment generates a set of 64 TSP instances with 100 nodes (TSP100). The node locations in these instances are ran- domly sampled from [0,1]2. This means that the nodes are positioned within a square area bounded by (0, 0) and (1, 1). The average gap from the optimal solu- tion, generated by Concorde [45], is used as the objective score. Solver. GLS was used as the solver for this benchmark. GLS explores the so- lution space using local search operations guided by heuristics. The idea behind using this solver is to explore the potential of search penalty heuristics for LLM-EPS. In this experiment, the traditional GLS algorithm was modified by including perturbation phases [46], where edges with higher heuristic values are given prior- Definition. In the OP at hand, the primary aim is to maximize the cumulative score achieved by visiting a series of designated nodes. This objective is subject to the condition that the total length of the tour does not exceed a specified maximum limit. The nodes represent key points of interest, each contributing a certain score, and the challenge lies in strategically planning the route to optimize the overall score while adhering to the constraints of the maximum allowable tour length. Dataset generation. Following the process of DeepACO [38] during the gen- eration of this synthetic dataset. In each problem instance, uniform distribution is used to sample 50 nodes (OP50), including the depot node, from the unit interval [0,1]2. This means that the nodes are positioned within a square area bounded by (0, 0) and (1, 1). Especially, a challenging prize distribution [47] also adopt: where d0irepresents the distance between the depot and node i, and the maxi- Solver. For this problem, ACO is used as a solver for this benchmark. ACO is an evolutionary algorithm that integrates solution sampling with pheromone trail updates. It employs stochastic solution sampling, which is biased towards more promising solution spaces based on heuristics. The population size is set at 20 forCDI (↑) Obj. (↓) CDI (↑) Obj. (↓) CDI (↑) Obj. (↓) ble, it was observed that while HSEvo still hasn’t obtained better CDI than EoH, HSEvo is able to achieve the best objective score on all tasks. On BPO, HSEvo outperforms both FunSearch and ReEvo by a huge margin. This highlights the im- portance of my findings from the analysis in Section 2.5, where it is crucial to To investigate the impact of my framework on the diversity of the population, 1Extending FunSearch to solve the OP problem with an ACO solver caused conflicts that couldscore of HSEvo through different runs on BPO, TSP, OP problems, respectively. It can draw a similar observation with findings in Section 2.4, where with a high SWDI and CDI, the objective score can be optimized significantly. One thing to 2.1, both have the SWDI decreasing over time. However, in HSEvo, the SWDI is at around 3.0 when the objective score improves significantly, then only decreases marginally after that. In ReEvo, the objective score improves when SWDI is at around 3.0 and 2.7, and the magnitude of the improvement is not as large as HSEvo, which implies the importance of diversity in the optimization of the problem and To gain a better understanding of novel framework HSEvo, The experiment ex- tends to conduct ablation studies on proposed components, the harmony search, As harmony search is a vital component in HSEvo framework, it will not be eliminated from HSEvo; instead, an experimental setup will incorporate harmony search into the ReEvo framework. The results of the experiment concerning the OP both ReEvo and the variant of ReEvo that includes harmony search, in terms of bothobjective score and CDI. Here, notice that harmony search can only marginally improve ReEvo. This can be explained that ReEvo does not have any mechanism to promote diversity, therefore it does not benefit from the advantage of harmony An additional experiment was conducted in which the reflection component used in ReEvo was replaced with a flash reflection component. Since flash re- flection is more cost-effective than the original reflection, reducing the number of tokens utilized for optimization to 150K tokens serves as a method to validate this. a smaller number of the timestep, ReEvo with flash reflection mechanism can out- on CDI. However, when running with a larger number of tokens, ReEvo with flash reflection cannot improve on both objective score and CDI, while HSEvo improves both metrics to 5.67 and -14.62, respectively. This implies that without a diversity- promoting mechanism, flash reflection is not enough to improve the optimization scores across BPO, TSP, and OP benchmarks while maintaining strong diversity indices. Additionally, the ablation studies highlight the critical roles of harmony search and flash reflection in enhancing HSEvo’s performance. These results val- idate HSEvo’s effectiveness and its potential to advance LLM-EPS frameworks in To sum up, experimental findings presented in this chapter underscore the sig- nificant advantages of the HSEvo in addressing COPs. By systematically bench- AHD problems. The results reveal HSEvo’s ability to consistently achieve superior objective scores while maintaining robust diversity indices. These outcomes val- idate the hypothesis that diversity-driven optimization strategies, as embodied inFurthermore, the ablation studies emphasize the pivotal contributions of the har- mony search and flash reflection components in enhancing performance of HSEvo. While harmony search bolsters the optimization process by promoting diversity, flash reflection offers a cost-effective yet impactful mechanism for refining solu- tions. Together, these components elevate HSEvo’s capabilities, establishing it as a robust framework for optimizing heuristic design. The results of this chapter lay a solid foundation for future exploration of HSEvo’s potential in broader optimiza- tion domains, reinforcing its role as a transformative advancement in LLM-EPS.This thesis provides the pivotal role of population diversity in enhancing the performance of LLM-EPS for AHD. Two novel diversity measurement metrics, the SWDI and CDI, were introduced to analyze and monitor population diversity a common oversight: the failure to adequately balance population diversity with In response to existing boundaries, the introduction of HSEvo a new state-of- the-art LLM-EPS framework marks a notable advancement in this field. HSEvo leverages a diversity-driven harmony search and genetic algorithm to maintain an optimal balance between exploration and exploitation within heuristic search spaces. By incorporating mechanisms like flash reflection and elitist mutation, HSEvo achieves high diversity indices and competitive objective scores across var- ious optimization problems, such as the BPO, TSP, and OP. Ablation studies also confirmed the effectiveness of HSEvo’s components, underscoring its capability to This work also establishes a new standard in the domain of LLM-EPS, high- lighting the important relationship between diversity and optimization. The results not only fill the voids in current frameworks but also create opportunities for fu- ture studies to expand on the suggested metrics and methods. By enhancing the comprehension of heuristic search spaces and promoting automated design tech- niques. This research offers significant insights and resources for tackling intricate Furthermore, creating effective stopping criteria for the search process is a promis- ing area for research. By integrating diversity metrics and evaluating objective scores. It may be possible to reduce the computational costs inherent in LLM op- LLM-EPS applications to AHD. Currently, experiments are only confined to GPT- 4o-mini model due to the high costs associated with utilizing closed-source APIs. Expanding evaluations to incorporate additional models such as Qwen, LLaMA, Gemini, and others could significantly reinforce the robustness of the findings of this thesis. Another area of concern is parameter sensitivity. The harmony search that influence performance. This is a gap that persists across the LLM-EPS, in-cluding in prior works like FunSearch, EoH, and ReEvo. Addressing parameter Parts of the work presented in this thesis have been accepted at the 39th Annual 1.Pham Vu Tuan Dat , Long Doan, and Huynh Thi Thanh Binh, "HSEvo: El- and Genetic Algorithm Using LLMs", The 39th Annual AAAI Conferenc