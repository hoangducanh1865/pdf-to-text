
For me, finishing this project signifies not just a milestone but the start of an
exciting new journey. While conducting this thesis, I have often felt lucky. This
work would not have been possible without the invaluable guidance and mentorship
of Associate Professor Huỳnh Thị Thanh Bình. I am also grateful to my mentor,
Long Đoàn, for our weekly discussions and his thoughtful feedback. I would like
to extend my thanks to the members of the MSO Lab, who offered their time to
enrich this work in countless ways. Last but not least, I want to express my heartfelt
appreciation to my beloved family and best friends, who have always been my
source of emotional support and encouragement during the most challenging times.
Thank you all from the bottom of my heart!
Automatic Heuristic Design (AHD) is an active research area due to its utility in
solving complex search and NP-hard combinatorial optimization problems in the
real world. Recent advances in Large Language Models (LLMs) introduce new possibilities by coupling LLMs with Evolutionary Computation (EC) to automatically
generate heuristics, known as LLM-based Evolutionary Program Search (LLM-
EPS). While previous LLM-EPS studies obtained great performance on various
tasks, there is still a gap in understanding the properties of heuristic search spaces
and achieving a balance between exploration and exploitation, which is a critical
factor in large heuristic search spaces. This research addresses this gap by proposing two diversity measurement metrics and performing an analysis of previous
LLM-EPS approaches, including FunSearch, EoH, and ReEvo. Results on blackbox AHD problems reveal that while EoH demonstrates higher diversity than FunSearch and ReEvo, its objective score is unstable. Conversely, ReEvo’s reflection
mechanism yields good objective scores but fails to optimize diversity effectively.
In light of these findings, HSEvo was introduced as an adaptive LLM-EPS framework that strikes a balance between diversity and convergence through the use of
a harmony search algorithm. Experimentation demonstrated that HSEvo achieved
high diversity indices and good objective scores while remaining cost-effective.
These results underscore the importance of balancing exploration and exploitation
and understanding heuristic search spaces in designing frameworks in LLM-EPS.
NP-hard Combinatorial Optimization Problems (COPs) frequently arise in realworld scenarios. Examples include groups of problems such as routing and logistics, scheduling and planning, network design, and others. To address these problems, heuristic methods are among the most commonly employed approaches due
to their superior efficiency in terms of time and resources needed to find solutions
that approximate the optimal. Over the past few decades, significant efforts have
been dedicated to designing effective search methods, resulting in techniques such
as simulated annealing, tabu search, and iterated local search, among many others. These manually crafted methods have been successfully applied in numerous
However, due to the diverse nature of practical problems, each application comes
with its constraints and objectives, often requiring customization or the selection of
specific search methods tailored to the situation at hand. Manually creating, tuning,
and configuring search methods for particular problems is not only labor-intensive
but also demands deep expertise. This represents a bottleneck in many application
domains. Consequently, AHD has emerged as a promising solution, aiming to automate the selection, tuning, or construction of efficient search methods for specific
The goal of AHD is to address the difficulties involved in the creation of manual
heuristics by utilizing computational methods to automatically generate or enhance
heuristics. Among the various approaches within AHD, both Hyper-Heuristics
(HHs) [1], [2] and Neural Combinatorial Optimization (NCO) [3]–[5] have gained
substantial attention for their potential to address the limitations of traditional
HHs operate within predefined heuristic spaces curated by human experts, selecting or generating heuristics from these established sets. While HHs have demonstrated success, their performance is inherently limited by the scope and quality of
the predefined heuristic space, often restricting the discovery of novel or more efOn the other hand, NCO employs neural networks to learn patterns and predict
solutions for COPs. By leveraging data-driven models, NCO seeks to generalize
across problem instances, offering a more dynamic approach to heuristic development. Despite its promise, NCO faces significant challenges. Effective generalization across diverse problem instances requires robust inductive biases [6], which
are difficult to design and implement. Additionally, the interpretability and explainability of the learned models remain pressing concerns [5], making it challenging
to validate or trust the solutions produced. These limitations highlight the need for
NCO frameworks to evolve further, incorporating mechanisms that enhance their
The constraints associated with HHS and NCO underscore the need for more
flexible, adaptive, and innovative methods in AHD. Addressing these limitations
can unlock the full potential of heuristic design, enabling the development of systems that not only adapt to complex and evolving problem landscapes but also expand the horizons of heuristic discovery beyond current boundaries. Such advancements will play a crucial role in shaping the future of optimization and decisionmaking in both academic research and practical applications.
Recently, the rise of LLMs has opened up new possibilities for AHD. It is believed that LLMs [7], [8] could be a powerful tool for generating new ideas and
heuristics. However, standalone LLMs with prompt engineering can be insufficient
for producing novel and useful ideas beyond existing knowledge [9]. Some attempts
have been made to couple LLMs with EC to automatically generate heuristics,
known as LLM-EPS [10]–[12]. Initial works such as FunSearch [13] and subsequent developments like Evolution of Heuristic (know as EoH) [14] and Reflective
Evolution (know as ReEvo) [15] have demonstrated significant improvements over
previous approaches, generating quality heuristics that often surpass current methods. Even so, ReEvo yields state-of-the-art and competitive results compared with
evolutionary algorithms, neural-enhanced meta-heuristics, and neural solvers.
A key difference between LLM-EPS and classic AHD lies in the search spaces
of heuristics. Classic AHD typically operates within well-defined mathematical
spaces such as Rn, whereas LLM-EPS involves searching within the space of functions, where each function represents a heuristic as a program. This functional
search space introduces unique challenges and opportunities, necessitating a deeper
understanding of the properties and characteristics of heuristics within this domain.
Despite the advancements made by LLM-EPS frameworks, there remains a significant gap in foundational theories and principles that underpin AHD within these
expansive and complex search spaces.
This research focuses on the intersection of EC and LLMs, exploring their potential synergy in AHD. History has demonstrated that evolutionary algorithms
excel in optimization tasks ranging from combinatorial problems and image e
hancement to industrial scheduling thanks to their population-based search mechanisms that balance exploration and exploitation. Simultaneously, LLMs such as
ChatGPT, Claude, and Gemini have achieved remarkable advancements in natural language processing and code generation, showcasing their ability to produce
coherent and contextually relevant outputs.
The primary objective of this research is to explore how these two fields can
collaborate to design heuristics for solving COPs. While current research has made
progress in metrics such as solution quality and runtime efficiency, maintaining
and leveraging diversity among candidate solutions remains underexplored. This
oversight may limit the range of potential solutions, ultimately hindering opportuThus, the scope of this study aims at three objectives. First, "tools" should be developed to measure the diversity level promoted by various methods within LLM-
EPS frameworks. This quantification will enable straightforward comparison and
selection of suitable diversity-enhancing methods, while also serving as a foundation for assessing the balance between exploration and exploitation. Second, the
study seeks to analyze how varying levels of diversity impact the search quality of
different LLM-EPS frameworks. Finally, this work aims to design and evaluate a
new LLM-EPS framework that balances exploration and exploitation, incorporating insights drawn from the second objective through the tools developed in the
first objective. This framework will be tested on AHD problems, with results compared against existing LLM-EPS methods. This comparison will help evaluate the
proposed framework and provide evidence of its potential contributions based on
This thesis makes three key contributions to the field of AHD:
•Two new metrics are the Shannon-Wiener Diversity Index (SWDI) and the
Cumulative Diversity Index (CDI) proposed to evaluate and track the diversity
of heuristic populations in LLM-EPS frameworks. SWDI evaluates diversity
based on the distribution of heuristic clusters, encouraging a balance between
exploration and exploitation. CDI assesses the overall spread and uniformity
of heuristics in the search space by analyzing their distribution through a minimum spanning tree. These metrics address challenges in quantifying diversity in function-based search spaces. Both metrics offer practical insights into
population dynamics, revealing how diverse heuristics contribute to escaping
local optima and improving optimization outcomes.
•A comprehensive analysis of heuristic search spaces has been conducted. This
investigation explores the relationship between the proposed diversity metrics
and the performance of various LLM-EPS frameworks across different optimization problems. The analysis highlights that while higher diversity generally supports exploration, excessive diversity without proper optimization can
hinder performance. Conversely, frameworks that overly prioritize exploitation risk premature convergence. These findings establish the significance of
balanced diversity in heuristic design.
•Introduce a novel framework called HSEvo. HSEvo integrates the Harmony
Search (HS) algorithm to optimize diversity and convergence within LLM-
EPS frameworks. It refines traditional evolutionary phases such as initialization, crossover, and mutation by incorporating advanced techniques like
flash reflection and elitist mutation. Flash reflection uses LLM-driven analyses to guide heuristic improvements, while elitist mutation focuses on refining
top-performing heuristics. The HS component tunes heuristic parameters to
enhance population quality while maintaining diversity. Experimental results
demonstrate that HSEvo outperforms existing frameworks by achieving high
diversity indices and superior objective scores, providing a cost-effective and
The remainder of this thesis is organized as follows.
Chapter 1 offers a thorough review of relevant literature, focusing on the foundational concepts of EC and LLMs. It also explores the emerging field of LLM-
EPS and emphasizes the significance of diversity in evolutionary algorithms. The
chapter concludes by identifying gaps in current research and laying the groundwork for the proposed work.
Chapter 2 introduces two novel diversity metrics SWDI and CDI to measure
and analyze the diversity of heuristic populations in LLM-EPS frameworks. Additionally, it explores the correlation between these diversity metrics and the objective scores of various optimization problems, providing insights into the role of
diversity in heuristic search spaces.
Chapter 3 explains the proposed HSEvo framework, which integrates the HS algorithm with EC to balance exploration and exploitation in heuristic search spaces.
It provides detailed descriptions of HSEvo, including population initialization,
flash reflection, crossover, elitist mutation, and harmony search.
Chapter 4 presents the experimental setup, benchmark datasets, and results of
applying HSEvo to various AHD problems, including Bin Packing Online, Traveling Salesman Problem, and Orienteering Problem. After that, compares the performance of HSEvo with previous LLM-EPS frameworks and includes an ablation
study to evaluate the effectiveness of the proposed components.
Finally, Conclusions summarizes the key findings of the thesis, highlighting the
importance of diversity in LLM-EPS frameworks and the effectiveness of the proposed HSEvo framework. It also suggests potential directions for future research in
Artificial Intelligence (AI) is evolving at an extraordinary pace, inspiring innovative methods that blend disparate computational fields to tackle some of the most
demanding challenges. In this rapidly shifting landscape, two particular paradigms
stand out for their strengths yet have traditionally advanced on separate tracks: EC
and LLMs. EC, inspired by natural evolution, explores populations of candidate
solutions to discover high-quality outcomes, whereas LLMs are adept at generating coherent, context-aware outputs ranging from textual prose to executable code.
As AI applications become more ambitious, a growing area of research, known as
LLM-EPS, seeks to harness the best of both worlds: leveraging the adaptive search
prowess of EC and the creative and generative capabilities of LLMs. This emerging
fusion offers a novel route for solving challenges in AHD.
Initial research indicates that aligning the iterative adjustments of EC with the
adaptable generation abilities of LLMs can lead to significant advancements, especially in the creation of heuristic programs, code fragments, or even written solutions. Nevertheless, an essential question persists: How can diversity be ensured
throughout the evolutionary process? This is particularly important when candidate solutions are no longer merely simple numeric vectors but are instead diverse
forms of code or text. Preserving diversity is paramount to avoid converging too
quickly on suboptimal solutions and to maintain an expansive search of the solution landscape. The subsequent sections provide an overview of EC principles,
discuss the capabilities and constraints of modern LLMs, and survey recent strides
in LLM-EPS research, with special attention to how future investigations might
harness diversity to unlock even more powerful solutions.
Although the convergence of EC and LLMs for optimization and AI is still a
relatively new domain, it builds upon two longstanding bodies of research that
have more often evolved in isolation. EC traditionally tackles numerical or combinatorial challenges, while LLMs excel at text-oriented tasks like language translation, content generation, and summarization. Current work in LLM-EPS illustrates how these fields can be merged: LLMs become generators of candidate solutions whether code snippets, pseudo-code, or textual explanations, and evolutionary frameworks guide these outputs toward continuous improvement.
Recent studies show this collaboration. In code generation, studies [7], [16], [17]
showcase evolutionary strategies that refine LLM suggested programs for enhanced
quality and resilience. In text generation [18], [19] illustrates how evolutionary
refinement can produce more precise and diverse linguistic outputs. Together, these
works point to tangible gains, including improved error resolution, larger solution
Nevertheless, most existing research [13]–[15] focuses on performance indicators without explicitly examining how population diversity can be steered or maintained when solutions take the form of human-readable code or text. This missing
piece is critical for keeping evolutionary searches from prematurely collapsing on
local optima and for fostering creative, wide-ranging solution sets. Future efforts,
therefore, might delve deeper into incorporating diversity metrics to ensure that
LLM-driven evolutionary processes not only excel in performance but also yield
robust and flexible results across numerous domains.
EC [20] is one computational intelligence model used to mimic the biological evolution phenomenon. Currently, EC includes four algorithms: Genetic Algorithm (GA), Evolutionary Programming (EP), Evolution Strategies (ES), and
Genetic Programming (GP). GA was proposed by the American scholar Holland
in the 1950s in a study of self-adapting control. To study the finite-state machine of
AI, the American scholar Fogel proposed EP in the 1960s. At nearly the same time,
the German scholars Rechenberg and Schwefel proposed ES to solve numerical optimization. In the 1990s, based on the GA, the American scholar Koza proposed
GP to study the automatic design of computer programs.
Although the four algorithms were proposed by different scholars for different
purposes, their computing processes are similar and can be described as follows.
1. One group of initial feasible solutions are created;
2. The properties of the initial solutions are evaluated;
3. The initial solutions are selected according to their evaluation results;
4. The selected solutions are conducted by the evolutionary operations and the
next generation of feasible solutions can be obtained;
5. If the feasible solutions obtained during the above step can meet the requirements, then the computation will stop. Otherwise, the feasible solutions obtained during the above step are taken as the initial solutions, and the computation process returns to step 2.
Generally, as one global optimization method, EC has the following characteristics: (i) The search process begins from one group and not from one point; (ii) only
the objective function is used in the search process; and (iii) the random method is
used in the search process. Therefore, this method has the following advantages: (i)
Highly versatile and can be used for different problems; (ii) it can solve problems
that are highly nonlinear and nonconvex; and (iii) the model’s plasticity is very
high and can be deserialized very easily.
The three evolutionary computation algorithms related to the research on this
LLM-EPS include GA, EP, and GP. In the next section of the chapter, these algoAs the most widely used EC method, GA has a solid biologic foundation. Its
basic principles are from Darwin’s evolution theory and Mendel’s genetic theory
[21]. The basic GA flow chart is shown in Figure 1.1. The main operations of GA
1.Individual encoding. According to Mendel’s genetic theory, the individual
must be represented by its genotype. Therefore, the binary encoding method
is generally applied, which is one string of numbers 0 and 1. However, for a
complicated engineering problem, the real encoding method is always used,
which is one string of a real number.
2.Crossover operation. This operation is completed to mimic the genetic recombination during biological mating. Generally, random pairing is applied.
For different encoding methods, the crossover operation is different. For binary encoding, the point crossover is utilized. For real encoding, the discrete
crossover and arithmetic crossover are always used. Moreover, this operation
is conducted with a large probability.
3.Mutation operation. This operation is completed to mimic the gene mutation
during the genetic process. Generally, for binary encoding, the simple point
mutation is utilized. For real encoding, the uniform mutation and non-uniform
mutation are always used. Moreover, this operation is conducted with a little
4.Selection operation. This operation is completed to mimic the environment
selection of Darwin’s evolution theory. Therefore, it is also called the reproduction operation. Generally, roulette wheel selection is widely used. However, stochastic tournament selection is a better operation for complicated en5.Termination criterion. Generally, the termination criterion specifies that the
difference between the maximum fitness value and average fitness value of
one group is less than one error ϵ. To avoid infinite iteration, a maximum number of evolutionary generations is also specified. Moreover, there are some
parameters to be determined: the number of individuals, crossover probability, mutation probability, ϵand maximum number of evolutionary generations.
Generally, these parameters are determined based on experience and a test.
The main operations of EP are as follow [22], [23].
1.Mutation operation. This operation is the sole optimization operation; this is
the specialty of the EP. Generally, Gauss random mutation is utilized.
2.Testing feasibility of the individual. The fundamental nature of mutation involves a type of random alteration to the original individual, which means
it cannot ensure that the newly mutated individual remains within the search
space; in other words, a non-feasible individual could be generated. The existence of a non-feasible individual can not only make the result incorrect but
also make the efficiency low. To overcome this problem, a simple and easy
method is proposed [23]. If the new mutated individual is non-feasible, it will
be replaced by a randomly created feasible individual. However, this technique
may have a very low efficiency for certain problems.
3.Selection operation. The selection operation is a stochastic tournament model,
where individuals are probabilistically chosen based on their fitness, ensuring
a balance between exploration and exploitation.
4.Termination condition. This is the same as that of the GA. Moreover, there
are some parameters to be determined: the number of individuals, ϵand maximum number of evolutionary generations. Thus, the number of parameters
for EP is less than that for the GA. Generally, these parameters can also be
determined based on experience and a test.
Because GP is proposed based on the basic GA principles, the procedures of
two algorithms are nearly the same [24]. The main operations of GP are as follows.
1.Individual encoding. The tree encoding method is applied. In other words,
the layering tree structure expression is applied, which is shown in top of
2.Crossover operation. Generally, random pairing and point crossover are utilized. This operation is also conducted with a large probability.
3.Mutation operation. Generally, simple point mutation is utilized. This operation is conducted with a little probability.
4.Selection operation. Generally, roulette wheel selection is used.
5.Termination criterion. This is the same as that of GA. Moreover, the parameters are the same as those of GA.
LLMs are sophisticated AI systems that specialize in understanding, generating,
and predicting human-like text. By analyzing extensive datasets, these models have
made remarkable strides in various natural language processing tasks. Their capabilities include text generation, translation, and summarization, enhancing communication and information retrieval across different applications.
The evolution of language models has progressed from statistical methods to
•Statistical language models: Early models, such as n-grams, estimated the
probability of a word based on its preceding words but struggled with data
sparsity and limited context understanding.
•Neural network models: The introduction of neural networks allowed for
continuous word representations, alleviating some limitations of statistical
language models. Recurrent neural networks and long short-term memory networks were employed to capture sequential data dependencies.
•Transformer architecture: Introduced in [25], transformers utilize self-attention
mechanisms to process input data in parallel, enabling the capture of longrange dependencies more effectively. This architecture underpins many modLLMs are built upon several key concepts. Fristly, LLMs assign probabilities to
sequences of words, modeling the likelihood of a word following a given context.
where P(wi|w1, w2, . . . , w i−1)represents the conditional probability of word wi
Secondly, LLMs employ deep neural networks with multiple layers and parameters to learn complex language patterns. The training process involves adjusting
these parameters to minimize a loss function, typically the cross-entropy loss:
where Nis the number of words in the training corpus.
Finally the heart of modern LLMs lies the Transformer architecture (Figure 1.2).
Transformers use self-attention mechanisms to weigh the importance of different
words in a sequence, enabling the model to capture contextual relationships effectively. The self-attention mechanism is computed as:
where Q(queries), K(keys), and V(values) are matrices derived from the input
embeddings, and dkis the dimensionality of the keys.
Training LLMs involves several key steps. First, data collection is essential, as
it requires aggregating extensive text data from diverse sources to capture a wide
···LLM-based EPS: better heuristics are created via prompting LLMsover program codes.evolve···𝑝𝑜𝑝𝑢𝑙𝑎𝑡𝑖𝑜𝑛0exchange sub-parts···𝑝𝑜𝑝𝑢𝑙𝑎𝑡𝑖𝑜𝑛1selectionGenetic Programming: better heuristics are created through genetic operations over trees.···𝑝𝑜𝑝𝑢𝑙𝑎𝑡𝑖𝑜𝑛𝑛
Generate a heuristicinspired by the given examples
Function setadd  subfor  sum  ⋯⋯Terminal setbins itemstrue false⋯⋯specified byhuman experts
definitial_heuristic(item: float, bins: list) -> list:```Define priority with which we assign an item to each bin.:param item: Size of item to be added to the bin.:param bins: List of capacities for each bin.:return: a list showing priority score of each bin.```return[0] * len(bins)
paradigm (top section), for AHD [27].
LLM-EPS combines the adaptive search capabilities of evolutionary algorithms
with the generative power of LLMs, unlocking new possibilities for AHD. Unlike
traditional optimization methods that produce numeric solutions, LLM-EPS generates interpretable programs or code snippets, allowing for seamless interaction and
validation by human expert an attractive feature for real-world applications. Figure
FunSearch, short for "search in the function space," represents a groundbreaking approach in leveraging LLMs for program synthesis and optimization. Unlike
traditional methods, which often face limitations due to hallucinations or verification challenges. FunSearch innovatively combines an LLM with a systematic
evaluator. This evaluator rigorously scores candidate programs, ensuring correctness and quality, and plays a pivotal role in evolving low-performing solutions into
At the core of FunSearch’s architecture, as shown in Figure 1.4, is its evolutionary methodology, which starts with a user-defined "skeleton" program. This
skeleton encapsulates known functional structures and isolates the critical logic
for optimization, thereby reducing the search space and improving accuracy. The
system employs an island-based model to maintain a diverse pool of candidate programs, as shown in Figure 1.5. By avoiding local optima and encouraging diversity
grams it has generated so far (retrieved from the programs database), and asked to generate
an even better one. The programs proposed by the LLM are automatically executed, and
evaluated. The best programs are added to the database, for selection in subsequent cycles.
The user can at any point retrieve the highest-scoring programs discovered so far.
through genetic-like operations, FunSearch achieves robust exploration across a
wide range of program variations.
The distributed architecture of FunSearch enhances scalability and efficiency.
The system comprises three main components: a programs database, samplers,
and evaluators. The programs database stores correct and promising programs,
while samplers use LLMs to generate new variations based on prompts constructed
from high-scoring programs. Evaluators assess these programs by executing them
against a predefined "evaluate" function, which measures the quality of outputs on
specific inputs. These components operate asynchronously, leveraging parallelism
to handle computationally intensive tasks efficiently.
FunSearch’s iterative process integrates several innovative techniques. Best-shot
prompting ensures that high-quality programs guide the evolution process, while
clustering within islands helps preserve program diversity. The inclusion of Bolt
mann selection mechanisms and fitness-based sampling further refines the evolutionary trajectory, favoring both performance and interpretability. The system’s distributed nature supports scalability, enabling it to tackle large, complex problems
across a variety of domains, from mathematical discovery to algorithm design.
This architecture not only delivers impressive results but also highlights FunSearch’s adaptability. By systematically blending the creativity of LLMs with rigorous evaluation and evolutionary improvements, FunSearch sets a new benchmark
for program synthesis, pushing the boundaries of what LLMs can achieve in strucExample Algorithm MutationParent Algorithm 1
•Description :Select thenext unvisited node
•Description :Select thenext unvisited node
Algorithm Mutation Prompt EngineeringAlgorithm Crossover Prompt Engineeringdefname (input1, …, inputI)
return outputdefname (input1, …, inputI)
current node, subtracting thedistance to
thedestination node .•Code :defname (input1, …, inputI)
current node, subtracting thedistance to
thedestination node .•Code :defname (input1, …, inputI)
node that with thesmallest ratio ofthe
distance tothecurrent node divided by
thedistance tothedestination node•Code :defname (input1, …, inputI)
Prompt Strategies:1)Exploration: E1        E22)Modification: M1       M2       M3
Another noteworthy approach is Evolution of Heuristics (EoH), aimed explicitly
at AHD while minimizing manual labor. EoH represents heuristics both as natural
language “thoughts” and as executable code, mirroring the way human experts
conceptualize and then implement strategies. By employing a population-based
algorithm, EoH evolves these heuristics over successive generations with selection,
crossover, and mutation operations guided by LLMs.
EoH employs a population-based evolutionary model where heuristics evolve
over generations. Selection, crossover, and mutation operations enhanced by LLMs
generate new heuristics iteratively (Figure 1.6). Five specific prompt strategies
guide this evolution (Figure 1.7). Divide to, Exploration: includes generating novel
heuristics (E1) or exploring variations based on common ideas (E2); and Modification: focuses on refining heuristics for improved performance (M1), adjusting
parameters (M2), or simplifying structures by eliminating redundancies (M3).
This method requires fewer queries to LLMs compared to previous techniques
such as FunSearch. Additionally, it eliminates the need for custom-crafted features
or additional training. The upshot is a more streamlined method that outperforms
manually created heuristics across diverse optimization benchmarks. As EoH reduces the overhead of domain-specific engineering, it underscores the breadth of
In pursuit of state-of-the-art results, Reflective Evolution (ReEvo) integrates
LLMs with a GP foundation to tackle NP-hard combinatorial optimization tasks.
ReEvo employs two types of LLMs: a Generator LLM, which produces initial code
snippets or heuristics, and a Reflector LLM, which provides both short-term and
long-term reflections to guide iterative improvements.
The genetic cycle in ReEvo, as shown in Figure 1.8 consists of the following
steps: (i) Population Initialization: Prompts generate initial heuristic candidates.
(ii) Selection: Parent heuristics are chosen based on their performance. (iii) Shortterm Reflection: Comparative analysis of parent performance informs offspring
generation. (iv) Crossover: Generates offspring by combining parent features. (v)
Long-term Reflection: Consolidates insights from iterations to improve heuristic
generation. (vi) Elitist Mutation: Refines the best heuristic further using accumuBy synthesizing immediate feedback (short-term reflection) and broader insights (long-term reflection), ReEvo delivers sample-efficient heuristics, surpassing frameworks like EoH and FunSearch. Its verbal “gradient,” derived from reflective insights, leads to a gentler fitness landscape and boosts optimization stability,
making it a potent approach for real-world combinatorial problems.
Diversity is a cornerstone of successful evolutionary algorithms, especially in
scenarios with multiple objectives [29]. A population rich in diverse solutions
maintains broad coverage of the search space, mitigating the dangers of stagnation in local optima and supporting adaptability to non-static or convoluted fitness
landscapes. Various methods clustering, niching, or entropy-based measures have
been proposed to safeguard diversity, with Shannon entropy being a popular metric
for assessing how “spread out” a set of solutions is [30], [31].
However, embedding these diversity concepts into LLM-EPS raises fresh challenges. Populations are not numeric vectors but heuristic code snippets or descriptive text, which calls for alternative ways to quantify “distance” or “difference.”
What strategies can be employed to evaluate the semantic gap between two heuristic code snippets or two text proposals while effectively capturing their functional
differences? These inquiries highlight a pressing need for innovative strategies in
diversity management that are compatible with LLM-based searches.
Crucially, diversity acts not only as a buffer against premature convergence, but
also as a catalyst for creative or unconventional strategies, a vital ingredient in advanced heuristic design. By developing refined diversity metrics and preserving
variability throughout the evolutionary process, researchers can unlock a broader
range of potential solutions, leading to improved performance and more generalizThis chapter has charted the emerging partnership between EC and LLMs, showcasing how their combined strengths can transform AHD. In LLM-EPS, LLMs
contribute a wellspring of fresh heuristic ideas, and evolutionary methods syste
atically sharpen and validate these concepts. Yet, to extract maximum value from
this synergy, careful attention must be paid to diversity the balancing force that
fosters both robust exploration and refined exploitation.
From traditional GA to newer frameworks like FunSearch, EoH, and ReEvo,
the literature consistently highlights the importance of extensive exploration to
avoid becoming trapped in suboptimal solutions. Diversity is the crucial component that integrates a genuinely adaptive search process, facilitating breakthroughs
in problem-solving. In the following chapter, advanced methods for incorporating
diversity into LLM-EPS will be examined.
This chapter introduces a method for encoding the LLM-EPS population and
proposes two metrics for measuring diversity: SWDI and CDI. Following this, a
diversity analysis will be conducted on previous LLM-EPS frameworks, including
One particular problem in measuring diversity in LLM-EPS is how to encode
the population. While each individual in the traditional evolutionary algorithm is
encoded as a vector, in LLM-EPS they are usually presented as a string of code
snippets/programs. This poses a challenge in applying previous diversity metrics
to the population of LLM-EPS frameworks. To address this challenge, an encoding method is proposed that involves three steps: (i) removing comments and docstrings using abstract-syntax tree, (ii) standardizing code snippets into a common
coding style (e.g., PEP81), (iii) converting code snippets to vector representations
using a code embedding model.
Inspired by ecological studies, [32] provides a quantitative measure of species
diversity within a community. In the context of search algorithms, this index aims
to quantify the diversity of the population at any given time step based on clusters
of individuals. To compute the SWDI for a specific set of individuals within a
community, or archive, it is first necessary to determine the probability distribution
of individuals across the clusters. This is represented as:
where Ciis a cluster of individuals and Mrepresents the total number of individuals across all clusters {C1, . . . , C N}. The SWDI, H(X), is then calculated using the
This index serves as a crucial metric in maintaining the balance between exploration and exploitation within heuristic search spaces. A higher index score suggests a more uniform distribution of individuals across the search space, thus pr
moting exploration. Conversely, a lower index score indicates concentration around
specific regions, which may enhance exploitation but also increase the risk of preTo obtain the SWDI score, at each time step t, all individuals Ri={r1, . . . , r n}
generated during that time step are added to an archive. The population encoding
method described in Section 2.1 is utilized to derive vector representations V=
{v1, . . . , v n}. Following this, cosine similarity is used to compute the similarity
between two embedding vectors viandvj:
To identify clusters in the archive, each embedding vector viis analyzed. Next,
viwill be assigned to a cluster Ciif the similarity between viand all members of
Ciexceeds a specified threshold α. In mathematical terms, viis assigned to Ciif:
similarity (vi, vk)≥α∀k∈ {1, . . . ,|Ci|} (2.4)
If no cluster Ciin {C1, . . . , C N} meets the condition, a new cluster CN+1will
be created and viwill be assigned to CN+1. Finally, the SWDI computed score is
determined by calculating Equations (2.1) and (2.2) across the identified clusters.
While SWDI focuses on diversity of different groups of individuals, the CDI
plays a crucial role in understanding the spread and distribution of the whole population within the search space [33], [34]. In the context of heuristic search, the CDI
measures how well a system’s energy, or diversity, is distributed from a centralized
state to a more dispersed configuration.
Many entropy interpretations have been suggested over the years. The best known
are disorder, mixing, chaos, spreading, freedom and information [35]. The first description of entropy was proposed by Boltzmann to describe systems that evolve
from ordered to disordered states. Spreading was used by Guggenheim to indicate
the diffusion of a energy system from a smaller to a larger volume. Lewis stated
that, in a spontaneous expansion gas in an isolated system, information regarding
particles locations decreases while, missing information or uncertainty increases.
In graph theory, each edge can be assigned a weight representing, for example,
the energy required to traverse that edge. A graph with a complex structure and
varying edge weights exhibits higher entropy, indicating a more disordered system.
Conversely, a simpler structure with uniformly distributed weights corresponds to
An Minimum Spanning Tree (MST) of a connected, edge-weighted graph is a
subset of the edges that connects all vertices without any cycles and with the minimum possible total edge weight. Constructing an MST can be seen as a process
of transitioning from a high-entropy state (the original graph with all its edges and
weights) to a lower-entropy state. This transition reflects the idea of energy distribution from a "better located" state to a "more distributed" one, as the MST represents the most efficient way to connect all vertices, minimizing the total "energy"
Consider a connected, undirected graph G= (V, E)with a weight function w:
E→R+. The entropy Hof the graph can be associated with the distribution of
edge weights. One way to define this entropy is:
where p(e)is probability-like measure of edge ebased on its weight.
The MST, T= (V, ET), is a subgraph where ET⊆Eand the total weightP
e∈ETw(e)is minimized. By selecting the subset of edges that connect all vertices with the minimal total weight, the MST reduces the system’s complexity and,
To calculate the CDI, let’s consider all individuals within an archive, represented
by their respective embeddings. By constructing a MST that connects all individuals within the archive A, where each connection between individuals is calculated
using Euclidean distance. This MST provides a structure to assess the diversity of
the population. Let direpresent the distance of an edge within the MST, where
i∈ {1,2, . . . , #A−1}. The probability distribution of these distances is given by:
The cumulative diversity is then calculated using the Shannon entropy:
This method enables the collection of the complete diversity present in the
search space, offering valuable information about the range of solutions. Higher
CDI values indicate a more distributed and diverse population, which is essential
for maintaining a robust search process.
An interesting point in Shannon Diversity Index (SDI) theory is that normalizing
the SDI with the natural log of richness is equivalent to the evenness valueH′
where H′represents the richness of SDI and Sis the total number of possible
categories [36]. The significance of evenness is to demonstrate how the proportions
Now, consider normalizing with the two current diversity indices. Firstly, in the
case of SWID, if there are Nclusters and the count of individuals in each cluster
from C1toCNis uniform, then the evenness will consistently equal 1, regardless
of the value of N. This is not the preferred outcome since the number of clusters
should be considered as a factor of diversity. Likewise, normalizing the CDI will
also overlook the influence of the number of candidate heuristics, which correspond to the nodes in the MST. This leads to the proposal not to normalize the
SWDI and CDI metrics to achieve a [0,1]bound.
To investigate the relationship between the two diversity measurement metrics
and the objective score, three experimental runs were performed using ReEvo on
three distinct AHD problems, Bin Packing Online (BPO), Traveling Salesman
Problem (TSP) with Guided Local Search (GLS) [37], and Orienteering Problem (OP) with Ant Colony Optimization (ACO) solver [38]. Details of the experiments can be found in Section 4.1. The diversity evaluation process was conducted
with the code embedding model used is CodeT5+2[39], and similarity threshold
α= 0.95where α∈[0; 1]. The maximum budget for each run is 450K tokens. The
objective scores and the two diversity metrics from these runs on BPO, TSP, and
OP are presented in Figure 2.1, Figure 2.2, and Figure 2.3, respectively.
From figure 2.1, there are a few observations that can be drawn. First, the two diversity measurement metrics have a noticeable correlation with the objective scores
of the problem. When the objective score is converged into a local optima, the
framework either tries to focus on the exploration by increasing the diversity of the
population, through the increase of SWDI in the first and second runs or tries to
focus on the exploitation of the current population, thus decrease the SWDI in the
third run. Focusing on exploration can lead to considerable gains in the objective
score, whereas concentrating on exploitation may cause the population to remain
stuck in local optima for an extended period. However, if the whole population is
not diverse enough, as can be seen with the low CDI of the second run, the population might not be able to escape the local optima. These observations about
the correlation between the objective score and diversity measurement metrics are
similarly reflected in Figures 2.2 and 2.3.
To analyze the overall diversity of previous LLM-EPS frameworks, experiments
were conducted on three distinct LLM-EPS frameworks: FunSearch [13], ReEvo
[15], and EoH [14], focusing on three separate AHD problems (BPO, TSP, OP).
The details of each experiment are presented in Section 4.1. Note that, as hig
lighted in the previous section, SWDI focuses on understanding the diversity of
the population during a single run. As such, it may not help quantify the diversity
across multiple experiment runs. Fig. 2.4 presents the experiment results.
In BPO and TSP, EoH obtained the highest CDI but got the worst objective
score. This implies that EoH does not focus enough on exploitation to optimize
the population better. In contrast, while ReEvo and FunSearch obtain lower CDI
than EoH on BPO and TSP, they achieve better objective performance on all three
problems. The experiments conducted on the BPO and TSP problems illustrate
the inherent trade-off between diversity and objective performance in LLM-EPS
frameworks. However, for the OP problem, it is evident that a high CDI is necessary
to achieve a better objective score, which aligns with the findings discussed in
In conclusion, this chapter has introduced a comprehensive approach to measuring diversity within LLM-EPS frameworks, focusing on SWDI and CDI. The
proposed population encoding method addresses the unique challenge of representing code snippets as vectors, enabling the application of these diversity metrics. Through extensive experiments on various AHD problems, the correlation
between diversity metrics and objective scores has been explored, revealing the
delicate balance between exploration and exploitation in heuristic search spaces.
The analysis of previous LLM-EPS frameworks, including FunSearch, ReEvo, and
EoH, underscores the trade-offs between diversity and performance, highlighting
the importance of maintaining an optimal level of diversity to avoid premature convergence and enhance solution quality.
This chapter introduces a novel LLM-EPS framework called Harmony Search
Evolution (HSEvo). HSEvo aims to enhance the diversity of the population while
improving optimization performance and relieving the trade-off between these two
aspects through an individual tuning process based on harmony search. Additionally, the framework aims to minimize costs associated with LLMs by incorporating
an efficient flash reflection component. Figure 3.1 illustrates the pipeline of the
Individual encoding. In HSEvo, each individual within the evolutionary process is represented as a string of executable code (Figure 3.2a). This concept is a direct inheritance from earlier LLM-EPS such as EoH and ReEvo. This code snippet,
generated by a LLMs, embodies a specific heuristic function tailored to the target
optimization problem. This encoding method is deliberately flexible, eschewing
any predefined format or structure, which allows for the generation of diverse and
complex heuristics. Unlike traditional methods that might use fixed-length vectors
or predefined data structures. HSEvo’s code-based approach enables the expression of a wide range of algorithmic strategies, including conditional logic, loops,
and function calls, making it highly adaptable to various problem domains.
The choice of code as the encoding mechanism also facilitates direct evaluation
of the generated heuristics. Each code snippet is directly executable, either within a
sandboxed environment or through a standard Python subprocess, simplifying the
evaluation process and eliminating the need for intermediate translation steps. This
direct executability is particularly advantageous for AHD problems, where the goal
is to automatically discover effective heuristics.
Initialization. HSEvo initializes a heuristic population by prompting the generator LLM with task specifications that describe the problem and detail the signature
of the heuristic function to be searched. Additionally, to leverage existing knowledge, the framework can be initialized with a seed heuristic function and/or external domain knowledge. To promote diversity, prompts are created with various role
instructions (e.g., "You are an expert in the domain of optimization heuristics...",
"You are Albert Einstein, developer of relativity theory..."), details instructions in
the next Section 3.2.1. This approach aims to enrich the heuristic generation process by incorporating diverse perspectives and expertise.
of heur isticx NSelec tionCr osso v erFlash Reflec tion
Elitist MutationHar m on y Se ar c h
Se ar c h*T r y _ HS 
Heur isticDeep analy sisLLMElitist Heur isticReflec tionFlo w dir ec tionSt opThe Last
Selection. In HSEvo, the selection of parent pairs for crossover is performed
using a random selection strategy. This approach involves choosing two individuals from current population with equal probability, without any bias towards their
fitness or performance. The primary goal of this random selection is to maintain a
balance between exploration and exploitation during optimization process. By not
favoring the best-performing individuals, HSEvo ensures that search space is not
prematurely narrowed, allowing for the discovery of potentially novel and effective
heuristics that might be overlooked by more deterministic selection methods.
Furthermore, random selection mechanism is designed to counteract premature convergence, a phenomenon where the population becomes homogeneous too
quickly, hindering further exploration. This is particularly important in the context
of LLM-EPS, where the initial population might be biased towards certain types of
solutions. The random selection helps to maintain diversity within the population,
preventing the search from getting stuck in local optima and promoting a more
thorough exploration of the heuristic space. This is supported by observations of
the SWDI trajectory in Section 2.4, which suggests that random selection can help
to maintain a broader search width, thus avoiding premature convergence.
Flash reflection. Reflections can provide LLMs with reinforcement learning
reward signals in a verbal format for code generation tasks, as discussed by [40].
Later, [15] also proposed integrating Reflections into LLM-EPS in ReEvo as an
approach analogous to providing “verbal gradient” information within heuristic
spaces. HSEvo argue that reflecting on each pair of heuristic parents individually
is not generalized and wastes resources. To address these issues flash reflection
proposed to alternative Reflection method for LLM-EPS. The flash reflection process is structured into two distinct steps.
•First, at each time step t, the individuals selected during the selection phase
are grouped, and any duplicate individuals are removed. This ensures that the
analysis is performed on a unique set of heuristics. The LLM then undertakes
a deep analysis of the performance ranking of these individuals. This take inputs as mixed ranking pairs (i.e., one good performance parent and one worse
performance), good ranking pairs (i.e., both good performance), and worse
ranking pairs (i.e., both worse performance), then return a comprehensive description on how to improve as a text string.
•The second step, flash reflection involves comparing the current analysis at
time step twith the previous analysis from time step t−1. This comparison is
performed by the LLM, which then generates guide information based on the
differences and similarities between the two analyses. This guide information
is designed to be used in subsequent stages of the evolutionary process (e.g.
crossover and mutation) to further refine the heuristics. By incorporating this
temporal aspect, flash reflection allows LLM to learn from its past experiences and adapt its code generation strategy over time. This iterative process
of analysis and comparison enables LLM to continuously improve its understanding of the heuristic space and generate increasingly effective solutions.
In essence, flash reflection in HSEvo acts as a form of meta-learning, where the
LLM not only generates heuristics but also learns how to generate better heuristics
over time. By providing a more generalized and resource-efficient approach to reflection, HSEvo aims to overcome the limitations of previous methods and achieve
more effective and efficient heuristic discovery. The textual feedback generated by
flash reflection provides a rich source of information that guides the LLM towards
generating more promising solutions, ultimately leading to improved performance
in the target optimization problem.
Crossover. In this stage, new offspring algorithms are generated by combining
elements of the parent algorithms. The goal is to blend successful attributes from
two parents via guide result guide information part of flash reflections. Through
this step, HSEvo hopes to produce offspring that inherit the strengths of both,
which can potentially lead to better-performing heuristics. The prompt includes
task specifications, a pair of parent heuristics, guide information part of flash reElitist mutation. HSEvo employs an elitist mutation strategy, focusing its mut
tion efforts on the "elite" individual, which represents the best-performing heuristic found so far. This approach leverages the generator LLM to mutate this elite
individual, incorporating insights derived from the LLM-analyzed flash reflections. The mutation process is not random; instead, it is guided by the accumulated knowledge and feedback from the flash reflection, aiming to enhance the
elite heuristic’s performance. Each mutation prompt includes detailed task specifications, the code of the elite heuristic, the deep analysis from the flash reflections,
and specific generation instructions, providing the LLM with the necessary context
to perform a targeted and informed mutation.
This elitist mutation strategy is designed to ensure continuous improvement in
performance while preserving the quality of the top solutions. By focusing on the
best-performing heuristic, HSEvo aims to refine and enhance its capabilities, rather
than randomly exploring the search space. The incorporation of insights from the
flash reflections ensures that the mutation process is not arbitrary but rather directed towards making meaningful improvements based on the accumulated knowledge of the evolutionary process. This approach allows HSEvo to effectively exploit
the best solutions found so far, while still allowing for exploration through other
mechanisms like crossover and the initial population generation.
Harmony Search. From the analysis in Section 2.4 and 2.5. The hypothesis
is that if the population becomes too diverse, individual heuristics within it are
more likely to be unoptimized, which can negatively impact the overall optimization process. To mitigate this, HSEvo employs HS to fine-tune the parameters (e.g.,
thresholds, weights, etc.) of the best-performing individuals in the population. The
•First, once an individual is selected, the LLM is tasked with extracting parameters from the best individual (i.e., code snippets, programs) of the population
and defining a range for each parameter (Figure 3.2).
•Following the parameter extraction, HSEvo employs the HS algorithm, a metaheuristic optimization technique inspired by the improvisation process of musicians. The HS algorithm iteratively adjusts the extracted parameters within
their defined ranges, aiming to find a combination that maximizes the heuristic’s performance. This process involves creating a "harmony memory," which
stores a set of parameter combinations, and iteratively generating new harmonies by adjusting existing ones or creating new ones randomly. The performance of each harmony is evaluated, and the harmony memory is updated
with better-performing combinations. This iterative process continues for a
predefined number of iterations, allowing the HS algorithm to explore parameter space and identify optimal or near-optimal parameter settings.
•After the parameter optimization is complete, the optimized individual is marked
to prevent it from being optimized again in future time steps. This ensures that
the HS algorithm is not repeatedly applied to the same individuals, allowing
the framework to focus on optimizing other promising heuristics. The optimized individual is then added back to the population, contributing to the
overall diversity and performance of the population. This integration of HS
into the evolutionary process allows HSEvo to fine-tune the best-performing
heuristics, enhancing their performance and mitigating the negative effects of
To sum up, the HS component in HSEvo is a critical mechanism for enhancing
the performance of the best-performing heuristics by fine-tuning their parameters.
This process involves using the LLMs to extract parameters and define their ranges,
followed by the application of the HS algorithm to optimize these parameters. The
optimized individuals are then added back to the population, contributing to the
overall performance and diversity of the population. This strategic integration of
HS allows HSEvo to effectively balance exploration and exploitation, leading to
the discovery of more robust and high-performing heuristics.
An overview of the flow of HSEvo is also described through pseudo-algorithm
1, where function_evals <max_fe serves as the stopping condition. Here,
function_evals is a counter variable tracking the number of evaluated individuals, and max_fe is the maximum number of individuals HSEvo will evaluate.
Data: Configuration files, problem-specific parameters
Result: Best code found and its path
Initialization : Load configuration, initialize LLM prompts, and generate
Selection : Select parents randomly from the population;
Reflection : Perform flash and comprehensive reflection using LLM;
Crossover : Generate and evaluate new individuals by crossing parents
Mutation : Generate and evaluate new individuals by mutating the best
Select an individual for harmony search;
Extract parameters and code for harmony search using LLM;
Initialize harmony memory and create a new population;
while harmony search is ongoing do
Update harmony memory and population;
Add the best individual to the population;
Update : Update the best individual (elitist);
Increment iteration count and log results;
Check and update reflection lists based on elitist changes;
Return the best code found and its path;
This section synthesizes the prompts used in the HSEvo framework. The HSEvo
prompt system includes four stages: initialization, population, flash reflection, crossover,
Prompt 1: System prompt for generator LLM.
{role_init} helping to design heuristics that can effectively solve optimizaYour response outputs Python code and nothing else. Format your code as a
Python code string: “‘python ... “‘ .
{role_init} Your task is to write a {function_name} function for {probIn this work, the variable role_init is utilized to assign various roles to the
LLMs, allowing the creation of distinct personas [41] that can provide different
perspectives and promote population diversity. The roles are chosen in a roundrobin sequence from the list:
•You are an expert in the domain of optimization heuristics,
•You are Albert Einstein, relativity theory developer,
•You are Isaac Newton, the father of physics,
•You are Marie Curie, pioneer in radioactivity,
•You are Nikola Tesla, master of electricity,
•You are Galileo Galilei, champion of heliocentrism,
•You are Stephen Hawking, black hole theorist,
•You are Richard Feynman, quantum mechanics genius,
•You are Rosalind Franklin, DNA structure revealer,
•You are Ada Lovelace, computer programming pioneer.
correspond to the heuristic function name, the specific problem description, and
the function description, respectively, which will be detailed in the section 3.2.7.
Prompt 3: User prompt for population initialization.
Refer to the format of a trivial design above. Be very creative and give ‘{function_name}_v2’. Output code only and enclose your code with Python code
block: “‘python ... “‘ .
Here, task_description refer to Prompt 2, seed_ function is a trivial heuristic function for the corresponding problem.
Prompt 4: System prompt for reflector LLM.
You are an expert in the domain of optimization heuristics. Your task is to
provide useful advice based on analysis to design better heuristics.
Prompt 5: User prompt for flash reflection pharse 1.
Below is a list of design heuristics ranked from best to worst.
- Keep in mind, list of design heuristics ranked from best to worst. Meaning
the first function in the list is the best and the last function in the list is the
- The response in Markdown style and nothing else has the following struc+ Meticulously analyze comments, docstrings and source code of several
pairs (Better code - Worse code) in List heuristics to fill values for **AnalyExample: “Comparing (best) vs (worst), I see ...; (second best) vs (second
worst) ...; Comparing (1st) vs (2nd), I see ...; (3rd) vs (4th) ...; Comparing
(worst) vs (second worst), I see ...; Overall:...”
+ Self-reflect to extract useful experience for design better heuristics and fill
to **Experience:** ( <60 words).
I’m going to tip $999K for a better heuristics! Let’s think step by step.
list_ranked_heuristics is a list of heuristic functions obtained by grouping parent pairs from selection, removing duplicates, and ranking based on the
Prompt 6: User prompt for flash reflection pharse 2.
Your task is to redefine ‘Current self-reflection’ paying attention to avoid all
things in ‘Ineffective self-reflection’ in order to come up with ideas to design
Response ( <100 words) should have 4 bullet points: Keywords, Advice,
I’m going to tip $999K for a better heuristics! Let’s think step by step.
current_reflection is the output of the flash reflection step 1 from the
current generation. good_reflection is the output of the flash reflection step
2 from previous generations where a new heuristic was discovered. Conversely,
bad_reflection is the output of the flash reflection step 2 from previous generations where no new heuristic was discovered.
Prompt 7: User prompt for crossover.
Your task is to write an improved function ‘{func_name}_v2‘ by COMBIN-
ING elements of two above heuristics base Analyze & experience.
Output the code within a Python code block: “‘python ... “‘, has comment
and docstring ( <50 words) to description key idea of heuristics design.
I’m going to tip $999K for a better heuristics! Let’s think step by step.
nature_worse ,code_worse are the function signatures and code of the better
and worse parent individuals, respectively. flash_refection is the output of
the flash reflection step 2 from the current generation.
Prompt 8: System prompt for elitist mutation.
Now, think outside the box write a mutated function ‘{func_name}_v2‘ better than current version. You can using some hints if need:
Output code only and enclose your code with Python code block: “‘python
I’m going to tip $999K for a better solution!
function_signature_elitist ,elitist_code are the function signatures and code of elitist individuals of current generation.
Prompt 9: System prompt for Harmony Search.
You are an expert in code review. Your task extract all threshold, weight or
hardcode variable of the function make it become default parameters.
Prompt 10: User prompt for Harmony Search.
Now extract all threshold, weight or hardcode variable of the function make
it become default parameters and give me a ’parameter_ranges’ dictionary
representation. Key of dict is name of variable. Value of key is a tuple in
Python MUST include 2 float elements, first element is begin value, second
element is end value corresponding with parameter.
- Output code only and enclose your code with Python code block:
- Output ’parameter_ranges’ dictionary only and enclose your code with
other Python code block: “‘python ... “‘ .
elitist_code is the snippet/string code of the elitist individual in the current population that has not yet undergone harmony search.
The best heuristics generated by HSEvo for all problem settings are presented
in Figure 3.9, 3.10, 3.11, 3.12, 3.13, and 3.14.
To summarize, this chapter presented HSEvo, an innovative LLM-EPS framework that transforms heuristic optimization by integrating population diversity and
performance through advanced techniques such as flash reflection and harmony
search. HSEvo utilizes code-based individual encoding to enhance its adaptability
in addressing a wide range of intricate optimization challenges. Key components,
such as the random selection mechanism, flash reflection, and elitist mutation, ensure thorough exploration of the heuristic space while preventing premature convergence. By incorporating the HS algorithm for fine-tuning parameters, HSEvo
further optimizes its best-performing heuristics, achieving a synergy between exploration and exploitation. Along with this, detailed descriptions of prompts, pseudocode, and the best heuristics on various benchmarks using HSEvo are provided.
8 heuristics = (prize *
(a)Origin Code1import numpy as np
11 heuristics = (prize *
1def priority_v{version}(item: float , bins_remain_cap: np.ndarray)
local_opt_tour: np.ndarray, edge_n_used: np.ndarray) -> np.
1def heuristics_v{version}(node_attr: np.ndarray, edge_attr: np.
1def priority_v1(item: float , bins_remain_cap: np.ndarray) -> np.
2 """Returns priority with which I want to add item to each bin
5 item: Size of item to be added to the bin.
6 bins_remain_cap: Array of capacities for each bin.
9 Array of same size as bins_remain_cap with priority score
11 ratios = item / bins_remain_cap
local_opt_tour: np.ndarray, edge_n_used: np.ndarray) -> np.
4 edge_distance (np.ndarray): Original edge distance matrix
5 local_opt_tour (np.ndarray): Local optimal solution path.
6 edge_n_used (np.ndarray): Matrix representing the number
of times each edge is used.
8 updated_edge_distance: updated score of each edge
14 for iin range (num_nodes - 1):
16 next_node = local_opt_tour[i + 1]
17 updated_edge_distance[current_node, next_node] *= (1 +
*= (1 + edge_n_used[local_opt_tour[-1], local_opt_tour
1def heuristics_v1(node_attr: np.ndarray, edge_attr: np.ndarray,
8def priority_v2(item: float , bins_remain_cap: np.ndarray,
overflow_threshold: float = 1.0987600915713542, mild_penalty:
float = 0.5567025232550017, adaptability_lower: float =
9 """Enhanced dynamic scoring function for optimal bin
selection in online BPP with a more holistic approach."""
11 # Avoid division by zero by adjusting remaining capacities
12 adjusted_bins_remain_cap = np.maximum(bins_remain_cap, np.
14 # Calculate effective capacities
15 effective_cap = np.clip(bins_remain_cap - item, 0, None)
16 valid_bins = effective_cap >= 0
18 # Calculate occupancy ratios with controlled overflow
19 occupancy_ratio = item / adjusted_bins_remain_cap
20 occupancy_scores = np.where(valid_bins, occupancy_ratio, 0)
22 # Enhanced overflow penalty: stronger influence for near23 overflow_penalty = np.where(occupancy_ratio >
overflow_threshold, mild_penalty * (occupancy_ratio -
25 # Logarithmic penalty for remaining capacity to encourage
26 log_penalty = np.where(bins_remain_cap > 0, np.log1p(
4 # Adaptability based on remaining mean capacity
5 remaining_mean = np.mean(bins_remain_cap[bins_remain_cap >
6 adaptability_factor = np.where(bins_remain_cap <
8 # Comprehensive scoring integrating all metrics for a robust
9 scores = np.where(valid_bins, occupancy_scores *
overflow_penalty * log_penalty * adaptability_factor, -np.
11 # Normalize scores for prioritization
12 max_score = np. max(scores)
13 prioritized_scores = (scores - np. min(scores)) / (max_score -
np.min(scores)) ifmax_score > np. min(scores) else scores
15 # Invert scores for selecting the highest priority bin
16 inverted_priorities = 1 - prioritized_scores
9 Update edge distances using adaptive penalties and bonuses,
10 while ensuring nuanced performance in line with real-time
15 # Calculate average usage for dynamic adjustments
18 for iin range (num_nodes):
20 next_node = local_opt_tour[(i + 1) % num_nodes]
21 usage_count = edge_n_used[current_node, next_node]
23 # Adaptive penalty for overused edges
25 # Penalty increases exponentially with usage
26 penalty = penalty_factor * (usage_count -
4 elif usage_count < boost_threshold:
5 # Apply a bonus to underused edges
6 boost = bonus_factor * (1 + (boost_threshold -
10 # Dynamic scaling based on average usage
12 adjustment_factor = scaling_factor / (1 +
decay_factor ** (usage_count - avg_usage))
14 adjustment_factor = scaling_factor * (1 +
decay_factor ** (avg_usage - usage_count))
16 # Update the distance with adjustment and ensure non17 updated_edge_distance[current_node, next_node] = max(
4def heuristics_v2(node_attr: np.ndarray, edge_attr: np.ndarray,
node_constraint: float ) -> np.ndarray:
6 Enhanced heuristics incorporating contextual adjustments,
7 and adaptive responsiveness to edge conditions.
16 # Normalize node attributes (maintain zero division safety)
17 total_node_attr = np. sum(node_attr) + normalization_epsilon
18 normalized_node_attr = node_attr / total_node_attr
20 for iin range (n):
21 for jin range (n):
23 continue # Skip self-loops
25 # Calculate contextual adjustment for edge attributes
26 dynamic_edge = edge_attr[i, j] ** adaptability_factor
27 adjusted_edge = dynamic_edge / (node_constraint +
29 # Multi-dimensional scaling based on edge and node
31 scaling_factor = np.sqrt(adjusted_edge +
33 scaling_factor = influence_threshold / (
4 # Layered logic for combined node influence using non
6 (normalized_node_attr[i] ** non_linearity_base) +
8 ) ** 1.5 # Further amplify the influence
10 # Score calculation with contextual penalties (non11 score = combined_node_influence * scaling_factor
14 penalty_base = np.log1p(dynamic_edge) # Non15 penalty_factor = 1 / (1 + penalty_base ** 2) #
18 # Normalization to retain meaningful scale
19 score_matrix[i, j] = score / (dynamic_edge +
Furthermore, the ablation studies emphasize the pivotal contributions of the harmony search and flash reflection components in enhancing performance of HSEvo.
While harmony search bolsters the optimization process by promoting diversity,
flash reflection offers a cost-effective yet impactful mechanism for refining solutions. Together, these components elevate HSEvo’s capabilities, establishing it as a
robust framework for optimizing heuristic design. The results of this chapter lay a
solid foundation for future exploration of HSEvo’s potential in broader optimization domains, reinforcing its role as a transformative advancement in LLM-EPS.
This thesis provides the pivotal role of population diversity in enhancing the
performance of LLM-EPS for AHD. Two novel diversity measurement metrics,
the SWDI and CDI, were introduced to analyze and monitor population diversity
quantitatively. A comprehensive review of existing LLM-EPS frameworks reveals
a common oversight: the failure to adequately balance population diversity with
optimization effectiveness, often leading to suboptimal results.
In response to existing boundaries, the introduction of HSEvo a new state-ofthe-art LLM-EPS framework marks a notable advancement in this field. HSEvo
leverages a diversity-driven harmony search and genetic algorithm to maintain
an optimal balance between exploration and exploitation within heuristic search
spaces. By incorporating mechanisms like flash reflection and elitist mutation,
HSEvo achieves high diversity indices and competitive objective scores across various optimization problems, such as the BPO, TSP, and OP. Ablation studies also
confirmed the effectiveness of HSEvo’s components, underscoring its capability to
This work also establishes a new standard in the domain of LLM-EPS, highlighting the important relationship between diversity and optimization. The results
not only fill the voids in current frameworks but also create opportunities for future studies to expand on the suggested metrics and methods. By enhancing the
comprehension of heuristic search spaces and promoting automated design techniques. This research offers significant insights and resources for tackling intricate
Furthermore, creating effective stopping criteria for the search process is a promising area for research. By integrating diversity metrics and evaluating objective
scores. It may be possible to reduce the computational costs inherent in LLM operations, thereby enhancing efficiency without compromising performance.
However, the thesis acknowledges certain limitations, particularly in evaluating
LLM-EPS applications to AHD. Currently, experiments are only confined to GPT-
4o-mini model due to the high costs associated with utilizing closed-source APIs.
Expanding evaluations to incorporate additional models such as Qwen, LLaMA,
Gemini, and others could significantly reinforce the robustness of the findings of
this thesis. Another area of concern is parameter sensitivity. The harmony search
component introduces additional hyperparameters HMCR, PAR, and bandwidth
that influence performance. This is a gap that persists across the LLM-EPS, i
cluding in prior works like FunSearch, EoH, and ReEvo. Addressing parameter
sensitivity represents a promising avenue for future research.
Parts of the work presented in this thesis have been accepted at the 39th Annual
AAAI Conference on Artificial Intelligence (AAAI-25):
1.Pham Vu Tuan Dat , Long Doan, and Huynh Thi Thanh Binh, "HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search
and Genetic Algorithm Using LLMs", The 39th Annual AAAI Conference on
Artificial Intelligence , (AAAI-25), 2025. (accepted, rank A* ).